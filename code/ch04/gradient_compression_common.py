"""Shared gradient compression benchmark logic (single- and multi-GPU)."""

from __future__ import annotations

from typing import List, Optional

import torch

from core.harness.benchmark_harness import BaseBenchmark, BenchmarkConfig, WorkloadMetadata
from ch04.verification_payload_mixin import VerificationPayloadMixin


def attach_benchmark_metadata(bench: BaseBenchmark, module_file: str) -> BaseBenchmark:
    """Ensure subprocess runner calls get_benchmark() for parametrized benchmarks."""
    bench._module_file_override = module_file
    bench._factory_name_override = "get_benchmark"
    return bench


class GradientCompressionBenchmark(VerificationPayloadMixin, BaseBenchmark):
    """Gradient all-reduce benchmark with optional compression."""

    def __init__(
        self,
        *,
        compression: str,
        equivalence_group: str,
        output_tolerance: tuple[float, float],
        tensor_size_mb: int = 128,
        multi_gpu: bool = True,
    ) -> None:
        super().__init__()
        self.multi_gpu_required = bool(multi_gpu)
        self.signature_equivalence_group = equivalence_group
        self.signature_equivalence_ignore_fields = ("precision_flags",)
        self.compression = compression  # "none", "fp16", "int8"
        self.output_tolerance = output_tolerance
        self.tensor_size_mb = tensor_size_mb
        self.multi_gpu = bool(multi_gpu)
        self.world_size = 0
        self.devices: List[torch.device] = []
        self.inputs: List[torch.Tensor] = []
        self.output: Optional[torch.Tensor] = None
        self._verify_input: Optional[torch.Tensor] = None
        self._fp16_buffers: List[torch.Tensor] = []
        self._fp16_outputs: List[torch.Tensor] = []
        self._fp16_output_fp32: Optional[torch.Tensor] = None
        self._int8_buffers: List[torch.Tensor] = []
        self._int8_outputs: List[torch.Tensor] = []
        self._int8_float_buffers: List[torch.Tensor] = []
        self._int8_max_vals: List[torch.Tensor] = []
        self._int8_scales: List[torch.Tensor] = []
        self._int8_output_fp32: Optional[torch.Tensor] = None
        tokens = float(tensor_size_mb * 1024 * 1024)
        self._workload = WorkloadMetadata(
            requests_per_iteration=1.0,
            tokens_per_iteration=tokens,
        )
        self.register_workload_metadata(
            requests_per_iteration=1.0,
            tokens_per_iteration=tokens,
        )

    def setup(self) -> None:
        torch.manual_seed(42)
        torch.cuda.manual_seed_all(42)
        if self.multi_gpu:
            self.world_size = torch.cuda.device_count()
            if self.world_size < 2:
                raise RuntimeError("SKIPPED: requires >=2 GPUs")
            self.devices = [torch.device(f"cuda:{idx}") for idx in range(self.world_size)]
        else:
            if not torch.cuda.is_available():
                raise RuntimeError("SKIPPED: requires CUDA")
            self.world_size = 1
            self.devices = [self.device]
        numel = (self.tensor_size_mb * 1024 * 1024) // 4  # FP32 bytes
        self.inputs = [
            torch.randn(numel, device=device, dtype=torch.float32) for device in self.devices
        ]
        self._verify_input = self.inputs[0]
        if self.compression == "fp16":
            self._fp16_buffers = [
                torch.empty_like(t, dtype=torch.float16) for t in self.inputs
            ]
            self._fp16_outputs = [torch.empty_like(t) for t in self._fp16_buffers]
            self._fp16_output_fp32 = torch.empty_like(self.inputs[0])
        elif self.compression == "int8":
            self._int8_buffers = [
                torch.empty_like(t, dtype=torch.int8) for t in self.inputs
            ]
            self._int8_outputs = [torch.empty_like(t) for t in self._int8_buffers]
            self._int8_float_buffers = [torch.empty_like(t) for t in self.inputs]
            self._int8_max_vals = [
                torch.empty((), device=t.device, dtype=torch.float32) for t in self.inputs
            ]
            self._int8_scales = [
                torch.empty((), device=t.device, dtype=torch.float32) for t in self.inputs
            ]
            self._int8_output_fp32 = torch.empty_like(self.inputs[0])
        self._synchronize_all()

    def benchmark_fn(self) -> None:
        if not self.inputs:
            raise RuntimeError("Inputs not initialized")
        with self._nvtx_range(f"gradient_compression_{self.compression}"):
            if self.compression == "none":
                if self.multi_gpu:
                    outputs = [torch.empty_like(t) for t in self.inputs]
                    torch.cuda.nccl.all_reduce(self.inputs, outputs=outputs)
                    self.output = outputs[0]
                else:
                    self.output = self.inputs[0].clone()
            elif self.compression == "fp16":
                if not self._fp16_buffers:
                    raise RuntimeError("FP16 buffers not initialized")
                for src, buf in zip(self.inputs, self._fp16_buffers):
                    buf.copy_(src)
                if self.multi_gpu:
                    torch.cuda.nccl.all_reduce(self._fp16_buffers, outputs=self._fp16_outputs)
                    reduced = self._fp16_outputs[0]
                else:
                    reduced = self._fp16_buffers[0]
                if self._fp16_output_fp32 is None:
                    raise RuntimeError("FP16 output buffer not initialized")
                self._fp16_output_fp32.copy_(reduced)
                self.output = self._fp16_output_fp32
            elif self.compression == "int8":
                self.output = self._int8_all_reduce()
            else:
                raise ValueError(f"Unknown compression mode: {self.compression}")
        self._synchronize_all()
        if self.output is None:
            raise RuntimeError("benchmark_fn() must produce output for verification")

    def _int8_all_reduce(self) -> torch.Tensor:
        if not self._int8_buffers or not self._int8_float_buffers:
            raise RuntimeError("INT8 buffers not initialized")
        for src, max_buf in zip(self.inputs, self._int8_max_vals):
            max_buf.copy_(src.abs().max())
        if self.multi_gpu:
            # NCCL op value 2 maps to MAX for torch.cuda.nccl.all_reduce.
            torch.cuda.nccl.all_reduce(self._int8_max_vals, op=2)
            limit = max(1, 127 // self.world_size)
        else:
            limit = 127
        for idx, src in enumerate(self.inputs):
            scale = self._int8_max_vals[idx] / float(limit)
            if scale.item() == 0:
                self._int8_scales[idx].fill_(1.0)
            else:
                self._int8_scales[idx].copy_(scale)
            float_buf = self._int8_float_buffers[idx]
            float_buf.copy_(src)
            float_buf.div_(self._int8_scales[idx])
            float_buf.round_()
            float_buf.clamp_(-limit, limit)
            self._int8_buffers[idx].copy_(float_buf.to(torch.int8))
        if self.multi_gpu:
            torch.cuda.nccl.all_reduce(self._int8_buffers, outputs=self._int8_outputs)
            reduced = self._int8_outputs[0]
        else:
            reduced = self._int8_buffers[0]
        if self._int8_output_fp32 is None:
            raise RuntimeError("INT8 output buffer not initialized")
        self._int8_output_fp32.copy_(reduced.float())
        self._int8_output_fp32.mul_(self._int8_scales[0])
        return self._int8_output_fp32

    def capture_verification_payload(self) -> None:
        if self._verify_input is None or self.output is None:
            raise RuntimeError("setup() and benchmark_fn() must run before capture_verification_payload()")
        precision_flags = {
            "fp16": self.compression == "fp16",
            "bf16": False,
            "fp8": False,
            "tf32": torch.backends.cuda.matmul.allow_tf32,
        }
        self._set_verification_payload(
            inputs={"input": self._verify_input},
            output=self.output.detach().clone(),
            batch_size=int(self._verify_input.shape[0]),
            parameter_count=0,
            precision_flags=precision_flags,
            output_tolerance=self.output_tolerance,
            signature_overrides={
                "world_size": self.world_size,
                "ranks": list(range(self.world_size)),
                "collective_type": "all_reduce",
            },
        )

    def teardown(self) -> None:
        self.inputs = []
        self.output = None
        self._verify_input = None
        self._fp16_buffers = []
        self._fp16_outputs = []
        self._fp16_output_fp32 = None
        self._int8_buffers = []
        self._int8_outputs = []
        self._int8_float_buffers = []
        self._int8_max_vals = []
        self._int8_scales = []
        self._int8_output_fp32 = None
        torch.cuda.empty_cache()

    def get_config(self) -> BenchmarkConfig:
        return BenchmarkConfig(
            iterations=10,
            warmup=5,
            multi_gpu_required=self.multi_gpu,
        )

    def get_workload_metadata(self) -> Optional[WorkloadMetadata]:
        return self._workload

    def validate_result(self) -> Optional[str]:
        if not self.inputs:
            return "Inputs not initialized"
        return None

    def _synchronize_all(self) -> None:
        for device in self.devices or [self.device]:
            if torch.cuda.is_available():
                torch.cuda.synchronize(device)
