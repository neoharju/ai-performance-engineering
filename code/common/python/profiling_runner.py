"""Unified profiling runner for nsys and ncu.

Provides single interface for running nsys/ncu profiling with proper timeout handling,
error recovery, and artifact management.
"""

from __future__ import annotations

import inspect
import os
import shutil
import subprocess
import sys
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Protocol, TYPE_CHECKING, cast

# Use TYPE_CHECKING to avoid circular import at type-checking time
if TYPE_CHECKING:
    from common.python.benchmark_harness import Benchmark, BenchmarkConfig
    from common.python.metrics_extractor import NcuMetrics, NsysMetrics
else:
    NsysMetrics = Any
    NcuMetrics = Any

# Lazy initialization to avoid circular import at runtime
BenchmarkType = Any
BenchmarkConfigType = Any
BENCHMARK_AVAILABLE = False
_benchmark_types_initialized = False

def _init_benchmark_types():
    """Lazy initialization of benchmark types to avoid circular import."""
    global BenchmarkType, BenchmarkConfigType, BENCHMARK_AVAILABLE, _benchmark_types_initialized
    if _benchmark_types_initialized:
        return
    
    try:
        import common.python.benchmark_harness as _benchmark_harness
        BenchmarkType = _benchmark_harness.Benchmark
        BenchmarkConfigType = _benchmark_harness.BenchmarkConfig
        BENCHMARK_AVAILABLE = True
    except (ImportError, AttributeError):
        try:
            import common.python.benchmark_interface as _benchmark_interface
            import common.python.benchmark_config as _benchmark_config
            BenchmarkType = _benchmark_interface.Benchmark
            BenchmarkConfigType = _benchmark_config.BenchmarkConfig
            BENCHMARK_AVAILABLE = True
        except (ImportError, AttributeError):
            BENCHMARK_AVAILABLE = False
    
    _benchmark_types_initialized = True


class WrapperFactory(Protocol):
    def __call__(self, benchmark: BenchmarkType, benchmark_module: Any, benchmark_class: str, config: BenchmarkConfigType) -> Optional[Path]:
        ...

import torch

def _unavailable_wrapper(*args, **kwargs) -> Optional[Path]:
    return None

METRICS_EXTRACTOR_AVAILABLE = False
DEFAULT_PROFILER_CONFIG: Optional[Any] = None
create_benchmark_wrapper: WrapperFactory = _unavailable_wrapper

try:
    from common.python.metrics_extractor import extract_nsys_metrics, extract_ncu_metrics
    from common.python import profiler_config as _profiler_config
    from common.python.profiler_wrapper import create_benchmark_wrapper as _create_wrapper
    DEFAULT_PROFILER_CONFIG = _profiler_config.DEFAULT_PROFILER_CONFIG
    create_benchmark_wrapper = cast(WrapperFactory, _create_wrapper)
    METRICS_EXTRACTOR_AVAILABLE = True
except ImportError:
    def extract_nsys_metrics(nsys_rep_path: Path, timeout: int = 60) -> NsysMetrics:
        raise ImportError("metrics_extractor not available")
    
    def extract_ncu_metrics(ncu_rep_path: Path, timeout: int = 60) -> NcuMetrics:
        raise ImportError("metrics_extractor not available")


def check_nsys_available() -> bool:
    """Check if nsys is available on the system."""
    if shutil.which("nsys") is None:
        return False
    try:
        result = subprocess.run(
            ["nsys", "--version"],
            capture_output=True,
            timeout=5,
            check=False,
            env=os.environ.copy()
        )
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False


def check_ncu_available() -> bool:
    """Check if ncu is available on the system."""
    if shutil.which("ncu") is None:
        return False
    try:
        result = subprocess.run(
            ["ncu", "--version"],
            capture_output=True,
            timeout=5,
            check=False,
            env=os.environ.copy()
        )
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False


def run_nsys_profiling(
    benchmark: BenchmarkType,
    benchmark_module: Any,
    benchmark_class: str,
    output_dir: Path,
    config: BenchmarkConfigType,
    profiler_config: Optional[Any] = None,
    timeout_seconds: Optional[int] = None,
    metrics: Optional[str] = None
) -> Optional[Dict[str, Any]]:
    """Run nsys profiling on a benchmark.
    
    Args:
        benchmark: Benchmark instance to profile
        benchmark_module: Module containing the benchmark class
        benchmark_class: Name of benchmark class
        output_dir: Directory for profiling outputs
        config: Benchmark configuration
        profiler_config: Optional profiler configuration (uses default if None)
        timeout_seconds: Optional timeout for profiling run
        metrics: Optional metrics set name (e.g., 'roofline', 'deep_dive')
    
    Returns:
        Dictionary with 'profiling_outputs' and 'metrics' keys, or None on failure
    """
    _init_benchmark_types()
    if not BENCHMARK_AVAILABLE or not METRICS_EXTRACTOR_AVAILABLE:
        return None
    
    if profiler_config is None:
        profiler_config = DEFAULT_PROFILER_CONFIG
    if profiler_config is None:
        return None
    
    # Create wrapper script
    wrapper_script = create_benchmark_wrapper(
        benchmark, benchmark_module, benchmark_class, config
    )
    
    if not wrapper_script:
        return None
    
    try:
        # Create output path
        nsys_output = output_dir / f"nsys_{benchmark_class}"
        
        # Build nsys command using profiler_config
        nsys_command = profiler_config.get_nsys_command(
            str(nsys_output),
            str(wrapper_script),
        )
        
        # Run nsys
        result = subprocess.run(
            nsys_command,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            env=os.environ.copy()
        )
        
        if result.returncode != 0:
            return None
        
        # Extract metrics
        nsys_rep_path = Path(f"{nsys_output}.nsys-rep")
        if not nsys_rep_path.exists():
            return None
        
        metrics_obj = extract_nsys_metrics(nsys_rep_path)
        
        return {
            "profiling_outputs": {"nsys_rep": str(nsys_rep_path)},
            "metrics": metrics_obj
        }
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
        return None


def run_ncu_profiling(
    benchmark: BenchmarkType,
    benchmark_module: Any,
    benchmark_class: str,
    output_dir: Path,
    config: BenchmarkConfigType,
    profiler_config: Optional[Any] = None,
    timeout_seconds: Optional[int] = None,
    metrics: Optional[str] = None
) -> Optional[Dict[str, Any]]:
    """Run ncu profiling on a benchmark.
    
    Args:
        benchmark: Benchmark instance to profile
        benchmark_module: Module containing the benchmark class
        benchmark_class: Name of benchmark class
        output_dir: Directory for profiling outputs
        config: Benchmark configuration
        profiler_config: Optional profiler configuration (uses default if None)
        timeout_seconds: Optional timeout for profiling run
        metrics: Optional metrics set name (e.g., 'roofline', 'deep_dive')
    
    Returns:
        Dictionary with 'profiling_outputs' and 'metrics' keys, or None on failure
    """
    _init_benchmark_types()
    if not BENCHMARK_AVAILABLE or not METRICS_EXTRACTOR_AVAILABLE:
        return None
    
    if profiler_config is None:
        profiler_config = DEFAULT_PROFILER_CONFIG
    if profiler_config is None:
        return None
    
    # Create wrapper script
    wrapper_script = create_benchmark_wrapper(
        benchmark, benchmark_module, benchmark_class, config
    )
    
    if not wrapper_script:
        return None
    
    try:
        # Create output path
        ncu_output = output_dir / f"ncu_{benchmark_class}"
        
        # Build ncu command using profiler_config
        metrics_list = None
        if metrics:
            metrics_list = [metric.strip() for metric in metrics.split(",") if metric.strip()]
        ncu_command = profiler_config.get_ncu_command(
            str(ncu_output),
            str(wrapper_script),
            metrics=metrics_list
        )
        
        # Run ncu
        result = subprocess.run(
            ncu_command,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            env=os.environ.copy()
        )
        
        if result.returncode != 0:
            return None
        
        # Extract metrics
        ncu_rep_path = Path(f"{ncu_output}.ncu-rep")
        if not ncu_rep_path.exists():
            return None
        
        metrics_obj = extract_ncu_metrics(ncu_rep_path)
        
        return {
            "profiling_outputs": {"ncu_rep": str(ncu_rep_path)},
            "metrics": metrics_obj
        }
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
        return None


def run_profiling_orchestration(
    benchmark: BenchmarkType,
    config: BenchmarkConfigType,
    timing_fn: Callable[[Callable[..., Any], BenchmarkConfigType], List[float]],
    output_dir: Optional[Path] = None,
    profiler_config: Optional[Any] = None
) -> Dict[str, Any]:
    """Unified profiling orchestration: runs timing benchmark and optional nsys/ncu profiling.
    
    This function centralizes all profiling orchestration logic, including:
    - Running the timing benchmark
    - Creating profiling output directories
    - Running nsys profiling if enabled
    - Running ncu profiling if enabled
    - Extracting and returning metrics
    
    Args:
        benchmark: Benchmark instance to profile
        config: Benchmark configuration (with enable_nsys/enable_ncu flags)
        timing_fn: Function to run timing benchmark (takes benchmark_fn, config, returns List[float])
        output_dir: Optional output directory (defaults to "profiling_results")
        profiler_config: Optional profiler configuration
    
    Returns:
        Dictionary with:
        - 'times_ms': List[float] - timing results
        - 'profiling_outputs': Dict[str, str] - paths to profiling artifacts
        - 'nsys_metrics': Dict or Pydantic model - nsys metrics if available
        - 'ncu_metrics': Dict or Pydantic model - ncu metrics if available
    """
    _init_benchmark_types()
    if not BENCHMARK_AVAILABLE or not METRICS_EXTRACTOR_AVAILABLE:
        # Fallback: just run timing
        times_ms = timing_fn(benchmark.benchmark_fn, config)
        return {
            "times_ms": times_ms,
            "profiling_outputs": {},
            "nsys_metrics": {},
            "ncu_metrics": {},
        }
    
    # Create profiling output directory
    if output_dir is None:
        if config.profiling_output_dir:
            prof_dir = Path(config.profiling_output_dir)
        else:
            prof_dir = Path("profiling_results")
    else:
        prof_dir = Path(output_dir)
    
    prof_dir.mkdir(parents=True, exist_ok=True)
    
    # Get benchmark module and class info
    benchmark_module = inspect.getmodule(benchmark)
    benchmark_class = benchmark.__class__.__name__
    
    # If module is None, try to get it from the class
    if benchmark_module is None:
        benchmark_module = inspect.getmodule(benchmark.__class__)
    
    # Run timing benchmark first
    times_ms = timing_fn(benchmark.benchmark_fn, config)
    
    # Initialize return values
    profiling_outputs = {}
    nsys_metrics = {}
    ncu_metrics = {}
    
    # Run nsys profiling if enabled and available
    if config.enable_nsys and check_nsys_available():
        if benchmark_module is not None:
            # Use profiling_timeout_seconds if set, otherwise fall back to nsys_timeout_seconds
            profiling_timeout = getattr(config, 'get_effective_timeout', None)
            if profiling_timeout:
                nsys_timeout = profiling_timeout('profiling') or config.nsys_timeout_seconds
            else:
                nsys_timeout = getattr(config, 'profiling_timeout_seconds', None) or config.nsys_timeout_seconds
            nsys_result = run_nsys_profiling(
                benchmark, benchmark_module, benchmark_class, prof_dir, config,
                profiler_config=profiler_config,
                timeout_seconds=nsys_timeout
            )
            if nsys_result:
                profiling_outputs.update(nsys_result.get("profiling_outputs", {}))
                metrics_obj = nsys_result.get("metrics", {})
                if isinstance(metrics_obj, dict):
                    nsys_metrics = metrics_obj
                else:
                    nsys_metrics = metrics_obj.to_dict() if hasattr(metrics_obj, 'to_dict') else {}
    
    # Run ncu profiling if enabled and available
    if config.enable_ncu and check_ncu_available():
        if benchmark_module is not None:
            # Use profiling_timeout_seconds if set, otherwise fall back to ncu_timeout_seconds
            profiling_timeout = getattr(config, 'get_effective_timeout', None)
            if profiling_timeout:
                ncu_timeout = profiling_timeout('profiling') or config.ncu_timeout_seconds
            else:
                ncu_timeout = getattr(config, 'profiling_timeout_seconds', None) or config.ncu_timeout_seconds
            ncu_result = run_ncu_profiling(
                benchmark, benchmark_module, benchmark_class, prof_dir, config,
                profiler_config=profiler_config,
                timeout_seconds=ncu_timeout
            )
            if ncu_result:
                profiling_outputs.update(ncu_result.get("profiling_outputs", {}))
                metrics_obj = ncu_result.get("metrics", {})
                if isinstance(metrics_obj, dict):
                    ncu_metrics = metrics_obj
                else:
                    ncu_metrics = metrics_obj.to_dict() if hasattr(metrics_obj, 'to_dict') else {}
    
    return {
        "times_ms": times_ms,
        "profiling_outputs": profiling_outputs,
        "nsys_metrics": nsys_metrics,
        "ncu_metrics": ncu_metrics,
    }
