#pragma once

// DO NOT EDIT: Generated by tools/utilities/generate_arch_detection_header.py

#include <cuda_runtime.h>
#include <cstdint>
#include <cstdio>
#include <algorithm>
#include <initializer_list>

namespace cuda_arch {

struct TMALimits {
    uint32_t max_1d_box_size;
    uint32_t max_2d_box_width;
    uint32_t max_2d_box_height;
};

inline TMALimits get_tma_limits();

inline int get_max_shared_mem_per_block();

struct ArchitectureLimits {
    enum class TensorCoreGeneration {
        None,
        Ampere,
        Hopper,
        Blackwell
    };

    TMALimits tma{};
    int max_shared_mem_per_block = 48 * 1024;
    int max_shared_mem_per_sm = 0;
    int max_threads_per_block = 1024;
    int warp_size = 32;

    bool supports_clusters = false;
    int max_cluster_size = 1;

    TensorCoreGeneration tensor_core_gen = TensorCoreGeneration::None;
    int tensor_tile_m = 64;
    int tensor_tile_n = 64;
    int tensor_tile_k = 16;

    bool has_grace_coherence = false;
    bool has_nvlink_c2c = false;
    size_t kernel_parameter_limit = 4096;
};

struct TensorCoreTile {
    int m;
    int n;
    int k;
};

struct CapabilityData {
    int major;
    int minor;
    TMALimits tma;
    bool supports_clusters;
    bool has_dsmem;
    int max_cluster_size;
    ArchitectureLimits::TensorCoreGeneration tensor_core_gen;
    int tensor_tile_m;
    int tensor_tile_n;
    int tensor_tile_k;
    bool has_grace_coherence;
    bool has_nvlink_c2c;
};

inline constexpr CapabilityData kCapabilityTable[] = {
    {
        10, 0,
        {1024, 128, 128},
        true,
        false,
        8,
        ArchitectureLimits::TensorCoreGeneration::Blackwell,
        128, 128, 64,
        false,
        false
    }
};

inline const CapabilityData* find_capability(int major, int minor) {
    for (const auto& entry : kCapabilityTable) {
        if (entry.major == major && entry.minor == minor) {
            return &entry;
        }
    }
    return nullptr;
}

inline TMALimits get_tma_limits() {
    static TMALimits cached_limits = {0, 0, 0};
    if (cached_limits.max_1d_box_size != 0) {
        return cached_limits;
    }

    cudaDeviceProp props{};
    if (cudaGetDeviceProperties(&props, 0) != cudaSuccess) {
        cached_limits = {256, 64, 32};
        return cached_limits;
    }

    if (const CapabilityData* entry = find_capability(props.major, props.minor)) {
        cached_limits = entry->tma;
    } else {
        cached_limits = {256, 64, 32};
    }
    return cached_limits;
}

inline int get_max_shared_mem_per_block() {
    static int cached = -1;
    if (cached >= 0) {
        return cached;
    }

    int value = 0;
    if (cudaDeviceGetAttribute(&value, cudaDevAttrMaxSharedMemoryPerBlockOptin, 0) != cudaSuccess ||
        value == 0) {
        cudaDeviceGetAttribute(&value, cudaDevAttrMaxSharedMemoryPerBlock, 0);
    }

    if (value == 0) {
        value = 48 * 1024;
    }

    cached = value;
    return cached;
}

inline const ArchitectureLimits& get_architecture_limits() {
    static ArchitectureLimits cached{};
    static bool initialized = false;
    if (initialized) {
        return cached;
    }

    cudaDeviceProp props{};
    if (cudaGetDeviceProperties(&props, 0) != cudaSuccess) {
        cached.tma = get_tma_limits();
        initialized = true;
        return cached;
    }

    cached.tma = get_tma_limits();
    cached.max_shared_mem_per_block = get_max_shared_mem_per_block();
    cudaDeviceGetAttribute(&cached.max_shared_mem_per_sm, cudaDevAttrMaxSharedMemoryPerMultiprocessor, 0);
    cudaDeviceGetAttribute(&cached.max_threads_per_block, cudaDevAttrMaxThreadsPerBlock, 0);
    cudaDeviceGetAttribute(&cached.warp_size, cudaDevAttrWarpSize, 0);

    const CapabilityData* entry = find_capability(props.major, props.minor);
    if (entry) {
        cached.supports_clusters = entry->supports_clusters;
        cached.max_cluster_size = entry->max_cluster_size;
        cached.tensor_core_gen = entry->tensor_core_gen;
        cached.tensor_tile_m = entry->tensor_tile_m;
        cached.tensor_tile_n = entry->tensor_tile_n;
        cached.tensor_tile_k = entry->tensor_tile_k;
        cached.has_grace_coherence = entry->has_grace_coherence;
        cached.has_nvlink_c2c = entry->has_nvlink_c2c;
        cached.kernel_parameter_limit = entry->has_grace_coherence ? 32768 : 4096;
    } else {
        cached.kernel_parameter_limit = (props.major == 12 && props.minor >= 1) ? 32768 : 4096;
        if (props.major >= 10) {
            cached.supports_clusters = true;
            cached.max_cluster_size = 8;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::Blackwell;
            cached.tensor_tile_m = 128;
            cached.tensor_tile_n = 128;
            cached.tensor_tile_k = 64;
        } else if (props.major == 9) {
            cached.supports_clusters = true;
            cached.max_cluster_size = 4;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::Hopper;
            cached.tensor_tile_m = 128;
            cached.tensor_tile_n = 128;
            cached.tensor_tile_k = 64;
        } else if (props.major >= 8) {
            cached.supports_clusters = false;
            cached.max_cluster_size = 1;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::Ampere;
            cached.tensor_tile_m = 64;
            cached.tensor_tile_n = 64;
            cached.tensor_tile_k = 32;
        } else {
            cached.supports_clusters = false;
            cached.max_cluster_size = 1;
            cached.tensor_core_gen = ArchitectureLimits::TensorCoreGeneration::None;
            cached.tensor_tile_m = 32;
            cached.tensor_tile_n = 32;
            cached.tensor_tile_k = 16;
        }
        cached.has_grace_coherence = (props.major == 12 && props.minor >= 1);
        cached.has_nvlink_c2c = cached.has_grace_coherence;
    }

    initialized = true;
    return cached;
}

inline TensorCoreTile select_tensor_core_tile() {
    const auto& limits = get_architecture_limits();
    return {limits.tensor_tile_m, limits.tensor_tile_n, limits.tensor_tile_k};
}

template <typename T>
inline int select_square_tile_size(int shared_tiles,
                                   std::initializer_list<int> candidates,
                                   bool enforce_thread_bound = false) {
    const auto& limits = get_architecture_limits();
    int fallback = *std::min_element(candidates.begin(), candidates.end());

    for (int candidate : candidates) {
        std::size_t shared_bytes =
            static_cast<std::size_t>(shared_tiles) *
            static_cast<std::size_t>(candidate) *
            static_cast<std::size_t>(candidate) *
            sizeof(T);

        bool fits_shared = shared_bytes <= static_cast<std::size_t>(limits.max_shared_mem_per_block);
        bool fits_threads = !enforce_thread_bound || (candidate * candidate <= limits.max_threads_per_block);

        if (fits_shared && fits_threads) {
            return candidate;
        }
    }

    return fallback;
}

}  // namespace cuda_arch
