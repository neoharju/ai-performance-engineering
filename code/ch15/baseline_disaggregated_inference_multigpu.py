"""Baseline disaggregated inference benchmark (multi-GPU torchrun pipeline).

Chapter 15: Disaggregated Inference

This benchmark models a disaggregated prefill/decode pipeline across multiple GPUs.
Baseline behavior is serialized: prefill completes for all requests before decode starts.
The optimized pair overlaps prefill and decode via pipelined transfers.
"""

from __future__ import annotations

import argparse
import os
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple

import torch
import torch.distributed as dist

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from ch15.verification_payload_mixin import VerificationPayloadMixin  # noqa: E402
from core.benchmark.verification import PrecisionFlags  # noqa: E402
from core.harness.benchmark_harness import (  # noqa: E402
    BaseBenchmark,
    BenchmarkConfig,
    LaunchVia,
    TorchrunLaunchSpec,
)
from core.optimization.moe_inference import (  # noqa: E402
    MoeInferenceConfig,
    SimpleMoEGPT,
    allocate_kv_cache,
)

@dataclass(frozen=True)
class DisaggConfig:
    vocab_size: int = 16384
    hidden_size: int = 1024
    ffn_size: int = 4096
    num_layers: int = 8
    num_moe_layers: int = 4
    num_experts: int = 16
    top_k: int = 2
    batch_size: int = 1
    requests_per_rank: int = 4
    context_window: int = 512
    decode_tokens: int = 128
    dtype: torch.dtype = torch.bfloat16

    @property
    def tokens_per_request(self) -> int:
        return self.context_window + self.decode_tokens


@dataclass
class _LocalPair:
    prefill_device: torch.device
    decode_device: torch.device
    prefill_model: SimpleMoEGPT
    decode_model: SimpleMoEGPT
    prompts: torch.Tensor


def _build_moe_config(cfg: DisaggConfig) -> MoeInferenceConfig:
    return MoeInferenceConfig(
        vocab_size=cfg.vocab_size,
        hidden_size=cfg.hidden_size,
        ffn_size=cfg.ffn_size,
        num_layers=cfg.num_layers,
        num_moe_layers=cfg.num_moe_layers,
        num_experts=cfg.num_experts,
        top_k=cfg.top_k,
        moe_layer_frequency=1,
        batch_size=cfg.batch_size,
        context_window=cfg.context_window,
        decode_tokens=cfg.decode_tokens,
        router_noise=0.0,
        dtype=cfg.dtype,
    )


def _resolve_world_size() -> int:
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA required for multi-GPU disaggregation")
    world_size = torch.cuda.device_count()
    if world_size < 2:
        raise RuntimeError("disaggregated_inference_multigpu requires >=2 GPUs.")
    return world_size


def _init_distributed() -> Tuple[int, int, torch.device]:
    if not dist.is_available():
        raise RuntimeError("torch.distributed is required for multi-GPU disaggregation")
    if "LOCAL_RANK" not in os.environ:
        raise RuntimeError("Run with torchrun (missing LOCAL_RANK env var).")
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    if not dist.is_initialized():
        dist.init_process_group(backend="nccl", device_id=torch.device(f"cuda:{local_rank}"))
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    return rank, world_size, torch.device(f"cuda:{local_rank}")


def _run_prefill(
    cfg: DisaggConfig,
    model: SimpleMoEGPT,
    prompts: torch.Tensor,
) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
    kv_chunks: List[torch.Tensor] = []
    seed_chunks: List[torch.Tensor] = []
    with torch.no_grad():
        for req_idx in range(cfg.requests_per_rank):
            request_prompt = prompts[req_idx : req_idx + 1]
            hidden, logits = model.prefill(request_prompt)
            seed_tokens = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
            kv_chunks.append(hidden)
            seed_chunks.append(seed_tokens)
    return kv_chunks, seed_chunks


def _run_decode(
    cfg: DisaggConfig,
    model: SimpleMoEGPT,
    kv_chunks: List[torch.Tensor],
    seed_chunks: List[torch.Tensor],
    device: torch.device,
) -> List[torch.Tensor]:
    outputs: List[torch.Tensor] = []
    with torch.no_grad():
        for kv_prompt, seed_tokens in zip(kv_chunks, seed_chunks):
            kv_cache = allocate_kv_cache(
                cfg.batch_size,
                cfg.tokens_per_request,
                cfg.hidden_size,
                cfg.dtype,
                device,
            )
            kv_cache[:, : cfg.context_window] = kv_prompt
            tokens = seed_tokens
            for step in range(cfg.decode_tokens):
                _, decode_logits = model.decode(
                    tokens,
                    kv_cache=kv_cache,
                    position=cfg.context_window + step,
                )
                tokens = torch.argmax(decode_logits[:, -1, :], dim=-1, keepdim=True)
            outputs.append(tokens)
    return outputs


def _run_torchrun_worker(
    cfg: DisaggConfig,
    *,
    overlap: bool,
    label: str,
    iters: int,
    warmup: int,
) -> None:
    rank, world_size, device = _init_distributed()
    if world_size < 2:
        raise RuntimeError("disaggregated_inference_multigpu requires >=2 GPUs.")
    if torch.cuda.device_count() < world_size:
        raise RuntimeError(
            f"torchrun world_size={world_size} exceeds visible GPUs ({torch.cuda.device_count()})."
        )
    if world_size % 2 != 0:
        raise RuntimeError("world_size must be even (prefill ranks + decode ranks)")

    num_pairs = world_size // 2
    is_prefill = rank < num_pairs
    pair_id = rank if is_prefill else rank - num_pairs
    peer_rank = pair_id + num_pairs if is_prefill else pair_id
    pair_groups = [
        dist.new_group(ranks=[idx, idx + num_pairs]) for idx in range(num_pairs)
    ]
    device_index = 0 if device.index is None else int(device.index)
    comm_stream = torch.cuda.Stream(device=device, priority=1)

    def _barrier() -> None:
        dist.barrier(device_ids=[device_index])

    def _batch_isend(
        kv_cache: torch.Tensor,
        seed: torch.Tensor,
        *,
        ready_event: Optional[torch.cuda.Event] = None,
    ) -> List[dist.Work]:
        with torch.cuda.stream(comm_stream):
            if ready_event is not None:
                comm_stream.wait_event(ready_event)
            ops = [
                dist.P2POp(dist.isend, kv_cache, peer_rank, group=pair_groups[pair_id]),
                dist.P2POp(dist.isend, seed, peer_rank, group=pair_groups[pair_id]),
            ]
            return dist.batch_isend_irecv(ops)

    def _batch_irecv(
        kv_buf: torch.Tensor,
        seed_buf: torch.Tensor,
        *,
        record_event: bool = False,
    ) -> Tuple[List[dist.Work], Optional[torch.cuda.Event]]:
        with torch.cuda.stream(comm_stream):
            ops = [
                dist.P2POp(dist.irecv, kv_buf, peer_rank, group=pair_groups[pair_id]),
                dist.P2POp(dist.irecv, seed_buf, peer_rank, group=pair_groups[pair_id]),
            ]
            handles = dist.batch_isend_irecv(ops)
            ready_event = None
            if record_event:
                ready_event = torch.cuda.Event()
                ready_event.record()
            return handles, ready_event

    def _wait_handles(handles: List[dist.Work]) -> None:
        for req in handles:
            req.wait()

    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)

    moe_cfg = _build_moe_config(cfg)
    model = SimpleMoEGPT(moe_cfg, device=device).eval()

    prompts: Optional[torch.Tensor] = None
    if is_prefill:
        prompts = torch.randint(
            0,
            cfg.vocab_size,
            (cfg.requests_per_rank, cfg.context_window),
            device=device,
            dtype=torch.long,
        )

    recv_kv_buffers: List[torch.Tensor] = []
    recv_seed_buffers: List[torch.Tensor] = []
    kv_caches: List[torch.Tensor] = []
    if overlap and not is_prefill:
        recv_kv_buffers = [
            torch.empty(
                (cfg.batch_size, cfg.context_window, cfg.hidden_size),
                device=device,
                dtype=cfg.dtype,
            )
            for _ in range(cfg.requests_per_rank)
        ]
        recv_seed_buffers = [
            torch.empty((cfg.batch_size, 1), device=device, dtype=torch.long)
            for _ in range(cfg.requests_per_rank)
        ]
        kv_caches = [
            allocate_kv_cache(
                cfg.batch_size,
                cfg.tokens_per_request,
                cfg.hidden_size,
                cfg.dtype,
                device,
            )
            for _ in range(cfg.requests_per_rank)
        ]

    def run_iteration() -> List[torch.Tensor]:
        if is_prefill:
            if overlap:
                handles = []
                inflight_tensors: List[torch.Tensor] = []
                with torch.no_grad():
                    for req_idx in range(cfg.requests_per_rank):
                        request_prompt = prompts[req_idx : req_idx + 1]
                        hidden, logits = model.prefill(request_prompt)
                        seed_tokens = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
                        inflight_tensors.append(hidden)
                        inflight_tensors.append(seed_tokens)
                        ready = torch.cuda.Event()
                        ready.record()
                        handles.extend(_batch_isend(hidden, seed_tokens, ready_event=ready))
                _wait_handles(handles)
            else:
                kv_chunks, seed_chunks = _run_prefill(cfg, model, prompts)
                for kv_prompt, seed_tokens in zip(kv_chunks, seed_chunks):
                    ready = torch.cuda.Event()
                    ready.record()
                    handles = _batch_isend(kv_prompt, seed_tokens, ready_event=ready)
                    _wait_handles(handles)
            return []

        if overlap:
            recv_handles: List[dist.Work] = []
            recv_events: List[torch.cuda.Event] = []
            for kv_buf, seed_buf in zip(recv_kv_buffers, recv_seed_buffers):
                handles, ready_event = _batch_irecv(kv_buf, seed_buf, record_event=True)
                recv_handles.extend(handles)
                if ready_event is None:
                    raise RuntimeError("Missing ready event for overlap receive pipeline")
                recv_events.append(ready_event)
            outputs: List[torch.Tensor] = []
            compute_stream = torch.cuda.Stream(device=device)
            with torch.cuda.stream(compute_stream):
                with torch.no_grad():
                    for req_idx in range(cfg.requests_per_rank):
                        compute_stream.wait_event(recv_events[req_idx])
                        kv_cache = kv_caches[req_idx]
                        kv_cache[:, : cfg.context_window] = recv_kv_buffers[req_idx]
                        tokens = recv_seed_buffers[req_idx]
                        for step in range(cfg.decode_tokens):
                            _, decode_logits = model.decode(
                                tokens,
                                kv_cache=kv_cache,
                                position=cfg.context_window + step,
                            )
                            tokens = torch.argmax(decode_logits[:, -1, :], dim=-1, keepdim=True)
                        outputs.append(tokens)
            torch.cuda.current_stream(device).wait_stream(compute_stream)
            _wait_handles(recv_handles)
            return outputs

        kv_chunks: List[torch.Tensor] = []
        seed_chunks: List[torch.Tensor] = []
        recv_handles: List[dist.Work] = []
        for _ in range(cfg.requests_per_rank):
            kv_buf = torch.empty(
                (cfg.batch_size, cfg.context_window, cfg.hidden_size),
                device=device,
                dtype=cfg.dtype,
            )
            seed_buf = torch.empty(
                (cfg.batch_size, 1),
                device=device,
                dtype=torch.long,
            )
            kv_chunks.append(kv_buf)
            seed_chunks.append(seed_buf)
            handles, _ = _batch_irecv(kv_buf, seed_buf)
            recv_handles.extend(handles)
        _wait_handles(recv_handles)
        decoded = _run_decode(cfg, model, kv_chunks, seed_chunks, device)
        return decoded

    _barrier()
    torch.cuda.synchronize(device)

    for _ in range(max(warmup, 0)):
        run_iteration()
    torch.cuda.synchronize(device)
    _barrier()

    start = time.perf_counter()
    for _ in range(max(iters, 1)):
        run_iteration()
    torch.cuda.synchronize(device)
    _barrier()
    elapsed = time.perf_counter() - start

    if rank == 0:
        total_requests = cfg.requests_per_rank * num_pairs * cfg.batch_size
        tokens_per_iter = total_requests * cfg.tokens_per_request
        tokens_per_s = tokens_per_iter * (max(iters, 1) / max(elapsed, 1e-9))
        time_per_iter_ms = (elapsed / max(iters, 1)) * 1000.0
        print(f"rank0 {label} tokens/s: {tokens_per_s:.2f} tokens/s")
        print(f"rank0 {label} time_per_iter_ms: {time_per_iter_ms:.3f}")

    dist.destroy_process_group()


class _DisaggregatedInferenceMultiGPUBenchmark(VerificationPayloadMixin, BaseBenchmark):
    """Shared multi-GPU disaggregated inference harness."""

    def __init__(self, *, overlap: bool, label: str) -> None:
        super().__init__()
        self.cfg = DisaggConfig()
        self.world_size = _resolve_world_size()
        if self.world_size % 2 != 0:
            raise RuntimeError("world_size must be even for disaggregated inference")
        self.num_pairs = self.world_size // 2
        self.overlap = bool(overlap)
        self.label = label
        self._pairs: List[_LocalPair] = []
        self._output: Optional[torch.Tensor] = None
        self._verify_prompt: Optional[torch.Tensor] = None
        self._param_count: int = 0

        total_requests = self.cfg.requests_per_rank * self.num_pairs * self.cfg.batch_size
        tokens_per_iter = total_requests * self.cfg.tokens_per_request
        self.register_workload_metadata(
            requests_per_iteration=float(total_requests),
            tokens_per_iteration=float(tokens_per_iter),
        )

    def setup(self) -> None:
        if not torch.cuda.is_available():
            raise RuntimeError("SKIPPED: CUDA required for multi-GPU disaggregation")
        if torch.cuda.device_count() < self.world_size:
            raise RuntimeError(
                f"SKIPPED: requires >= {self.world_size} GPUs (found {torch.cuda.device_count()})"
            )

        torch.manual_seed(42)
        torch.cuda.manual_seed_all(42)

        moe_cfg = _build_moe_config(self.cfg)
        self._pairs = []
        total_params = 0
        for pair_id in range(self.num_pairs):
            prefill_device = torch.device(f"cuda:{pair_id}")
            decode_device = torch.device(f"cuda:{pair_id + self.num_pairs}")
            prefill_model = SimpleMoEGPT(moe_cfg, device=prefill_device).eval()
            decode_model = SimpleMoEGPT(moe_cfg, device=decode_device).eval()
            decode_model.load_state_dict(prefill_model.state_dict())
            prompts = torch.randint(
                0,
                self.cfg.vocab_size,
                (self.cfg.requests_per_rank, self.cfg.context_window),
                device=prefill_device,
                dtype=torch.long,
            )
            total_params += sum(p.numel() for p in prefill_model.parameters())
            total_params += sum(p.numel() for p in decode_model.parameters())
            self._pairs.append(
                _LocalPair(
                    prefill_device=prefill_device,
                    decode_device=decode_device,
                    prefill_model=prefill_model,
                    decode_model=decode_model,
                    prompts=prompts,
                )
            )

        self._param_count = total_params
        if not self._pairs:
            raise RuntimeError("Failed to initialize prompts for verification")
        self._verify_prompt = self._pairs[0].prompts
        for pair in self._pairs:
            torch.cuda.synchronize(pair.prefill_device)
            torch.cuda.synchronize(pair.decode_device)

    def benchmark_fn(self) -> None:
        if not self._pairs:
            raise RuntimeError("setup() must run before benchmark_fn()")

        outputs: List[torch.Tensor] = []
        with torch.no_grad():
            for pair in self._pairs:
                kv_chunks, seed_chunks = _run_prefill(self.cfg, pair.prefill_model, pair.prompts)
                decoded = _run_decode(
                    self.cfg,
                    pair.decode_model,
                    [kv.to(pair.decode_device, non_blocking=self.overlap) for kv in kv_chunks],
                    [seed.to(pair.decode_device, non_blocking=self.overlap) for seed in seed_chunks],
                    pair.decode_device,
                )
                outputs.extend(decoded)

        self._output = torch.cat([out.detach().cpu() for out in outputs], dim=0)

    def capture_verification_payload(self) -> None:
        if self._output is None or self._verify_prompt is None:
            raise RuntimeError("benchmark_fn() must run before capture_verification_payload()")
        tf32_enabled = torch.cuda.is_available() and bool(torch.backends.cuda.matmul.allow_tf32)
        meta_dtype = torch.float32
        self._set_verification_payload(
            inputs={
                "prompt": self._verify_prompt,
                "decode_tokens": torch.zeros((self.cfg.decode_tokens,), dtype=meta_dtype),
                "hidden_size": torch.zeros((self.cfg.hidden_size,), dtype=meta_dtype),
                "num_layers": torch.zeros((self.cfg.num_layers,), dtype=meta_dtype),
                "num_experts": torch.zeros((self.cfg.num_experts,), dtype=meta_dtype),
            },
            output=self._output,
            batch_size=int(self._output.shape[0]),
            parameter_count=int(self._param_count),
            precision_flags=PrecisionFlags(bf16=True, tf32=tf32_enabled),
            output_tolerance=(0.0, 0.0),
            signature_overrides={
                "world_size": self.world_size,
                "pipeline_stages": 2,
                "pipeline_stage_boundaries": [
                    (0, self.num_pairs - 1),
                    (self.num_pairs, self.world_size - 1),
                ],
                "per_rank_batch_size": self.cfg.requests_per_rank,
                "collective_type": "send_recv",
            },
        )

    def _prepare_verification_payload(self) -> None:
        if hasattr(self, "_subprocess_verify_output"):
            return
        self.setup()
        try:
            self.benchmark_fn()
            self.capture_verification_payload()
            self._subprocess_verify_output = self.get_verify_output()
            self._subprocess_output_tolerance = self.get_output_tolerance()
            self._subprocess_input_signature = self.get_input_signature()
        finally:
            self.teardown()

    def teardown(self) -> None:
        self._pairs = []
        self._output = None
        self._verify_prompt = None
        torch.cuda.empty_cache()

    def validate_result(self) -> Optional[str]:
        if self._output is None:
            return "No output captured"
        return None

    def get_config(self) -> BenchmarkConfig:
        return BenchmarkConfig(
            launch_via=LaunchVia.TORCHRUN,
            nproc_per_node=self.world_size,
            iterations=4,
            warmup=5,
            multi_gpu_required=True,
            measurement_timeout_seconds=900,
        )

    def get_torchrun_spec(self, config: Optional[BenchmarkConfig] = None) -> TorchrunLaunchSpec:
        self._prepare_verification_payload()
        master_port = os.environ.get("MASTER_PORT", "29515")
        return TorchrunLaunchSpec(
            script_path=Path(__file__).resolve(),
            script_args=[],
            env={
                "OMP_NUM_THREADS": "1",
                "MASTER_PORT": master_port,
                "TORCH_NCCL_SHOW_EAGER_INIT_P2P_SERIALIZATION_WARNING": "0",
            },
            parse_rank0_only=True,
            multi_gpu_required=True,
            name=self.label,
            config_arg_map={
                "iterations": "--iters",
                "warmup": "--warmup",
            },
        )



class BaselineDisaggregatedInferenceMultiGPUBenchmark(_DisaggregatedInferenceMultiGPUBenchmark):
    """Serialized prefill then decode across multi-GPU ranks."""

    def __init__(self) -> None:
        super().__init__(overlap=False, label="baseline_disaggregated_inference_multigpu")


def get_benchmark() -> BaseBenchmark:
    return BaselineDisaggregatedInferenceMultiGPUBenchmark()


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument("--iters", type=int, default=4)
    parser.add_argument("--warmup", type=int, default=3)
    return parser.parse_args()


def main() -> None:
    args = _parse_args()
    _run_torchrun_worker(
        DisaggConfig(),
        overlap=False,
        label="baseline_disaggregated_inference_multigpu",
        iters=int(args.iters),
        warmup=int(args.warmup),
    )


if __name__ == "__main__":
    main()
