"""Baseline harness for Chapter 17 dynamic routing."""

from __future__ import annotations

import random
import statistics
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

import torch

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from core.harness.benchmark_harness import (  # noqa: E402
    BaseBenchmark,
    BenchmarkConfig,
    WorkloadMetadata,
)
from ch17.dynamic_routing import DisaggregatedRouter, Priority, Request, WorkerMetrics  # noqa: E402


class _DynamicRoutingBenchmark(BaseBenchmark):
    """Shared logic for baseline/optimized routing harnesses."""

    def __init__(self, *, batch_size: int, vectorized: bool):
        super().__init__()
        self.batch_size = batch_size
        self.vectorized = vectorized
        self.router = DisaggregatedRouter()
        self._history: Dict[str, List[float]] = {"lat_ms": []}
        self._workload = WorkloadMetadata(
            requests_per_iteration=float(batch_size),
            tokens_per_iteration=float(batch_size * 128),
        )
        self.output = None
        self._iteration = 0
        self._queue_length_table: Optional[torch.Tensor] = None
        self.register_workload_metadata(
            requests_per_iteration=float(batch_size),
            tokens_per_iteration=float(batch_size * 128),
        )
        # Pre-generated requests (created once in setup, reused in benchmark)
        self._cached_requests: List[Request] = []
        # Pre-allocated tensors for vectorized path (reused each iteration)
        self._prompt_lengths: Optional[torch.Tensor] = None
        self._cached_lengths: Optional[torch.Tensor] = None
        self._queue_lengths: Optional[torch.Tensor] = None
        self._priorities: Optional[torch.Tensor] = None

    def setup(self) -> None:
        random.seed(42)
        torch.manual_seed(42)
        torch.cuda.manual_seed_all(42)
        now = time.time()
        for idx in range(4):
            self.router.prefill_workers[f"prefill-{idx}"] = self._make_metrics(queue=idx, now=now)
            self.router.decode_workers[f"decode-{idx}"] = self._make_metrics(queue=idx // 2, now=now)
        
        # Pre-generate requests once (not part of benchmark timing)
        self._cached_requests = self._generate_requests()
        
        # Pre-allocate tensors for vectorized routing (avoids allocation in hot path)
        if self.vectorized:
            self._prompt_lengths = torch.tensor(
                [len(r.prompt_tokens) for r in self._cached_requests], dtype=torch.int32
            )
            self._cached_lengths = torch.tensor(
                [r.prefix_cached_length for r in self._cached_requests], dtype=torch.int32
            )
            self._queue_lengths = torch.zeros(self.batch_size, dtype=torch.int32)
            self._priorities = torch.tensor(
                [0 if r.priority is Priority.LOW else (2 if r.priority is Priority.HIGH else 1) 
                 for r in self._cached_requests], dtype=torch.int32
            )
        cfg = self.get_config()
        num_iters = (cfg.warmup or 0) + (cfg.iterations or 0) + 5
        high = self.router.PREFILL_QUEUE_MAX + 6  # match baseline randint upper bound
        self._queue_length_table = torch.randint(
            0,
            high,
            (num_iters, self.batch_size),
            dtype=torch.int32,
        )
        self._iteration = 0

    def _make_metrics(self, queue: int, now: float):
        return WorkerMetrics(
            queue_length=queue,
            gpu_utilization=random.uniform(0.4, 0.8),
            memory_usage=random.uniform(30.0, 70.0),
            kv_cache_usage=random.uniform(10.0, 50.0),
            active_requests=random.randint(1, 4),
            last_updated=now,
        )

    def _generate_requests(self) -> List[Request]:
        reqs: List[Request] = []
        for idx in range(self.batch_size):
            prompt_len = random.randint(64, 2048)
            cached = random.randint(0, min(prompt_len // 2, 512))
            reqs.append(
                Request(
                    id=f"req-{idx}",
                    prompt_tokens=list(range(prompt_len)),
                    priority=random.choice(list(Priority)),
                    timestamp=time.time(),
                    prefix_cached_length=cached,
                    expected_output_length=random.randint(16, 128),
                )
            )
        return reqs

    def benchmark_fn(self) -> Dict[str, float]:
        # Use pre-generated requests (generation time excluded from benchmark)
        requests = self._cached_requests
        rejects = 0
        offloaded = 0
        start = self._record_start()
        queue_lengths: Optional[torch.Tensor] = None
        if self._queue_length_table is not None:
            table_idx = self._iteration % self._queue_length_table.shape[0]
            queue_lengths = self._queue_length_table[table_idx]
        self._iteration += 1

        if self.vectorized and self._prompt_lengths is not None:
            # Vectorized routing decisions using pre-allocated tensors
            if (
                queue_lengths is None
                or self._queue_lengths is None
                or self._cached_lengths is None
                or self._priorities is None
            ):
                raise RuntimeError("Vectorized routing buffers not initialized")
            self._queue_lengths.copy_(queue_lengths)

            # Vectorized boolean operations (single pass over data)
            long_prefill = (self._prompt_lengths - self._cached_lengths) > self.router.PREFILL_LENGTH_THRESHOLD
            capacity = self._queue_lengths < self.router.PREFILL_QUEUE_MAX
            offload_mask = long_prefill & capacity

            est_ttft = (
                self.router.get_current_prefill_queue_length() * self.router.avg_prefill_time_per_req
                + self.router.get_current_decode_queue_length() * self.router.avg_decode_time_per_req
            )
            admit_mask = torch.ones_like(self._priorities, dtype=torch.bool)
            if est_ttft > self.router.TTFT_SLO_MAX:
                admit_mask = self._priorities != 0  # reject low-priority requests under high load

            rejects = int((~admit_mask).sum().item())
            offloaded = int(torch.logical_and(admit_mask, offload_mask).sum().item())
        else:
            # Python loop-based routing (sequential, one-at-a-time)
            for idx, req in enumerate(requests):
                if not self.router.admit_request(req):
                    rejects += 1
                    continue
                if queue_lengths is None:
                    queue_depth = random.randint(0, self.router.PREFILL_QUEUE_MAX + 5)
                else:
                    queue_depth = int(queue_lengths[idx % queue_lengths.numel()].item())
                if self.router.should_offload_prefill(len(req.prompt_tokens), req.prefix_cached_length, queue_depth):
                    offloaded += 1

        elapsed_ms = self._record_stop(start)
        self._history["lat_ms"].append(elapsed_ms)
        served = len(requests) - rejects

        self.output = torch.tensor([float(served), float(rejects), float(offloaded)])
        return {
            "requests": float(len(requests)),
            "served": float(served),
            "rejected": float(rejects),
            "offloaded": float(offloaded),
        }

    def get_config(self) -> BenchmarkConfig:
        return BenchmarkConfig(iterations=8, warmup=5)

    def get_workload_metadata(self) -> Optional[WorkloadMetadata]:
        return self._workload

    def get_custom_metrics(self) -> Optional[Dict[str, float]]:
        if not self._history["lat_ms"]:
            return None
        return {
            "routing.latency_ms": float(statistics.mean(self._history["lat_ms"])),
        }

    def get_verify_output(self) -> torch.Tensor:
        """Return output tensor for verification comparison."""
        if self.output is None:
            raise RuntimeError("Output not available - run benchmark first")
        return self.output

    def get_input_signature(self) -> dict:
        """Return workload signature for input verification."""
        return {"batch_size": self.batch_size, "vectorized": self.vectorized}

    def get_output_tolerance(self) -> tuple:
        """Return tolerance for numerical comparison - routing is deterministic."""
        return (0.1, 10.0)

    def validate_result(self) -> Optional[str]:
        return None


class BaselineDynamicRoutingBenchmark(_DynamicRoutingBenchmark):
    """Python loop-based routing decisions."""
    def __init__(self) -> None:
        # Match optimized batch size for fair comparison
        super().__init__(batch_size=1024, vectorized=False)


def get_benchmark():
    return BaselineDynamicRoutingBenchmark()


if __name__ == "__main__":
    from core.harness.benchmark_harness import benchmark_main
    benchmark_main(get_benchmark)
