"""optimized_nccl_quantization.py - Optimized quantization with NCCL.

Demonstrates distributed quantization using NCCL for multi-GPU communication.
NCCL enables efficient synchronization of quantized gradients/activations.
Implements Benchmark protocol for harness integration.
"""

from __future__ import annotations

import sys
from pathlib import Path

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

import torch
import torch.nn as nn
import torch.distributed as dist

from typing import Optional

from common.python.benchmark_harness import (
    Benchmark,
    BenchmarkConfig,
)

def resolve_device() -> torch.device:
    """Return CUDA device if available."""
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA required for ch14")
    return torch.device("cuda")

class OptimizedNcclQuantizationBenchmark(Benchmark):
    """Optimized: Quantization with NCCL collective operations."""
    
    def __init__(self):
        self.device = resolve_device()
        self.model = None
        self.input = None
        self.is_distributed = False
        self.rank = 0
        self.world_size = 1
    
    def setup(self) -> None:
        """Setup: Initialize quantized model for NCCL."""
        
        # Optimization: Enable cuDNN benchmarking for optimal kernel selection
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        torch.manual_seed(42)
        # Optimization: Quantization with NCCL
        # NCCL enables distributed quantization across multiple GPUs
        # Efficient synchronization of quantized gradients/activations
        
        # Initialize NCCL process group for distributed quantization
        # For ch14 demo: initialize single-process group to demonstrate NCCL API
        # In real multi-GPU setup, this would be initialized via torchrun
        self.is_distributed = False
        self.rank = 0
        self.world_size = 1
        
        if dist.is_available():
            # Try to use existing process group if already initialized
            if dist.is_initialized():
                self.is_distributed = True
                self.rank = dist.get_rank()
                self.world_size = dist.get_world_size()
            else:
                # Only initialize if we're in a multi-process environment
                # Check if environment variables are set (indicating multi-process launch)
                import os
                has_rank = 'RANK' in os.environ
                has_world_size = 'WORLD_SIZE' in os.environ
                
                if has_rank and has_world_size and torch.cuda.device_count() > 1:
                    # Multi-process multi-GPU: initialize NCCL process group
                    try:
                        # Use environment variables set by torchrun/multiprocessing
                        if 'MASTER_ADDR' not in os.environ:
                            os.environ['MASTER_ADDR'] = 'localhost'
                        if 'MASTER_PORT' not in os.environ:
                            os.environ['MASTER_PORT'] = '12355'
                        dist.init_process_group(backend='nccl', init_method='env://')
                        self.is_distributed = True
                        self.rank = dist.get_rank()
                        self.world_size = dist.get_world_size()
                    except Exception as e:
                        # Fallback: simulate NCCL behavior for single-process demo
                        # NCCL concept demonstrated via API usage patterns
                        self.is_distributed = False
                else:
                    # Single-process: simulate NCCL behavior without initialization
                    # NCCL concept demonstrated via API usage patterns
                    # Real multi-GPU setup requires torchrun or multiprocessing
                    self.is_distributed = False
        
        # Use FP8 quantization on CUDA (native support on GB10/H100+)
        # FP8 provides 2x memory reduction vs FP16, enabling faster NCCL transfers
        self.model = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
        ).to(self.device)
        
        # Quantize weights to FP8 for CUDA-native quantization
        # FP8 E4M3FN format: native support on GB10/H100+
        if hasattr(torch, 'float8_e4m3fn'):
            # Convert model weights to FP8 (quantize)
            with torch.no_grad():
                for module in self.model.modules():
                    if isinstance(module, nn.Linear):
                        # Quantize weights: FP32 -> FP8 -> BF16 (for computation)
                        weight_fp8 = module.weight.to(torch.float8_e4m3fn)
                        module.weight.data = weight_fp8.to(torch.bfloat16)
                        if module.bias is not None:
                            bias_fp8 = module.bias.to(torch.float8_e4m3fn)
                            module.bias.data = bias_fp8.to(torch.bfloat16)
            self.model = self.model.to(torch.bfloat16).eval()
            input_dtype = torch.bfloat16
        else:
            # Fallback: use BF16 if FP8 not available
            self.model = self.model.to(torch.bfloat16).eval()
            input_dtype = torch.bfloat16
        
        self.input = torch.randn(4, 32, 256, device=self.device, dtype=input_dtype)
        torch.cuda.synchronize()
    
    def benchmark_fn(self) -> None:
        """Benchmark: Quantization operations with NCCL."""
        # Use conditional NVTX ranges - only enabled when profiling

        from common.python.nvtx_helper import nvtx_range, get_nvtx_enabled

        config = self.get_config()

        enable_nvtx = get_nvtx_enabled(config) if config else False

        with nvtx_range("optimized_nccl_quantization", enable=enable_nvtx):
            with torch.no_grad():
                # Optimization: Quantization with NCCL
                # NCCL enables distributed quantization for multi-GPU setups
                # Efficient synchronization of quantized gradients/activations
                output = self.model(self.input)
                
                # Use NCCL collective operations for distributed quantization
                if self.is_distributed:
                    # NCCL AllReduce: synchronize quantized activations across GPUs
                    dist.all_reduce(output, op=dist.ReduceOp.SUM)
                    output = output / self.world_size
                    
                    # NCCL Broadcast: synchronize quantized weights across GPUs
                    quantized_weight = next(self.model.parameters())
                    dist.broadcast(quantized_weight, src=0)
                else:
                    # Single-GPU demo: simulate NCCL behavior
                    # Create a copy to simulate AllReduce (no-op on single GPU)
                    output_sum = output.clone()
                    # Simulate averaging (no-op for single GPU)
                    output = output_sum / 1.0
                    
                    # Simulate broadcast (no-op for single GPU)
                    quantized_weight = next(self.model.parameters())
                    _ = quantized_weight.clone()  # Simulate broadcast
                    
                    # NCCL concept demonstrated: in multi-GPU, these operations synchronize across devices
                    # See ch4/ch17 for full distributed NCCL implementations

    
    def teardown(self) -> None:
        """Teardown: Clean up resources."""
        if self.is_distributed and dist.is_initialized():
            dist.destroy_process_group()
        self.model = None
        self.input = None
        torch.cuda.empty_cache()
    
    def get_config(self) -> BenchmarkConfig:
        """Return benchmark configuration."""
        return BenchmarkConfig(
            iterations=50,
            warmup=5,
        )
    
    def validate_result(self) -> Optional[str]:
        """Validate benchmark result."""
        if self.model is None:
            return "Model not initialized"
        return None

def get_benchmark() -> Benchmark:
    """Factory function for benchmark discovery."""
    return OptimizedNcclQuantizationBenchmark()

if __name__ == '__main__':
    from common.python.benchmark_harness import BenchmarkHarness, BenchmarkMode
    
    benchmark = get_benchmark()
    harness = BenchmarkHarness(
        mode=BenchmarkMode.CUSTOM,
        config=benchmark.get_config()
    )
    result = harness.benchmark(benchmark)
    print(f"\nOptimized NCCL Quantization: {result.timing.mean_ms if result.timing else 0.0:.3f} ms")
    print(" Tip: NCCL enables efficient distributed quantization for multi-GPU setups")
