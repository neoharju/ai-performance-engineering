"""optimized_cuda_python.py - Native CUDA Python kernels (cuda.core API).

This demonstrates native CUDA Python as a first-class GPU programming
option in PyTorch 2.10+, providing an alternative to Triton for cases
where lower-level control is needed without dropping to CUDA C++.

Native CUDA Python offers:
- Direct kernel authoring with Python syntax
- JIT compilation via nvrtc
- cuda.core Device/Stream/Memory APIs
- cuda.bindings for low-level CUDA access
- Better integration than raw pycuda/cupy

Use cases vs Triton:
- CUDA Python: Maximum control, custom memory patterns, legacy CUDA port
- Triton: Portable, auto-tuned, higher abstraction

Use cases vs torch.compile:
- CUDA Python: Custom ops not expressible in PyTorch
- torch.compile: Standard PyTorch model optimization
"""

from __future__ import annotations

import sys
from pathlib import Path

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

import torch
from typing import Optional
import math

from core.harness.benchmark_harness import (
    BaseBenchmark,
    BenchmarkConfig,
    BenchmarkHarness,
    BenchmarkMode,
    WorkloadMetadata,
)


# Require CUDA Python - no fallbacks
# cuda-python 13.x uses cuda.bindings instead of cuda.cuda
try:
    from cuda.bindings import driver as cuda_driver
    from cuda.bindings import nvrtc
    CUDA_PYTHON_AVAILABLE = True
except ImportError:
    raise ImportError(
        "cuda-python is required for this benchmark. Install with:\n"
        "  pip install cuda-python\n"
        "Or run: bash setup.sh"
    )

# Check for cuda.core (newer experimental API)
CUDA_CORE_AVAILABLE = False
try:
    from cuda.core.experimental import Device, LaunchConfig, launch
    from cuda.core.experimental.utils import compile_ptx
    CUDA_CORE_AVAILABLE = True
except ImportError:
    pass  # cuda.core is optional, cuda.bindings.driver is sufficient


# Fused LayerNorm + GELU + Mask kernel in CUDA C (for nvrtc compilation)
FUSED_KERNEL_SOURCE = r'''
#include <cuda_fp16.h>

extern "C" __global__ void fused_layernorm_gelu_mask(
    const half* __restrict__ input,
    const half* __restrict__ weight,
    const half* __restrict__ bias,
    const bool* __restrict__ mask,
    half* __restrict__ output,
    const half* __restrict__ residual,
    int batch_size,
    int seq_len,
    int hidden_size,
    float eps
) {
    // Each block handles one (batch, seq) position
    int batch_idx = blockIdx.x;
    int seq_idx = blockIdx.y;
    int tid = threadIdx.x;
    
    if (batch_idx >= batch_size || seq_idx >= seq_len) return;
    
    // Check mask early
    bool is_masked = !mask[batch_idx * seq_len + seq_idx];
    
    // Base pointer for this position
    int base_idx = (batch_idx * seq_len + seq_idx) * hidden_size;
    
    // Shared memory for reduction
    extern __shared__ float smem[];
    float* s_sum = smem;
    float* s_sum_sq = smem + blockDim.x;
    
    // Step 1: Compute mean and variance (parallel reduction)
    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;
    
    for (int i = tid; i < hidden_size; i += blockDim.x) {
        float val = __half2float(input[base_idx + i]);
        local_sum += val;
        local_sum_sq += val * val;
    }
    
    s_sum[tid] = local_sum;
    s_sum_sq[tid] = local_sum_sq;
    __syncthreads();
    
    // Block reduction
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            s_sum[tid] += s_sum[tid + stride];
            s_sum_sq[tid] += s_sum_sq[tid + stride];
        }
        __syncthreads();
    }
    
    float mean = s_sum[0] / hidden_size;
    float var = s_sum_sq[0] / hidden_size - mean * mean;
    float inv_std = rsqrtf(var + eps);
    
    // Step 2: Normalize, scale, bias, GELU, mask, residual - all fused
    for (int i = tid; i < hidden_size; i += blockDim.x) {
        float x = __half2float(input[base_idx + i]);
        float w = __half2float(weight[i]);
        float b = __half2float(bias[i]);
        float res = __half2float(residual[base_idx + i]);
        
        // LayerNorm
        float normed = (x - mean) * inv_std * w + b;
        
        // GELU: x * 0.5 * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
        float gelu;
        {
            float x3 = normed * normed * normed;
            float inner = 0.7978845608f * (normed + 0.044715f * x3);
            gelu = 0.5f * normed * (1.0f + tanhf(inner));
        }
        
        // Apply mask and residual
        float result = is_masked ? res : (gelu + res);
        
        output[base_idx + i] = __float2half(result);
    }
}
'''


class OptimizedCudaPythonBenchmark(BaseBenchmark):
    """Optimized: Native CUDA Python with fused kernel.
    
    Demonstrates CUDA Python first-class support:
    
    1. **Kernel Source in Python**: Write CUDA C as a string, compile with nvrtc
    2. **cuda.core Device API**: Modern Python interface to CUDA
    3. **Fused Operations**: LayerNorm + GELU + Mask + Residual in one kernel
    4. **Zero Intermediate Tensors**: Direct inputâ†’output with no allocations
    
    Benefits:
    - Single kernel launch (vs 4 in baseline)
    - No intermediate tensor allocations
    - Custom memory access patterns
    - Full control over shared memory usage
    
    Expected speedup: 1.5-2.5x over unfused PyTorch
    """
    
    def __init__(self):
        super().__init__()
        self.batch = 64
        self.seq_len = 2048
        self.hidden = 4096
        self.input = None
        self.weight = None
        self.bias = None
        self.output = None
        self.kernel = None
        self.kernel_compiled = False
        tokens = self.batch * self.seq_len
        self._workload = WorkloadMetadata(
            requests_per_iteration=float(self.batch),
            tokens_per_iteration=float(tokens),
        )
        self.register_workload_metadata(
            requests_per_iteration=float(self.batch),
            tokens_per_iteration=float(tokens),
        )
        self._flops = 0
    
    def _compile_kernel(self) -> bool:
        """Compile the fused kernel using nvrtc."""
        if not CUDA_PYTHON_AVAILABLE:
            return False
        
        try:
            import os
            
            # Create program
            err, prog = nvrtc.nvrtcCreateProgram(
                FUSED_KERNEL_SOURCE.encode(), 
                b"fused_kernel.cu",
                0, None, None
            )
            if err != nvrtc.nvrtcResult.NVRTC_SUCCESS:
                return False
            
            # Get compute capability
            cc = torch.cuda.get_device_capability()
            arch_flag = f"--gpu-architecture=sm_{cc[0]}{cc[1]}".encode()
            
            # Find CUDA include path
            cuda_home = os.environ.get('CUDA_HOME', '/usr/local/cuda')
            include_flag = f"-I{cuda_home}/include".encode()
            
            # Compile with include path
            opts = [arch_flag, include_flag, b"--use_fast_math"]
            err, = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)
            
            if err != nvrtc.nvrtcResult.NVRTC_SUCCESS:
                # Get log for debugging
                err, log_size = nvrtc.nvrtcGetProgramLogSize(prog)
                log = b" " * log_size
                nvrtc.nvrtcGetProgramLog(prog, log)
                print(f"Compilation failed: {log.decode()}")
                return False
            
            # Get PTX
            err, ptx_size = nvrtc.nvrtcGetPTXSize(prog)
            ptx = b" " * ptx_size
            err, = nvrtc.nvrtcGetPTX(prog, ptx)
            
            # Load module
            err, ctx = cuda_driver.cuCtxGetCurrent()
            err, module = cuda_driver.cuModuleLoadData(ptx)
            err, self.kernel = cuda_driver.cuModuleGetFunction(
                module, b"fused_layernorm_gelu_mask"
            )
            
            nvrtc.nvrtcDestroyProgram(prog)
            return True
            
        except Exception as e:
            print(f"Kernel compilation error: {e}")
            return False
    
    def setup(self) -> None:
        """Setup: Initialize tensors and compile kernel."""
        torch.manual_seed(42)
        
        # Input tensor (batch, seq, hidden)
        self.input = torch.randn(
            self.batch, self.seq_len, self.hidden, 
            device=self.device, dtype=torch.float16
        )
        
        # LayerNorm parameters
        self.weight = torch.ones(self.hidden, device=self.device, dtype=torch.float16)
        self.bias = torch.zeros(self.hidden, device=self.device, dtype=torch.float16)
        
        # Mask for sparse operations
        self.mask = torch.rand(self.batch, self.seq_len, device=self.device) > 0.3
        
        # Output buffer
        self.output = torch.zeros_like(self.input)
        
        # Try to compile CUDA Python kernel
        self.kernel_compiled = self._compile_kernel()
        
        # Calculate FLOPs
        n_elements = self.batch * self.seq_len * self.hidden
        self._flops = 15 * n_elements
        
        torch.cuda.synchronize(self.device)
    
    def _run_fused_kernel(self) -> None:
        """Launch the fused CUDA Python kernel."""
        if not self.kernel_compiled:
            return
        
        # Kernel launch configuration
        block_size = min(256, self.hidden)
        grid = (self.batch, self.seq_len, 1)
        block = (block_size, 1, 1)
        shared_mem = 2 * block_size * 4  # For reduction
        
        # Prepare kernel arguments
        eps = 1e-5
        args = [
            self.input.data_ptr(),
            self.weight.data_ptr(),
            self.bias.data_ptr(),
            self.mask.data_ptr(),
            self.output.data_ptr(),
            self.input.data_ptr(),  # residual = input
            self.batch,
            self.seq_len,
            self.hidden,
            eps,
        ]
        
        # Pack arguments for cuLaunchKernel
        import ctypes
        arg_ptrs = []
        for arg in args:
            if isinstance(arg, int):
                ptr = ctypes.c_void_p(arg)
            elif isinstance(arg, float):
                ptr = ctypes.c_float(arg)
            else:
                ptr = ctypes.c_void_p(arg)
            arg_ptrs.append(ptr)
        
        # Launch kernel
        cuda_driver.cuLaunchKernel(
            self.kernel,
            grid[0], grid[1], grid[2],
            block[0], block[1], block[2],
            shared_mem,
            0,  # default stream
            (ctypes.c_void_p * len(arg_ptrs))(*[ctypes.addressof(p) for p in arg_ptrs]),
            0
        )
    
    
    def benchmark_fn(self) -> None:
        """Benchmark: Fused CUDA Python kernel.
        
        Single kernel launch that performs:
        1. LayerNorm (reduction + normalization)
        2. GELU activation
        3. Mask application
        4. Residual addition
        
        All in one pass over memory - optimal for memory-bound ops.
        """
        if not self.kernel_compiled:
            raise RuntimeError(
                "CUDA Python kernel failed to compile. "
                "Ensure cuda-python is properly installed and CUDA is available."
            )
        
        with self._nvtx_range("optimized_cuda_python"):
            self._run_fused_kernel()
        
        self._synchronize()
    
    def teardown(self) -> None:
        """Teardown: Clean up resources."""
        self.input = None
        self.weight = None
        self.bias = None
        self.mask = None
        self.output = None
        self.kernel = None
        torch.cuda.empty_cache()
    
    def get_config(self) -> BenchmarkConfig:
        """Return benchmark configuration."""
        return BenchmarkConfig(
            iterations=100,
            warmup=10,
        )
    
    def get_workload_metadata(self) -> Optional[WorkloadMetadata]:
        return self._workload
    
    def get_custom_metrics(self) -> Optional[dict]:
        """Return domain-specific metrics using standardized helper."""
        from core.benchmark.metrics import compute_triton_metrics
        return compute_triton_metrics(
            num_elements=getattr(self, 'N', getattr(self, 'num_elements', 1024)),
            elapsed_ms=getattr(self, '_last_elapsed_ms', 1.0),
            block_size=getattr(self, 'BLOCK_SIZE', 1024),
            num_warps=getattr(self, 'num_warps', 4),
        )

    def validate_result(self) -> Optional[str]:
        """Validate benchmark result."""
        if self.input is None:
            return "Input not initialized"
        if self.output is None:
            return "Output not computed"
        if torch.isnan(self.output).any():
            return "Output contains NaN"
        return None

    def get_verify_output(self) -> torch.Tensor:
        """Return output tensor for verification comparison."""
        if self.output is None:
            raise RuntimeError("Output not available - run benchmark first")
        return self.output.float()

    def get_input_signature(self) -> dict:
        """Return workload signature for input verification."""
        return {"batch": self.batch, "seq_len": self.seq_len, "hidden": self.hidden}

    def get_output_tolerance(self) -> tuple:
        """Return tolerance for numerical comparison."""
        return (0.5, 5.0)


def get_benchmark() -> BaseBenchmark:
    """Factory function for harness discovery."""
    return OptimizedCudaPythonBenchmark()


def demonstrate_cuda_python_api():
    """Demonstrate CUDA Python API usage."""
    print("=" * 70)
    print("CUDA Python First-Class Support Demo")
    print("=" * 70)
    print()
    
    print("Availability Check:")
    print(f"  cuda.cuda (driver API): {CUDA_PYTHON_AVAILABLE}")
    print(f"  cuda.core (modern API): {CUDA_CORE_AVAILABLE}")
    print()
    
    if CUDA_PYTHON_AVAILABLE:
        print("CUDA Python provides:")
        print("  - nvrtc: Runtime compilation of CUDA C kernels")
        print("  - cuda.cuda: Low-level driver API bindings")
        print("  - Direct kernel launch without C++ compilation")
        print()
        print("Example kernel compilation flow:")
        print("  1. Write CUDA C kernel as Python string")
        print("  2. nvrtc.nvrtcCreateProgram() - create program")
        print("  3. nvrtc.nvrtcCompileProgram() - compile to PTX")
        print("  4. cuda.cuModuleLoadData() - load module")
        print("  5. cuda.cuModuleGetFunction() - get kernel handle")
        print("  6. cuda.cuLaunchKernel() - execute kernel")
    
    if CUDA_CORE_AVAILABLE:
        print()
        print("cuda.core (experimental) provides:")
        print("  - Device: GPU device management")
        print("  - LaunchConfig: Kernel launch configuration")
        print("  - launch(): High-level kernel launch")
        print("  - compile_ptx(): PTX compilation utility")
    
    if not CUDA_PYTHON_AVAILABLE and not CUDA_CORE_AVAILABLE:
        print("CUDA Python not installed. Install with:")
        print("  pip install cuda-python")
        print()
        print("Or use conda:")
        print("  conda install -c nvidia cuda-python")


if __name__ == "__main__":
    from core.harness.benchmark_harness import benchmark_main
    benchmark_main(get_benchmark)
