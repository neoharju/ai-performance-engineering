#!/usr/bin/env python3
"""
Unified metric extractor for all profiler outputs.
Extracts performance metrics from test outputs, NCU reports, Nsys reports,
PyTorch profiler outputs, and benchmark JSON files.
"""

import json
import re
import subprocess
from pathlib import Path
from typing import Dict, Optional, List, Any

# Regex patterns for extracting metrics from text outputs
PATTERNS = {
    # Bandwidth patterns
    "bandwidth_gbs": r"[Bb]andwidth[:\s]+(\d+\.?\d*)\s*GB/s",
    "bandwidth_tbs": r"[Bb]andwidth[:\s]+(\d+\.?\d*)\s*TB/s",
    "nvlink_bandwidth": r"NVLink.*?[Bb]andwidth[:\s]+(\d+\.?\d*)\s*GB/s",
    "hbm_bandwidth": r"HBM.*?[Bb]andwidth[:\s]+(\d+\.?\d*)\s*[GT]B/s",
    
    # FLOPS patterns
    "tflops": r"(\d+\.?\d*)\s*TFLOPS",
    "fp8_tflops": r"FP8.*?(\d+\.?\d*)\s*TFLOPS",
    "fp16_tflops": r"FP16.*?(\d+\.?\d*)\s*TFLOPS",
    
    # Speedup patterns
    "speedup": r"[Ss]peedup[:\s]+(\d+\.?\d*)x",
    "speedup_alt": r"(\d+\.?\d*)x.*?speedup",
    "stream_speedup": r"Stream speedup.*?(\d+\.?\d*)x",
    "stream_overlap_percent": r"Stream overlap percent[:\s]+(\d+\.?\d*)\s*%",
    "fp8_speedup": r"FP8 Speedup.*?(\d+\.?\d*)x",
    "fp8_compiled_speedup": r"FP8 Compiled Speedup.*?(\d+\.?\d*)x",
    "tensor_core_utilization_percent": r"Tensor core utilization percent[:\s]+(\d+\.?\d*)\s*%",
    
    # Utilization/efficiency patterns
    "utilization_percent": r"[Uu]tilization[:\s]+(\d+\.?\d*)\s*%",
    "efficiency_percent": r"[Ee]fficiency[:\s]+(\d+\.?\d*)\s*%",
    "occupancy_percent": r"[Oo]ccupancy[:\s]+(\d+\.?\d*)\s*%",
    
    # Latency patterns
    "latency_ms": r"[Ll]atency[:\s]+(\d+\.?\d*)\s*ms",
    "latency_us": r"[Ll]atency[:\s]+(\d+\.?\d*)\s*[uÎ¼]s",
    
    # Overhead/reduction patterns
    "overhead_reduction": r"[Oo]verhead.*?[Rr]eduction[:\s]+(\d+\.?\d*)\s*%",
    "launch_overhead": r"[Ll]aunch.*?[Oo]verhead[:\s]+(\d+\.?\d*)\s*%",
}


def extract_from_text(text: str, patterns: Optional[Dict[str, str]] = None) -> Dict[str, float]:
    """
    Extract metrics from text using regex patterns.
    
    Args:
        text: Input text to parse
        patterns: Optional custom patterns, defaults to PATTERNS
    
    Returns:
        Dictionary of extracted metrics
    """
    if patterns is None:
        patterns = PATTERNS
    
    metrics = {}
    for metric_name, pattern in patterns.items():
        matches = re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            try:
                value = float(match.group(1))
                # Store first match, or overwrite with more specific matches
                if metric_name not in metrics or "alt" not in metric_name:
                    metrics[metric_name] = value
            except (ValueError, IndexError):
                continue
    
    return metrics


def extract_from_test_output(txt_path: Path) -> Dict[str, float]:
    """
    Extract metrics from test output text files.
    
    Args:
        txt_path: Path to test output .txt file
    
    Returns:
        Dictionary of extracted metrics with chapter prefix
    """
    if not txt_path.exists():
        return {}
    
    try:
        text = txt_path.read_text()
    except Exception:
        return {}
    
    # Extract chapter from filename (e.g., ch2_nvlink.txt -> ch2)
    chapter = ""
    filename = txt_path.stem
    if filename.startswith("ch"):
        chapter = filename.split("_")[0]
    
    metrics = extract_from_text(text)
    
    # Add chapter prefix to metric names
    if chapter:
        metrics = {f"{chapter}_{k}": v for k, v in metrics.items()}
    
    return metrics


def extract_from_benchmark_json(json_path: Path) -> Dict[str, float]:
    """
    Extract metrics from benchmark JSON files.
    
    Args:
        json_path: Path to BENCHMARK_PEAK_RESULTS_*.json file
    
    Returns:
        Dictionary of extracted metrics
    """
    if not json_path.exists():
        return {}
    
    try:
        with json_path.open() as f:
            data = json.load(f)
    except Exception:
        return {}
    
    metrics = {}
    
    # Extract HBM3e bandwidth
    if "hbm3e" in data:
        hbm_data = data["hbm3e"]
        if "peak_bandwidth_tbs" in hbm_data:
            metrics["hbm3e_bandwidth_tbs"] = hbm_data["peak_bandwidth_tbs"]
        if "peak_utilization_percent" in hbm_data:
            metrics["hbm3e_utilization_percent"] = hbm_data["peak_utilization_percent"]
    
    # Extract FP16 compute
    if "fp16_compute" in data:
        fp16_data = data["fp16_compute"]
        if "peak_tflops" in fp16_data:
            metrics["fp16_compute_tflops"] = fp16_data["peak_tflops"]
    
    # Extract torch.compile speedup
    if "torch_compile" in data:
        compile_data = data["torch_compile"]
        if "speedup" in compile_data:
            metrics["torch_compile_speedup"] = compile_data["speedup"]
    
    return metrics


def extract_from_ncu_report(ncu_rep: Path) -> Dict[str, float]:
    """
    Extract metrics from NCU report files.
    
    Args:
        ncu_rep: Path to .ncu-rep file
    
    Returns:
        Dictionary of extracted metrics
    """
    if not ncu_rep.exists():
        return {}
    
    metrics = {}
    
    # Try to use ncu CLI to export metrics
    try:
        # Export as CSV
        result = subprocess.run(
            ["ncu", "--csv", "--page", "details", "--import", str(ncu_rep)],
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            # Parse CSV output
            csv_metrics = _parse_ncu_csv(result.stdout)
            metrics.update(csv_metrics)
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
        pass
    
    # Also check for companion CSV/JSON files
    companion_csv = ncu_rep.with_suffix(".csv")
    if companion_csv.exists():
        try:
            csv_text = companion_csv.read_text()
            csv_metrics = _parse_ncu_csv(csv_text)
            metrics.update(csv_metrics)
        except Exception:
            pass
    
    return metrics


def _parse_ncu_csv(csv_text: str) -> Dict[str, float]:
    """Parse NCU CSV output for key metrics."""
    metrics = {}
    
    # Look for key metrics in CSV
    patterns = {
        "sm_throughput_percent": r"sm__throughput.*?,(\d+\.?\d*)",
        "dram_throughput_percent": r"dram__throughput.*?,(\d+\.?\d*)",
        "tensor_core_utilization": r"tensor.*?active.*?,(\d+\.?\d*)",
        "occupancy": r"achieved.*?occupancy.*?,(\d+\.?\d*)",
    }
    
    for metric_name, pattern in patterns.items():
        match = re.search(pattern, csv_text, re.IGNORECASE)
        if match:
            try:
                metrics[metric_name] = float(match.group(1))
            except (ValueError, IndexError):
                pass
    
    return metrics


def extract_from_nsys_report(nsys_rep: Path) -> Dict[str, float]:
    """
    Extract metrics from Nsys report files.
    
    Args:
        nsys_rep: Path to .nsys-rep or .sqlite file
    
    Returns:
        Dictionary of extracted metrics
    """
    if not nsys_rep.exists():
        return {}
    
    metrics = {}
    
    # Try to use nsys CLI to export stats
    try:
        result = subprocess.run(
            ["nsys", "stats", "--report", "cuda_gpu_sum", "--format", "csv", str(nsys_rep)],
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            # Parse timing information
            csv_metrics = _parse_nsys_csv(result.stdout)
            metrics.update(csv_metrics)
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
        pass
    
    return metrics


def _parse_nsys_csv(csv_text: str) -> Dict[str, float]:
    """Parse Nsys CSV output for timing metrics."""
    metrics = {}
    
    # Extract total GPU time
    match = re.search(r"Total GPU Time.*?,(\d+\.?\d*)", csv_text, re.IGNORECASE)
    if match:
        try:
            metrics["total_gpu_time_ms"] = float(match.group(1))
        except (ValueError, IndexError):
            pass
    
    return metrics


def extract_from_pytorch_profile(profile_dir: Path) -> Dict[str, float]:
    """
    Extract metrics from PyTorch profiler output directory.
    
    Args:
        profile_dir: Path to PyTorch profiler output directory
    
    Returns:
        Dictionary of extracted metrics
    """
    if not profile_dir.exists() or not profile_dir.is_dir():
        return {}
    
    metrics = {}
    
    # Check for metadata.json
    metadata_path = profile_dir / "metadata.json"
    if metadata_path.exists():
        try:
            with metadata_path.open() as f:
                metadata = json.load(f)
            
            if "duration_seconds" in metadata:
                metrics["duration_seconds"] = metadata["duration_seconds"]
        except Exception:
            pass
    
    # Check for key_averages JSON files
    for json_file in profile_dir.glob("key_averages_*.json"):
        try:
            with json_file.open() as f:
                data = json.load(f)
            
            # Sum CUDA time
            total_cuda_time = sum(
                op.get("cuda_time_total", 0) for op in data if isinstance(op, dict)
            )
            if total_cuda_time > 0:
                metrics["pytorch_cuda_time_us"] = total_cuda_time
            
            # Sum CPU time
            total_cpu_time = sum(
                op.get("cpu_time_total", 0) for op in data if isinstance(op, dict)
            )
            if total_cpu_time > 0:
                metrics["pytorch_cpu_time_us"] = total_cpu_time
        except Exception:
            pass
    
    # Check for summary text files
    for txt_file in profile_dir.glob("summary_*.txt"):
        try:
            text = txt_file.read_text()
            text_metrics = extract_from_text(text)
            metrics.update(text_metrics)
        except Exception:
            pass
    
    return metrics


def discover_and_extract_all(directory: Path) -> Dict[str, Any]:
    """
    Discover all profiler outputs in a directory and extract metrics.
    
    Args:
        directory: Root directory to search
    
    Returns:
        Dictionary with extracted metrics organized by source
    """
    results = {
        "test_outputs": {},
        "benchmark": {},
        "ncu": {},
        "nsys": {},
        "pytorch": {},
    }
    
    if not directory.exists():
        return results
    
    # Extract from test output files
    for txt_file in directory.glob("*.txt"):
        if txt_file.stem not in ["SUMMARY", "summary"]:
            metrics = extract_from_test_output(txt_file)
            results["test_outputs"][txt_file.name] = metrics
    
    # Extract from benchmark JSON
    for json_file in directory.glob("BENCHMARK_PEAK_RESULTS_*.json"):
        metrics = extract_from_benchmark_json(json_file)
        results["benchmark"][json_file.name] = metrics
    
    # Extract from NCU reports
    for ncu_file in directory.rglob("*.ncu-rep"):
        metrics = extract_from_ncu_report(ncu_file)
        results["ncu"][ncu_file.name] = metrics
    
    # Extract from Nsys reports
    for nsys_file in directory.rglob("*.nsys-rep"):
        metrics = extract_from_nsys_report(nsys_file)
        results["nsys"][nsys_file.name] = metrics
    
    for sqlite_file in directory.rglob("*.sqlite"):
        if "nsys" in sqlite_file.name or "nsight" in sqlite_file.name.lower():
            metrics = extract_from_nsys_report(sqlite_file)
            results["nsys"][sqlite_file.name] = metrics
    
    # Extract from PyTorch profiler outputs
    for pytorch_dir in directory.rglob("pytorch*"):
        if pytorch_dir.is_dir():
            for subdir in pytorch_dir.iterdir():
                if subdir.is_dir():
                    metrics = extract_from_pytorch_profile(subdir)
                    results["pytorch"][subdir.name] = metrics
    
    return results


def flatten_metrics(nested_results: Dict[str, Any]) -> Dict[str, float]:
    """
    Flatten nested results into a single metrics dictionary.
    
    Args:
        nested_results: Results from discover_and_extract_all
    
    Returns:
        Flattened metrics dictionary
    """
    flattened = {}
    
    for source_type, sources in nested_results.items():
        for source_name, metrics in sources.items():
            for metric_name, value in metrics.items():
                # Avoid duplicates by prefixing with source if needed
                key = metric_name
                if key in flattened:
                    key = f"{source_type}_{metric_name}"
                flattened[key] = value
    
    # Alias common metrics that feed multiple chapters
    if "ch7_bandwidth_tbs" in flattened and "ch2_hbm3e_bandwidth_tbs" not in flattened:
        flattened["ch2_hbm3e_bandwidth_tbs"] = flattened["ch7_bandwidth_tbs"]

    return flattened
