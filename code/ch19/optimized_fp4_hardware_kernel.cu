/*
 * Optimized FP4 Hardware Kernel using cuBLASLt tensor cores.
 *
 * Uses native NVFP4 (CUDA_R_4F_E2M1) with VEC16_UE4M3 block scaling
 * for maximum tensor core throughput on Blackwell.
 *
 * Baseline uses manual FP4 packing without tensor cores.
 * This version demonstrates the massive speedup from proper tensor core usage.
 */

#include <cuda_runtime.h>
#include <cublasLt.h>
#include <cuda_fp4.h>
#include <cuda_fp8.h>
#include <cuda_fp16.h>
#include <iostream>
#include <fstream>
#include <algorithm>
#include <vector>
#include <random>
#include <cmath>
#include <string>

#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ << " - " << cudaGetErrorString(err) << std::endl; \
            exit(1); \
        } \
    } while(0)

#define CUBLASLT_CHECK(call) \
    do { \
        cublasStatus_t status = call; \
        if (status != CUBLAS_STATUS_SUCCESS) { \
            std::cerr << "cuBLASLt error " << __FILE__ << ":" << __LINE__ << " - " << status << std::endl; \
            exit(1); \
        } \
    } while(0)

constexpr int FP4_BLOCK_SIZE = 16;

// Quantize float to NVFP4 with block scaling
void quantize_to_nvfp4(const float* input, uint8_t* output_packed, 
                       __nv_fp8_e4m3* scales, int rows, int cols) {
    // Pack as 2 FP4 values per byte, along the contiguous (row-major) dimension.
    // This matches the packing used by NVIDIA cuBLASLt NVFP4 examples.
    const int packed_cols = cols / 2;
    const int num_scale_cols = cols / FP4_BLOCK_SIZE;
    
    // Compute per-row scales (block-scaled along the column dimension).
    for (int r = 0; r < rows; ++r) {
        for (int block = 0; block < num_scale_cols; ++block) {
            const int block_start = block * FP4_BLOCK_SIZE;
            
            float max_abs = 0.0f;
            for (int i = 0; i < FP4_BLOCK_SIZE; ++i) {
                max_abs = std::max(max_abs, std::abs(input[r * cols + block_start + i]));
            }
            
            float scale = (max_abs > 0.0f) ? max_abs / 6.0f : 1.0f;
            scales[r * num_scale_cols + block] = __nv_fp8_e4m3(scale);
        }
    }

    // Pack in row-major order: consecutive elements are adjacent columns.
    for (int r = 0; r < rows; ++r) {
        for (int block = 0; block < num_scale_cols; ++block) {
            const int block_start = block * FP4_BLOCK_SIZE;
            float scale = static_cast<float>(scales[r * num_scale_cols + block]);
            for (int i = 0; i < FP4_BLOCK_SIZE; i += 2) {
                float v0 = input[r * cols + block_start + i];
                float v1 = input[r * cols + block_start + i + 1];

                __nv_fp4_storage_t fp4_0 =
                    __nv_cvt_float_to_fp4(v0 / scale, __NV_E2M1, cudaRoundNearest);
                __nv_fp4_storage_t fp4_1 =
                    __nv_cvt_float_to_fp4(v1 / scale, __NV_E2M1, cudaRoundNearest);

                int packed_idx = r * packed_cols + (block_start + i) / 2;
                output_packed[packed_idx] = ((fp4_1 & 0x0F) << 4) | (fp4_0 & 0x0F);
            }
        }
    }
}

int main(int argc, char** argv) {
    std::cout << "=== Optimized FP4 Hardware Kernel (cuBLASLt Tensor Cores) ===" << std::endl;
    
    // Check GPU
    cudaDeviceProp prop;
    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));
    std::cout << "GPU: " << prop.name << " (SM" << prop.major << "." << prop.minor << ")" << std::endl;
    
    // Matrix dimensions (aligned for tensor cores). Must match the baseline.
    const int M = 512, N = 512, K = 512;
    
    // FP4 packed sizes
    const size_t packed_K = K / 2;
    const size_t packed_N = N / 2;
    const size_t elements_A_packed = M * packed_K;
    const size_t elements_B_packed = K * packed_N;
    const size_t elements_C = M * N;
    const size_t num_scales_A = M * (K / FP4_BLOCK_SIZE);
    const size_t num_scales_B = K * (N / FP4_BLOCK_SIZE);

    // Host allocation
    std::vector<float> h_A_fp32(M * K);
    std::vector<float> h_B_fp32(K * N);
    std::vector<uint8_t> h_A_packed(elements_A_packed);
    std::vector<uint8_t> h_B_packed(elements_B_packed);
    std::vector<__nv_fp8_e4m3> h_A_scales(num_scales_A);
    std::vector<__nv_fp8_e4m3> h_B_scales(num_scales_B);
    std::vector<__half> h_C(elements_C);

    // Initialize with random values
    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dis(-1.0f, 1.0f);
    for (auto& v : h_A_fp32) v = dis(gen);
    for (auto& v : h_B_fp32) v = dis(gen);

    // Quantize to NVFP4
    quantize_to_nvfp4(h_A_fp32.data(), h_A_packed.data(), h_A_scales.data(), M, K);
    quantize_to_nvfp4(h_B_fp32.data(), h_B_packed.data(), h_B_scales.data(), K, N);

    // Device allocation
    uint8_t *d_A = nullptr, *d_B = nullptr;
    __nv_fp8_e4m3 *d_A_scales = nullptr, *d_B_scales = nullptr;
    __half *d_C = nullptr;
    CUDA_CHECK(cudaMalloc(&d_A, elements_A_packed));
    CUDA_CHECK(cudaMalloc(&d_B, elements_B_packed));
    CUDA_CHECK(cudaMalloc(&d_A_scales, num_scales_A * sizeof(__nv_fp8_e4m3)));
    CUDA_CHECK(cudaMalloc(&d_B_scales, num_scales_B * sizeof(__nv_fp8_e4m3)));
    CUDA_CHECK(cudaMalloc(&d_C, elements_C * sizeof(__half)));

    // Copy to device
    CUDA_CHECK(cudaMemcpy(d_A, h_A_packed.data(), elements_A_packed, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B_packed.data(), elements_B_packed, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_A_scales, h_A_scales.data(), num_scales_A * sizeof(__nv_fp8_e4m3), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B_scales, h_B_scales.data(), num_scales_B * sizeof(__nv_fp8_e4m3), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaDeviceSynchronize());

    // Initialize cuBLASLt
    cublasLtHandle_t ltHandle;
    CUBLASLT_CHECK(cublasLtCreate(&ltHandle));

    cudaStream_t stream;
    CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));

    // Create matmul descriptor
    cublasLtMatmulDesc_t matmulDesc;
    CUBLASLT_CHECK(cublasLtMatmulDescCreate(&matmulDesc, CUBLAS_COMPUTE_32F, CUDA_R_32F));

    cublasOperation_t transa = CUBLAS_OP_T;
    cublasOperation_t transb = CUBLAS_OP_N;
    CUBLASLT_CHECK(cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSA, &transa, sizeof(transa)));
    CUBLASLT_CHECK(cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSB, &transb, sizeof(transb)));

    // Set scale mode
    cublasLtMatmulMatrixScale_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_VEC16_UE4M3;
    CUBLASLT_CHECK(cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode)));
    CUBLASLT_CHECK(cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode)));

    void* d_A_scales_ptr = d_A_scales;
    void* d_B_scales_ptr = d_B_scales;
    CUBLASLT_CHECK(cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &d_A_scales_ptr, sizeof(d_A_scales_ptr)));
    CUBLASLT_CHECK(cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &d_B_scales_ptr, sizeof(d_B_scales_ptr)));

    // Matrix layouts.
    //
    // NOTE: NVFP4 elements are stored packed (2 values per byte). The layout
    // leading dimensions are specified in *elements* (not bytes).
    cublasLtMatrixLayout_t layoutA, layoutB, layoutC;
    CUBLASLT_CHECK(cublasLtMatrixLayoutCreate(&layoutA, CUDA_R_4F_E2M1, M, K, M));
    CUBLASLT_CHECK(cublasLtMatrixLayoutCreate(&layoutB, CUDA_R_4F_E2M1, K, N, K));
    CUBLASLT_CHECK(cublasLtMatrixLayoutCreate(&layoutC, CUDA_R_16F, M, N, M));

    cublasLtOrder_t order = CUBLASLT_ORDER_COL;
    CUBLASLT_CHECK(cublasLtMatrixLayoutSetAttribute(layoutA, CUBLASLT_MATRIX_LAYOUT_ORDER, &order, sizeof(order)));
    CUBLASLT_CHECK(cublasLtMatrixLayoutSetAttribute(layoutB, CUBLASLT_MATRIX_LAYOUT_ORDER, &order, sizeof(order)));
    CUBLASLT_CHECK(cublasLtMatrixLayoutSetAttribute(layoutC, CUBLASLT_MATRIX_LAYOUT_ORDER, &order, sizeof(order)));

    float alpha = 1.0f, beta = 0.0f;

    // Workspace
    // cuBLASLt NVFP4 algorithms typically require a larger workspace.
    size_t workspaceSize = 1024 * 1024 * 64;
    void* d_workspace = nullptr;
    CUDA_CHECK(cudaMalloc(&d_workspace, workspaceSize));

    // Algorithm selection
    cublasLtMatmulPreference_t preference;
    CUBLASLT_CHECK(cublasLtMatmulPreferenceCreate(&preference));
    CUBLASLT_CHECK(cublasLtMatmulPreferenceSetAttribute(preference, CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES,
                                                         &workspaceSize, sizeof(workspaceSize)));

    cublasLtMatmulHeuristicResult_t heuristicResult = {};
    int returnedResults = 0;
    cublasStatus_t heuristicStatus = cublasLtMatmulAlgoGetHeuristic(
        ltHandle, matmulDesc, layoutA, layoutB, layoutC, layoutC,
        preference, 1, &heuristicResult, &returnedResults);
    
    if (heuristicStatus != CUBLAS_STATUS_SUCCESS || returnedResults == 0) {
        std::cerr << "No cuBLASLt NVFP4 algorithm found (status=" << heuristicStatus
                  << ", returnedResults=" << returnedResults << ")." << std::endl;
        return 1;
    }

    // Warmup
    for (int i = 0; i < 5; ++i) {
        CUBLASLT_CHECK(cublasLtMatmul(ltHandle, matmulDesc, &alpha,
                                       d_A, layoutA, d_B, layoutB, &beta,
                                       d_C, layoutC, d_C, layoutC,
                                       &heuristicResult.algo, d_workspace, workspaceSize, stream));
    }
    CUDA_CHECK(cudaStreamSynchronize(stream));

    // Benchmark
    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));

    const int iterations = 100;
    CUDA_CHECK(cudaEventRecord(start, stream));
    for (int i = 0; i < iterations; ++i) {
        CUBLASLT_CHECK(cublasLtMatmul(ltHandle, matmulDesc, &alpha,
                                       d_A, layoutA, d_B, layoutB, &beta,
                                       d_C, layoutC, d_C, layoutC,
                                       &heuristicResult.algo, d_workspace, workspaceSize, stream));
    }
    CUDA_CHECK(cudaEventRecord(stop, stream));
    CUDA_CHECK(cudaEventSynchronize(stop));

    float total_ms = 0.0f;
    CUDA_CHECK(cudaEventElapsedTime(&total_ms, start, stop));
    float avg_ms = total_ms / iterations;

    double flops = 2.0 * M * N * K;
    double tflops = flops / (avg_ms * 1e9);

    std::cout << "RESULT_MS: " << avg_ms << std::endl;

    // Optional dump for harness verification: --dump-output <path>
    std::string dump_path;
    for (int i = 1; i < argc; ++i) {
        std::string arg(argv[i]);
        if (arg == "--dump-output") {
            if (i + 1 >= argc) {
                std::cerr << "--dump-output requires a path argument" << std::endl;
                return 2;
            }
            dump_path = std::string(argv[i + 1]);
        }
    }
    if (!dump_path.empty()) {
        std::vector<__half> h_C(elements_C);
        CUDA_CHECK(cudaMemcpy(h_C.data(), d_C, elements_C * sizeof(__half), cudaMemcpyDeviceToHost));
        std::ofstream out(dump_path, std::ios::binary);
        if (!out) {
            std::cerr << "Failed to open dump path: " << dump_path << std::endl;
            return 2;
        }
        out.write(reinterpret_cast<const char*>(h_C.data()), h_C.size() * sizeof(__half));
        out.close();
    }

    // Cleanup
    CUBLASLT_CHECK(cublasLtMatmulPreferenceDestroy(preference));
    CUBLASLT_CHECK(cublasLtMatmulDescDestroy(matmulDesc));
    CUBLASLT_CHECK(cublasLtMatrixLayoutDestroy(layoutA));
    CUBLASLT_CHECK(cublasLtMatrixLayoutDestroy(layoutB));
    CUBLASLT_CHECK(cublasLtMatrixLayoutDestroy(layoutC));
    CUBLASLT_CHECK(cublasLtDestroy(ltHandle));
    CUDA_CHECK(cudaEventDestroy(start));
    CUDA_CHECK(cudaEventDestroy(stop));
    CUDA_CHECK(cudaStreamDestroy(stream));
    CUDA_CHECK(cudaFree(d_workspace));
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_A_scales));
    CUDA_CHECK(cudaFree(d_B_scales));
    CUDA_CHECK(cudaFree(d_C));

    return 0;
}
