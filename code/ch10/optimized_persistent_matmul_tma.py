"""optimized_persistent_matmul_tma.py

Persistent matmul using Triton TMA (Tensor Memory Accelerator) on Blackwell GPUs.
Uses tensor descriptors for efficient asynchronous memory transfers.

This implementation uses the stable Triton 3.5+ TMA API:
- tl.make_tensor_descriptor() - Create TMA descriptors
- tl.load_tensor_descriptor() - Load tiles via TMA hardware
- tl.store_tensor_descriptor() - Store tiles via TMA hardware

Requires SM100 (Blackwell) or newer for TMA hardware acceleration.
"""

from __future__ import annotations

import sys
from pathlib import Path
from typing import Optional

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

import torch

from core.harness.benchmark_harness import (
    BaseBenchmark,
    BenchmarkConfig,
    WorkloadMetadata,
)

try:
    import triton
    import triton.language as tl
    from triton.runtime import _allocation as triton_allocation
    TRITON_AVAILABLE = True
except ImportError as exc:
    TRITON_AVAILABLE = False
    raise ImportError("Triton is required for this example") from exc

# Verify TMA tensor descriptor API is available
REQUIRED_TMA_FEATURES = ['make_tensor_descriptor', 'load_tensor_descriptor', 'store_tensor_descriptor']
_missing_features = [f for f in REQUIRED_TMA_FEATURES if not hasattr(tl, f)]
if _missing_features:
    raise RuntimeError(
        f"FAIL FAST: Triton {triton.__version__} is missing required TMA features: {_missing_features}. "
        f"This benchmark requires Triton 3.5+ with TMA tensor descriptor support."
    )


# ============================================================================
# TMA Allocator Bridge (Required for Triton TMA kernels)
# ============================================================================

class _TorchCudaBuffer:
    """Simple wrapper for pointers returned by PyTorch's caching allocator."""
    __slots__ = ("_ptr",)

    def __init__(self, ptr: int):
        self._ptr = ptr

    def data_ptr(self) -> int:
        return self._ptr

    def __del__(self):
        if self._ptr:
            torch.cuda.caching_allocator_delete(self._ptr)
            self._ptr = 0


class _TorchCudaAllocator:
    """Allocator that lets Triton reuse PyTorch's caching allocator for TMA scratch buffers."""

    def __call__(self, size: int, alignment: int, stream: int | None):
        if size == 0:
            return _TorchCudaBuffer(0)
        if stream is None:
            current_stream = torch.cuda.current_stream()
            stream = current_stream.cuda_stream
            device_idx = current_stream.device.index
        else:
            device_idx = torch.cuda.current_device()
        if device_idx is None:
            device_idx = torch.cuda.current_device()
        ptr = torch.cuda.caching_allocator_alloc(size, device_idx, stream=stream)
        return _TorchCudaBuffer(ptr)


def _ensure_triton_allocator():
    """Set Triton's allocator once so TMA kernels can grab scratch buffers.
    
    Must be called AFTER CUDA is initialized (after first tensor creation).
    """
    if not torch.cuda.is_available():
        return
    # Ensure CUDA context is initialized
    torch.cuda.init()
    torch.cuda.current_stream()  # Force stream creation
    current = triton_allocation._allocator.get()
    if isinstance(current, triton_allocation.NullAllocator):
        triton.set_allocator(_TorchCudaAllocator())


# Don't call at import time - let setup() handle it


# ============================================================================
# TMA Persistent Matmul Kernel
# ============================================================================

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=8, num_stages=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4, num_stages=4),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4, num_stages=4),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, num_warps=4, num_stages=3),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def persistent_matmul_tma(
    A_ptr, B_ptr, C_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """TMA-backed persistent matmul kernel using tensor descriptors.
    
    Uses Triton's TMA API for efficient async memory transfers on Blackwell.
    """
    pid = tl.program_id(0)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n

    m_off = pid_m * BLOCK_M
    n_off = pid_n * BLOCK_N

    # Create TMA tensor descriptors for A and B
    A_desc = tl.make_tensor_descriptor(
        A_ptr,
        shape=[M, K],
        strides=[stride_am, stride_ak],
        block_shape=[BLOCK_M, BLOCK_K],
    )
    B_desc = tl.make_tensor_descriptor(
        B_ptr,
        shape=[K, N],
        strides=[stride_bk, stride_bn],
        block_shape=[BLOCK_K, BLOCK_N],
    )

    # Accumulator in FP32 for precision
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Main loop over K dimension
    for k in range(0, K, BLOCK_K):
        # Load tiles using TMA hardware
        a_tile = tl.load_tensor_descriptor(A_desc, [m_off, k])
        b_tile = tl.load_tensor_descriptor(B_desc, [k, n_off])
        
        # Tensor core GEMM
        acc += tl.dot(a_tile, b_tile, out_dtype=tl.float32)

    # Store result
    offs_m = m_off + tl.arange(0, BLOCK_M)
    offs_n = n_off + tl.arange(0, BLOCK_N)
    c_ptrs = C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc.to(tl.float16), mask=c_mask)


def run_optimized(M=1024, N=1024, K=1024):
    """Run TMA-accelerated persistent matmul.
    
    Uses Triton tensor descriptors for efficient memory access on Blackwell.
    """
    a = torch.randn((M, K), device="cuda", dtype=torch.float16)
    b = torch.randn((K, N), device="cuda", dtype=torch.float16)
    c = torch.empty((M, N), device="cuda", dtype=torch.float16)

    grid = lambda META: (triton.cdiv(M, META["BLOCK_M"]) * triton.cdiv(N, META["BLOCK_N"]),)
    
    persistent_matmul_tma[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
    )
    return c


# ============================================================================
# Benchmark Harness Integration
# ============================================================================

class PersistentMatmulTMABenchmark(BaseBenchmark):
    """Benchmark harness wrapper for TMA persistent matmul."""

    def __init__(self):
        super().__init__()
        self.a = None
        self.b = None
        self.c = None
        # Larger matrices to show TMA benefits over simple load/store
        self.M = 4096
        self.N = 4096
        self.K = 4096
        self._last = 0.0
        
        self._workload = WorkloadMetadata(
            requests_per_iteration=1.0,
            tokens_per_iteration=float(self.M * self.N),
        )
        self.register_workload_metadata(
            requests_per_iteration=1.0,
            tokens_per_iteration=float(self.M * self.N),
        )

    def setup(self) -> None:
        """Setup: Initialize matrices and warmup TMA kernel."""
        torch.manual_seed(42)
        
        # Initialize CUDA context first - CRITICAL: must happen before Triton allocator
        torch.cuda.init()
        _ = torch.empty(1, device='cuda')  # Force CUDA context creation
        torch.cuda.current_stream()  # Ensure stream is ready
        
        # Set allocator BEFORE creating tensors (required for Triton TMA autotuning)
        _ensure_triton_allocator()
        
        self.a = torch.randn(self.M, self.K, device=self.device, dtype=torch.float16)
        self.b = torch.randn(self.K, self.N, device=self.device, dtype=torch.float16)
        self.c = torch.empty(self.M, self.N, device=self.device, dtype=torch.float16)
        
        # Warmup (triggers autotuning) - allocator must be set before this
        for _ in range(3):
            grid = lambda META: (triton.cdiv(self.M, META["BLOCK_M"]) * triton.cdiv(self.N, META["BLOCK_N"]),)
            persistent_matmul_tma[grid](
                self.a, self.b, self.c,
                self.M, self.N, self.K,
                self.a.stride(0), self.a.stride(1),
                self.b.stride(0), self.b.stride(1),
                self.c.stride(0), self.c.stride(1),
            )
        torch.cuda.synchronize(self.device)

    def benchmark_fn(self) -> None:
        """Benchmark: TMA persistent matmul."""
        # Ensure allocator is set (subprocess may not have it from module init)
        _ensure_triton_allocator()
        
        grid = lambda META: (triton.cdiv(self.M, META["BLOCK_M"]) * triton.cdiv(self.N, META["BLOCK_N"]),)
        persistent_matmul_tma[grid](
            self.a, self.b, self.c,
            self.M, self.N, self.K,
            self.a.stride(0), self.a.stride(1),
            self.b.stride(0), self.b.stride(1),
            self.c.stride(0), self.c.stride(1),
        )
        self._last = float(self.c.sum())
        self._synchronize()

    def teardown(self) -> None:
        """Teardown: Clean up resources."""
        self.a = None
        self.b = None
        self.c = None
        torch.cuda.empty_cache()

    def get_config(self) -> BenchmarkConfig:
        return BenchmarkConfig(iterations=20, warmup=5)
    
    def get_workload_metadata(self) -> Optional[WorkloadMetadata]:
        return self._workload

    def get_custom_metrics(self) -> Optional[dict]:
        """Return TMA-specific metrics."""
        return {
            "matrix_size": f"{self.M}x{self.N}x{self.K}",
            "tma_enabled": True,
            "dtype": "float16",
        }

    def validate_result(self) -> Optional[str]:
        if not TRITON_AVAILABLE:
            return "Triton not available"
        # Verify result against torch.matmul
        expected = torch.matmul(self.a, self.b)
        if not torch.allclose(self.c, expected, rtol=0.05, atol=0.05):
            return "Result verification failed"
        return None

    def get_verify_output(self) -> torch.Tensor:
        """Return output tensor for verification."""
        if self.c is None:
            raise RuntimeError("Output not available - run benchmark first")
        return self.c.float()

    def get_input_signature(self) -> dict:
        """Return input signature for verification."""
        return {"M": self.M, "N": self.N, "K": self.K}

    def get_output_tolerance(self) -> tuple:
        """Return tolerance for numerical comparison - wider due to FP16."""
        return (0.5, 5.0)


def get_benchmark() -> BaseBenchmark:
    """Factory function for benchmark discovery."""
    return PersistentMatmulTMABenchmark()


if __name__ == "__main__":
    torch.manual_seed(0)
    
    # Test correctness
    M, N, K = 256, 256, 256
    a = torch.randn(M, K, device='cuda', dtype=torch.float16)
    b = torch.randn(K, N, device='cuda', dtype=torch.float16)
    
    # Run TMA matmul
    c = run_optimized(M, N, K)
    
    # Verify
    c_ref = torch.matmul(a, b)
    # Note: We don't verify here since run_optimized creates its own tensors
    
    print(f"âœ“ TMA persistent matmul completed")
    print(f"  Output shape: {c.shape}")
    print(f"  Output mean: {c.mean().item():.4f}")
