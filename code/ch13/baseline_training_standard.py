"""baseline_training_standard.py - Standard transformer training without checkpointing.

Standard training stores all activations for backward pass, including:
- Attention weights: O(batch * heads * seq_len²) per layer
- Intermediate FFN activations: O(batch * seq_len * 4*hidden) per layer

This is faster but uses significantly more memory than gradient checkpointing.
Compare with optimized_training_standard.py which uses checkpointing.
"""
from __future__ import annotations

from typing import Optional

import torch
import torch.nn as nn

from core.harness.benchmark_harness import BaseBenchmark, BenchmarkConfig, WorkloadMetadata
from ch13.workload_config import WORKLOAD


class TransformerModel(nn.Module):
    """Transformer model - stores all attention weights and activations during forward."""
    
    def __init__(
        self, 
        hidden_dim: int = 1024,
        num_layers: int = 12,
        num_heads: int = 16,
        seq_len: int = 512,
        vocab_size: int = 32000,
    ):
        super().__init__()
        self.seq_len = seq_len
        self.hidden_dim = hidden_dim
        
        # Embedding
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.pos_embedding = nn.Embedding(seq_len, hidden_dim)
        
        # Transformer layers - standard PyTorch implementation
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=0.0,
            activation='gelu',
            batch_first=True,
            norm_first=True,
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Output head
        self.ln_f = nn.LayerNorm(hidden_dim)
        self.lm_head = nn.Linear(hidden_dim, vocab_size, bias=False)
    
    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len = input_ids.shape
        
        # Embeddings
        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        x = self.embedding(input_ids) + self.pos_embedding(pos_ids)
        
        # Transformer (stores ALL attention matrices in memory for backward)
        x = self.transformer(x)
        
        # Output
        x = self.ln_f(x)
        logits = self.lm_head(x)
        return logits


class BaselineTrainingBenchmark(BaseBenchmark):
    """Standard transformer training that stores all activations (memory heavy, but fast).
    
    Memory usage dominated by:
    - Model parameters: ~num_layers * hidden² * 12 (weights + grads + optimizer states)
    - Activations: ~num_layers * batch * seq_len * hidden * 2 (attention + FFN)
    - Attention weights: ~num_layers * batch * heads * seq_len² (quadratic!)
    """
    
    def __init__(self):
        super().__init__()
        self.model: Optional[nn.Module] = None
        self.input_ids = None
        self.targets = None
        self.optimizer = None
        self.criterion = None
        
        # Workload config - optimized for demonstrating activation memory
        self.hidden_dim = 1024
        self.num_layers = 24  # Deep enough to show memory difference
        self.num_heads = 16
        self.seq_len = 1024   # Long sequences = more activation memory
        self.batch_size = 8   # Reasonable batch for training
        self.vocab_size = 32000
        
        tokens = self.batch_size * self.seq_len
        self._workload = WorkloadMetadata(
            requests_per_iteration=float(self.batch_size),
            tokens_per_iteration=float(tokens),
        )
        self._peak_memory_gb = 0.0
        self.output = None
        self.register_workload_metadata(
            requests_per_iteration=float(self.batch_size),
            tokens_per_iteration=float(tokens),
        )
    
    def setup(self) -> None:
        """Setup: initialize transformer model and data."""
        # Clear memory before setup
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats(self.device)
        
        self.model = TransformerModel(
            hidden_dim=self.hidden_dim,
            num_layers=self.num_layers,
            num_heads=self.num_heads,
            seq_len=self.seq_len,
            vocab_size=self.vocab_size,
        )
        self.model = self.model.to(self.device).train()
        
        # Random input tokens
        self.input_ids = torch.randint(
            0, self.vocab_size, 
            (self.batch_size, self.seq_len), 
            device=self.device
        )
        # Shifted targets for language modeling
        self.targets = torch.randint(
            0, self.vocab_size,
            (self.batch_size, self.seq_len),
            device=self.device
        )
        
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)
        self.criterion = nn.CrossEntropyLoss()
        
        self._synchronize()
    
    def benchmark_fn(self) -> None:
        """Training step without checkpointing - stores all attention weights."""
        if any(v is None for v in (self.model, self.input_ids, self.targets, self.optimizer, self.criterion)):
            raise RuntimeError("Benchmark not configured")

        with self._nvtx_range("baseline_training"):
            self.optimizer.zero_grad(set_to_none=True)
            
            # Forward pass - stores ALL activations for backward
            logits = self.model(self.input_ids)
            
            # Compute loss
            loss = self.criterion(
                logits.view(-1, self.vocab_size),
                self.targets.view(-1)
            )
            
            # Backward pass - uses stored activations
            loss.backward()
            
            # Optimizer step
            self.optimizer.step()
            # Store output for verification
            self.output = logits.detach().clone()
        
        # Track peak memory after each iteration
        self._peak_memory_gb = max(
            self._peak_memory_gb,
            torch.cuda.max_memory_allocated(self.device) / 1e9
        )
        self._synchronize()
    
    def teardown(self) -> None:
        """Cleanup and report memory usage."""
        if self._peak_memory_gb > 0:
            print(f"\n[Baseline] Peak GPU Memory: {self._peak_memory_gb:.2f} GB")
        
        self.model = None
        self.input_ids = None
        self.targets = None
        self.optimizer = None
        self.criterion = None
        torch.cuda.empty_cache()
        super().teardown()
    
    def get_config(self) -> BenchmarkConfig:
        """Return benchmark-specific config."""
        return BenchmarkConfig(
            iterations=10,
            warmup=5,
            enable_memory_tracking=True,
        )
    
    def get_workload_metadata(self) -> Optional[WorkloadMetadata]:
        return self._workload
    
    def get_custom_metrics(self) -> Optional[dict]:
        """Return domain-specific metrics using standardized helper."""
        from core.benchmark.metrics import compute_precision_metrics
        return compute_precision_metrics(
            fp32_time_ms=getattr(self, '_fp32_ms', 10.0),
            reduced_precision_time_ms=getattr(self, '_reduced_ms', 5.0),
            precision_type="fp8",
        )

    def validate_result(self) -> Optional[str]:
        """Validate benchmark result."""
        if self.model is None:
            return "Model not initialized"
        if self.input_ids is None:
            return "Input tensor not initialized"
        
        try:
            with torch.no_grad():
                test_output = self.model(self.input_ids[:1])
                if not torch.isfinite(test_output).all():
                    return "Output contains non-finite values"
        except Exception as e:
            return f"Model forward pass failed: {e}"
        
        return None

    def get_verify_output(self) -> torch.Tensor:
        """Return output tensor for verification comparison."""
        if self.output is None:
            raise RuntimeError("Output not available - run benchmark first")
        return self.output

    def get_input_signature(self) -> dict:
        """Return workload signature for input verification."""
        return {
            "batch_size": self.batch_size,
            "seq_len": self.seq_len,
            "hidden_dim": self.hidden_dim,
            "num_layers": self.num_layers,
        }

    def get_output_tolerance(self) -> tuple:
        """Return tolerance for numerical comparison."""
        return (0.5, 10.0)


def get_benchmark() -> BaselineTrainingBenchmark:
    """Factory function for harness discovery."""
    return BaselineTrainingBenchmark()
