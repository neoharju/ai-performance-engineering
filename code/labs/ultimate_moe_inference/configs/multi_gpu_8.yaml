# 8-GPU Configuration for gpt-oss-120b (Full Node)
# Target: 8x NVIDIA B200 (1.44TB total HBM3e)

model:
  name: "openai/gpt-oss-120b"
  precision: "mxfp4"

parallelism:
  tensor_parallel: 8
  pipeline_parallel: 1
  expert_parallel: 1  # Could enable for MoE expert sharding

optimizations:
  # Layer 1 (Ch1-6): Basics
  enable_tf32: true
  enable_cudnn_benchmark: true
  enable_numa_binding: true
  
  # Layer 2 (Ch7-8): Memory
  enable_memory_efficient_attention: true
  
  # Layer 3 (Ch9-10): Pipelining
  enable_flash_attention: true
  enable_double_buffering: true
  enable_tma: true  # Tensor Memory Accelerator for B200
  
  # Layer 4 (Ch11-12): Concurrency
  use_cuda_graphs: true
  num_graph_buckets: 8
  enable_stream_overlap: true
  
  # Layer 5 (Ch13-14): PyTorch
  use_torch_compile: true
  compile_mode: "max-autotune"
  use_fp8_kv_cache: true
  
  # Layer 6 (Ch15-20): Advanced
  use_speculative_decode: true
  draft_model: "openai/gpt-oss-20b"
  speculation_length: 6
  use_paged_attention: true
  page_block_size: 16
  use_gqa_optimization: true
  
  # Advanced features
  use_prefix_caching: true
  prefix_cache_size_gb: 64.0
  use_continuous_batching: true
  max_batch_size: 128
  use_chunked_prefill: true
  prefill_chunk_size: 16384
  use_ring_attention: true
  ring_attention_chunk_size: 8192

nccl:
  # NVLink optimizations for 8-GPU
  enable_nvlink: true
  enable_sharp: false  # InfiniBand SHARP (if available)
  buffer_size: 4194304  # 4MB

benchmark:
  prompt_tokens: 8192
  decode_tokens: 1024
  batch_size: 16
  warmup_iterations: 3
  benchmark_iterations: 10
  workload_type: "sharegpt"
  num_samples: 1000

profiling:
  enable_nvtx: true
  enable_nsys: false
  enable_ncu: false
  enable_power_monitoring: true
  power_sample_interval: 0.1

metrics:
  track_ttft: true
  track_tpot: true
  track_tokens_per_sec: true
  track_memory: true
  track_energy: true
  track_nvlink_bandwidth: true
  compute_percentiles: [50, 90, 95, 99]

