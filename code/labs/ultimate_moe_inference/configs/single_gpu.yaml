# Single GPU Configuration for gpt-oss-20b
# Target: 1x NVIDIA B200 (180GB HBM3e)

model:
  # Local path (faster) or HuggingFace hub ID
  name: "/mnt/dev-fin-03/ai-performance-engineering/code/gpt-oss-20b"
  # Alternative: "openai/gpt-oss-20b" (downloads from HF)
  precision: "mxfp4"  # Native precision for gpt-oss
  # Alternative: "fp8" (Transformer Engine), "bf16" (baseline)
  
parallelism:
  tensor_parallel: 1
  pipeline_parallel: 1
  expert_parallel: 1

optimizations:
  # Layer 1 (Ch1-6): Basics
  enable_tf32: true
  enable_cudnn_benchmark: true
  enable_numa_binding: true
  
  # Layer 2 (Ch7-8): Memory
  enable_memory_efficient_attention: true
  
  # Layer 3 (Ch9-10): Pipelining
  enable_flash_attention: true
  enable_double_buffering: true
  
  # Layer 4 (Ch11-12): Concurrency
  use_cuda_graphs: true
  num_graph_buckets: 4  # For variable sequence lengths
  
  # Layer 5 (Ch13-14): PyTorch
  use_torch_compile: true
  compile_mode: "max-autotune"
  use_fp8_kv_cache: true
  
  # Layer 6 (Ch15-20): Advanced
  use_speculative_decode: false  # N-gram only for single GPU
  use_ngram_speculation: true
  speculation_length: 4
  use_paged_attention: true
  page_block_size: 16
  
  # Advanced features
  use_prefix_caching: true
  prefix_cache_size_gb: 8.0
  use_continuous_batching: true
  max_batch_size: 16
  use_chunked_prefill: true
  prefill_chunk_size: 4096

benchmark:
  # Workload
  prompt_tokens: 2048
  decode_tokens: 256
  batch_size: 1
  
  # Iterations
  warmup_iterations: 3
  benchmark_iterations: 10
  
  # Workload source
  workload_type: "sharegpt"  # or "synthetic"
  num_samples: 100

profiling:
  enable_nvtx: true
  enable_nsys: false
  enable_ncu: false
  enable_power_monitoring: true
  power_sample_interval: 0.1

metrics:
  track_ttft: true
  track_tpot: true
  track_tokens_per_sec: true
  track_memory: true
  track_energy: true
  compute_percentiles: [50, 90, 95, 99]

