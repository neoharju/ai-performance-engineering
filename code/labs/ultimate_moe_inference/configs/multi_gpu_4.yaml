# 4-GPU Configuration for gpt-oss-120b
# Target: 4x NVIDIA B200 (720GB total HBM3e)

model:
  name: "openai/gpt-oss-120b"
  precision: "mxfp4"

parallelism:
  tensor_parallel: 4
  pipeline_parallel: 1
  expert_parallel: 1

optimizations:
  # Layer 1 (Ch1-6): Basics
  enable_tf32: true
  enable_cudnn_benchmark: true
  enable_numa_binding: true
  
  # Layer 2 (Ch7-8): Memory
  enable_memory_efficient_attention: true
  
  # Layer 3 (Ch9-10): Pipelining
  enable_flash_attention: true
  enable_double_buffering: true
  
  # Layer 4 (Ch11-12): Concurrency
  use_cuda_graphs: true
  num_graph_buckets: 4
  
  # Layer 5 (Ch13-14): PyTorch
  use_torch_compile: true
  compile_mode: "max-autotune"
  use_fp8_kv_cache: true
  
  # Layer 6 (Ch15-20): Advanced
  use_speculative_decode: true
  draft_model: "openai/gpt-oss-20b"
  speculation_length: 5
  use_paged_attention: true
  page_block_size: 16
  
  # Advanced features
  use_prefix_caching: true
  prefix_cache_size_gb: 32.0
  use_continuous_batching: true
  max_batch_size: 64
  use_chunked_prefill: true
  prefill_chunk_size: 8192
  use_ring_attention: false  # Enable for very long sequences

benchmark:
  prompt_tokens: 4096
  decode_tokens: 512
  batch_size: 8
  warmup_iterations: 3
  benchmark_iterations: 10
  workload_type: "sharegpt"
  num_samples: 500

profiling:
  enable_nvtx: true
  enable_nsys: false
  enable_ncu: false
  enable_power_monitoring: true
  power_sample_interval: 0.1

metrics:
  track_ttft: true
  track_tpot: true
  track_tokens_per_sec: true
  track_memory: true
  track_energy: true
  compute_percentiles: [50, 90, 95, 99]

