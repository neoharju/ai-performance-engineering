{
  "_comment": "Performance expectations for Ultimate MoE Inference Lab",
  "_hardware": "NVIDIA B200 (180GB HBM3e)",
  "_model": "openai/gpt-oss-20b (single GPU)",
  "_last_updated": "2024-01-01",
  
  "baseline_ultimate_inference": {
    "mean_ms": 5000.0,
    "std_ms": 500.0,
    "tokens_per_sec": 100.0,
    "ttft_ms": 1500.0,
    "improvement_threshold": 0.9,
    "regression_threshold": 1.1,
    "techniques": ["No optimizations"]
  },
  
  "optimized_layer1_basics": {
    "mean_ms": 4500.0,
    "std_ms": 400.0,
    "expected_speedup_vs_baseline": 1.1,
    "techniques": [
      "TF32 (Ch6)",
      "cuDNN benchmark mode (Ch6)",
      "NUMA binding (Ch3)",
      "NVTX markers (Ch2)",
      "Tensor Core selection (Ch6)"
    ],
    "chapters": [1, 2, 3, 4, 5, 6]
  },
  
  "optimized_layer2_memory": {
    "mean_ms": 3800.0,
    "std_ms": 350.0,
    "expected_speedup_vs_baseline": 1.3,
    "techniques": [
      "Coalesced memory access (Ch7)",
      "Vectorized loads - float4 (Ch7)",
      "L2 cache persistence (Ch7)",
      "Read-only cache hints (Ch7)",
      "Occupancy tuning (Ch8)",
      "Register pressure management (Ch8)",
      "ILP - Instruction Level Parallelism (Ch8)",
      "Loop unrolling (Ch8)"
    ],
    "chapters": [7, 8]
  },
  
  "optimized_layer3_pipelining": {
    "mean_ms": 2500.0,
    "std_ms": 300.0,
    "expected_speedup_vs_baseline": 2.0,
    "techniques": [
      "Tiling for data reuse (Ch9)",
      "Shared memory bank conflict avoidance (Ch9)",
      "Warp shuffle intrinsics (Ch9)",
      "Double buffering (Ch10)",
      "CUDA Pipeline API (Ch10)",
      "TMA - Tensor Memory Accelerator (Ch10)",
      "TMEM for 2-CTA clusters (Ch10)",
      "Warp specialization (Ch10)",
      "FlashAttention via SDPA (Ch10)",
      "Persistent kernels (Ch10)"
    ],
    "chapters": [9, 10]
  },
  
  "optimized_layer4_graphs": {
    "mean_ms": 2000.0,
    "std_ms": 250.0,
    "expected_speedup_vs_baseline": 2.5,
    "techniques": [
      "CUDA streams (Ch11)",
      "Stream events for sync (Ch11)",
      "Async H2D/D2H transfers (Ch11)",
      "Cooperative Groups (Ch11)",
      "DSMEM - Distributed Shared Memory (Ch11)",
      "CUDA graph capture (Ch12)",
      "Graph replay (Ch12)",
      "Stream-ordered allocation (Ch12)"
    ],
    "chapters": [11, 12]
  },
  
  "optimized_layer5_compile": {
    "mean_ms": 1400.0,
    "std_ms": 180.0,
    "expected_speedup_vs_baseline": 3.5,
    "techniques": [
      "FP8 E4M3/E5M2 (Ch13)",
      "DelayedScaling recipe (Ch13)",
      "FP8 KV cache (Ch13)",
      "torch.compile (Ch14)",
      "TorchInductor (Ch14)",
      "max-autotune mode (Ch14)",
      "Triton kernels (Ch14)",
      "Triton autotuning (Ch14)",
      "Kernel fusion (Ch14)"
    ],
    "chapters": [13, 14]
  },
  
  "optimized_ultimate_inference": {
    "mean_ms": 1000.0,
    "std_ms": 150.0,
    "expected_speedup_vs_baseline": 5.0,
    "techniques": [
      "All Layer 1-5 techniques",
      "MoE expert parallelism (Ch15)",
      "Expert overlap via streams (Ch15)",
      "PagedAttention (Ch16)",
      "Prefix caching (Ch16)",
      "Continuous batching (Ch17)",
      "Chunked prefill (Ch17)",
      "Speculative decoding (Ch18)",
      "FlashMLA (Ch18)",
      "GQA/MQA optimization (Ch18)",
      "Dynamic precision switching (Ch19)",
      "NVFP4 with block scaling (Ch19)"
    ],
    "chapters": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
    "tokens_per_sec": 2000.0,
    "ttft_ms": 100.0
  },
  
  "profiling_targets": {
    "nsys": {
      "expected_regions": [
        "baseline_inference",
        "layer1_basics", 
        "layer2_memory",
        "layer3_pipelining", 
        "layer4_graphs", 
        "layer5_compile",
        "ultimate_inference"
      ],
      "cuda_kernel_overhead_threshold_pct": 5.0
    },
    "ncu": {
      "workload_type": "tensor_core",
      "metrics": [
        "sm__cycles_active.avg",
        "sm__warps_active.avg.pct_of_peak_sustained_active",
        "sm__pipe_tensor_cycles_active.avg",
        "dram__bytes_read.sum",
        "dram__bytes_write.sum"
      ],
      "expected_occupancy_pct": 50.0,
      "expected_tensor_core_util_pct": 30.0
    },
    "roofline": {
      "expected_bound": "memory",
      "ridge_point_flop_byte": 500,
      "comments": "LLM inference is typically memory-bound due to KV cache access"
    }
  },
  
  "blackwell_specific": {
    "tma_available": true,
    "tmem_available": true,
    "dsmem_available": true,
    "fp4_tensor_cores": true,
    "hbm3e_bandwidth_tb_s": 8.0,
    "compute_capability": [10, 0]
  }
}
