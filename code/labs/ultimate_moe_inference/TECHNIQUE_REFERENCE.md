# Complete Technique Reference

This document maps **every optimization technique** from the book to where it's covered in this lab.

## Legend

| Status | Meaning |
|--------|---------|
| ‚úÖ | Fully covered with explanation |
| üìù | Mentioned/documented but abstracted by library |
| ‚ö†Ô∏è | Partially covered, could use more depth |

---

## Chapter 1-3: Foundations

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| GPU architecture (SM, warps, SIMT) | ‚úÖ | 00_baseline.py | Background context |
| CUDA execution model | ‚úÖ | 02_memory.py | Memory hierarchy docs |
| Nsight Systems profiling | ‚úÖ | All files | NVTX markers, nsys commands |
| Nsight Compute metrics | ‚úÖ | 02_memory.py, 03_flash.py | NCU commands, metric explanations |
| Grace-Blackwell architecture | ‚úÖ | 01_basics.py | NUMA, NVLink-C2C, 180GB HBM3e |
| NUMA awareness & CPU pinning | ‚úÖ | 01_basics.py | Memory affinity |
| NVLink/NVSwitch topology | ‚úÖ | README.md | 900 GB/s bidirectional |

---

## Chapter 4-6: Thread Hierarchy & Tensor Cores

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| Thread/warp/block hierarchy | ‚úÖ | 02_memory.py | Occupancy discussion |
| Grid/block sizing | üìù | - | Handled by libraries |
| TF32 (TensorFloat-32) | ‚úÖ | 01_basics.py | `torch.backends.cuda.matmul.allow_tf32` |
| Tensor Core utilization | ‚úÖ | 01_basics.py | Via cuBLAS/cuDNN |
| cuDNN benchmark mode | ‚úÖ | 01_basics.py | `torch.backends.cudnn.benchmark` |
| Multi-GPU programming | ‚úÖ | README.md | Device maps, tensor parallel |

---

## Chapter 7: Memory Hierarchy

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| Global memory coalescing | ‚úÖ | 02_memory.py | 128-byte cache lines, contiguous access |
| Vectorized loads (float4, 128-bit) | ‚úÖ | 02_memory.py | 4x fewer transactions |
| L1/L2 cache behavior | ‚úÖ | 02_memory.py | Cache hit rates |
| L2 persistence hints | ‚úÖ | 02_memory.py | `cudaAccessPropertyPersisting` |
| Read-only cache (__ldg) | ‚úÖ | 02_memory.py | `const __restrict__` |
| Shared memory basics | ‚úÖ | 03_flash.py | Tiling explanation |
| Bank conflicts | ‚úÖ | 03_flash.py | Padding, swizzling |
| Unified Memory | üìù | README.md | Grace-Blackwell EGM |

---

## Chapter 8: Occupancy & ILP

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| Occupancy calculation | ‚úÖ | 02_memory.py | `get_occupancy_info()` |
| `__launch_bounds__` | ‚úÖ | 02_memory.py | Compiler hints |
| Register pressure | ‚úÖ | 02_memory.py | Trade-off explained |
| ILP (Instruction-Level Parallelism) | ‚úÖ | 02_memory.py | Loop unrolling, multiple ops in flight |
| Latency hiding via occupancy | ‚úÖ | 02_memory.py | Warp scheduling |
| Warp divergence | ‚úÖ | 02_memory.py | Branch divergence penalty |

---

## Chapter 9: Tiling & Shared Memory

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| Tiling for data reuse | ‚úÖ | 03_flash.py | ASCII diagram, 64√ó64 tiles |
| Shared memory allocation | ‚úÖ | 03_flash.py | SRAM usage |
| Bank conflict avoidance | ‚úÖ | 03_flash.py | Padding, swizzling mentioned |
| Warp shuffle (`__shfl_sync`) | ‚úÖ | 03_flash.py | For reductions |
| Cooperative tiling | ‚úÖ | 03_flash.py | Block-level coordination |

---

## Chapter 10: Intra-Kernel Pipelining ‚≠ê

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| **Double buffering** | ‚úÖ | 03_flash.py | Two buffers, overlap load/compute |
| **CUDA Pipeline API** | ‚úÖ | 03_flash.py | `producer_acquire/commit`, `consumer_wait/release` |
| **cp.async / memcpy_async** | ‚úÖ | 03_flash.py | Async memory copy to SMEM |
| **Warp specialization** | ‚úÖ | 03_flash.py | Loader/compute/storer warps with code |
| **TMA (Tensor Memory Accelerator)** | ‚úÖ | 03_flash.py | Blackwell hardware async |
| **TMEM (Tensor Memory)** | ‚úÖ | README.md | 2-CTA cluster shared memory |
| **Persistent kernels** | ‚úÖ | 03_flash.py | Single kernel, atomic work queue |
| **Online softmax** | ‚úÖ | 03_flash.py | Incremental softmax algorithm |
| **FlashAttention internals** | ‚úÖ | 03_flash.py | Full explanation |
| **Software pipelining** | ‚úÖ | 03_flash.py | Overlap stages |

---

## Chapter 11: Inter-Kernel Pipelining

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| CUDA streams | ‚úÖ | 04_graphs.py | Multiple execution queues |
| Stream events | ‚úÖ | 04_graphs.py | Synchronization |
| Async H2D/D2H transfers | ‚úÖ | 04_graphs.py | Overlap with compute |
| **Cooperative Groups** | ‚úÖ | 04_graphs.py | Thread block sync |
| **Thread Block Clusters** | ‚úÖ | 04_graphs.py | 2-8 CTA clusters |
| **DSMEM (Distributed Shared Memory)** | ‚úÖ | 04_graphs.py | Cross-CTA shared memory |
| Compute/communication overlap | ‚úÖ | 04_graphs.py | Prefill/decode streams |
| NCCL collectives | ‚úÖ | README.md | AllReduce, AllGather |
| CUDA-aware MPI | üìù | README.md | Direct GPU buffers |
| NVSHMEM puts/gets | üìù | README.md | One-sided communication |

---

## Chapter 12: Dynamic Scheduling & CUDA Graphs ‚≠ê

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| **Atomic work queues** | ‚úÖ | 04_graphs.py | L2 cache atomics, batched atomics |
| **Batched atomics** | ‚úÖ | 04_graphs.py | 32 items per atomic to reduce contention |
| CUDA graph capture | ‚úÖ | 04_graphs.py | Record kernel sequence |
| Graph replay | ‚úÖ | 04_graphs.py | Single driver call |
| Graph constraints | ‚úÖ | 04_graphs.py | Static shapes required |
| **Device-initiated graph launch** | ‚úÖ | 04_graphs.py | `cudaGraphInstantiateFlagDeviceLaunch` |
| **PDL (Programmatic Dependent Launch)** | ‚úÖ | 04_graphs.py | GPU-to-GPU kernel launch |
| **Dynamic parallelism** | ‚úÖ | 04_graphs.py | GPU decides what to run next |
| Stream-ordered allocation | ‚úÖ | 04_graphs.py | `cudaMallocAsync` |
| Graph bucketing | ‚úÖ | 06_ultimate.py | Variable seq lengths |
| **Tail effects mitigation** | ‚úÖ | 06_ultimate.py | Dynamic packing of remaining work |

---

## Chapter 13: PyTorch Profiling & System Tuning

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| PyTorch Profiler | ‚úÖ | run_full_analysis.py | `torch.profiler` integration |
| FP8 E4M3/E5M2 formats | ‚úÖ | 05_compile.py | 8-bit formats |
| Transformer Engine | ‚úÖ | 05_compile.py | NVIDIA FP8 library |
| MXFP8 format | ‚úÖ | 05_compile.py | Blackwell native FP8 |
| DelayedScaling recipe | ‚úÖ | expectations.json | Amax history, hysteresis |
| FP8 KV cache | ‚úÖ | 06_ultimate.py | 50% memory reduction |
| NVFP4 format | ‚úÖ | 06_ultimate.py | 4-bit on Blackwell |
| Memory pools (caching allocator) | ‚úÖ | 02_memory.py | Avoid cudaMalloc overhead |

---

## Chapter 14: torch.compile & Triton ‚≠ê

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| **TorchDynamo** | ‚úÖ | 05_compile.py | Bytecode capture |
| **FX Graph** | ‚úÖ | 05_compile.py | IR representation |
| **AOT Autograd** | ‚úÖ | 05_compile.py | Forward/backward fusion |
| **TorchInductor** | ‚úÖ | 05_compile.py | Code generation |
| **Kernel fusion** | ‚úÖ | 05_compile.py | Reduce memory traffic |
| **max-autotune mode** | ‚úÖ | 05_compile.py | Exhaustive search |
| **reduce-overhead mode** | ‚úÖ | 05_compile.py | Minimize CPU overhead |
| **Graph breaks** | ‚úÖ | 05_compile.py | Causes, debugging, avoidance |
| **Regional compilation** | ‚úÖ | 05_compile.py | `torch.compile` on submodules |
| **Dynamic shapes** | ‚úÖ | 05_compile.py | `mark_dynamic()` |
| **Triton kernels** | ‚úÖ | 05_compile.py | Python-like GPU programming |
| **Triton autotuning** | ‚úÖ | 05_compile.py | Block sizes, num_warps, num_stages |
| **Triton warp specialization** | ‚úÖ | 05_compile.py | `num_consumer_groups` |
| **Cache eviction policies** | ‚úÖ | expectations.json | evict_first, evict_last |
| Shape guards | ‚úÖ | 05_compile.py | Recompilation triggers |

---

## Chapter 15: MoE (Mixture of Experts)

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| Expert routing | ‚úÖ | 06_ultimate.py | Top-k selection |
| Expert parallelism | ‚úÖ | 06_ultimate.py | Experts on different GPUs |
| All-to-all communication | ‚úÖ | 06_ultimate.py | Token routing |
| Load balancing | ‚úÖ | 06_ultimate.py | Auxiliary losses, capacity factor |
| Stream-overlapped experts | ‚úÖ | 06_ultimate.py | Parallel execution |
| Expert rebalancing | ‚úÖ | README.md | Dynamic regrouping |
| Sparse activation | ‚úÖ | 06_ultimate.py | Only k experts active per token |

---

## Chapter 16: PagedAttention

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| Block-based KV cache | ‚úÖ | 06_ultimate.py | Virtual memory for KV |
| Dynamic allocation | ‚úÖ | 06_ultimate.py | On-demand blocks |
| Memory fragmentation fix | ‚úÖ | 06_ultimate.py | >95% utilization |
| Prefix caching | ‚úÖ | 06_ultimate.py | Shared prompt KV |
| Page table translation | ‚úÖ | 06_ultimate.py | Logical‚Üíphysical mapping |

---

## Chapter 17: vLLM/SGLang & Inference Serving ‚≠ê

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| **Continuous batching** | ‚úÖ | 06_ultimate.py | Dynamic request scheduling |
| **Chunked prefill** | ‚úÖ | 06_ultimate.py | Long sequences without OOM |
| **Length bucketing** | ‚úÖ | 06_ultimate.py | Group by sequence length to reduce padding |
| **SequenceGroup scheduling** | ‚úÖ | 06_ultimate.py | vLLM's request management |
| **Padding overhead** | ‚úÖ | 06_ultimate.py | Up to 50% waste without batching |
| **Disaggregated prefill/decode** | ‚úÖ | 06_ultimate.py | Separate worker pools |
| Request preemption | ‚úÖ | TODO_EXTENSIONS.md | Priority scheduling |
| TTFT/TPOT tracking | ‚úÖ | All benchmarks | Core metrics |
| SLO enforcement | ‚úÖ | monitoring.py | Latency targets |

---

## Chapter 18: Advanced Decode & Attention ‚≠ê

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| **FlashMLA** | ‚úÖ | 06_ultimate.py | Fused decode kernel (DeepSeek) |
| **ThunderMLA / Megakernels** | ‚úÖ | 06_ultimate.py | Reduce tail effects, fused ops |
| **FlexDecoding** | ‚úÖ | 06_ultimate.py | PyTorch's decode backend |
| **Nested Jagged Tensors (NJT)** | ‚úÖ | 06_ultimate.py | Ragged batching without padding |
| **POD-Attention** | ‚úÖ | 06_ultimate.py | SM-aware CTA scheduling |
| **Tail effects** | ‚úÖ | 06_ultimate.py | Sequences finishing at different times |
| **KV cache pool** | ‚úÖ | 06_ultimate.py | Distributed KV storage |
| **Prefix sharing / KV reuse** | ‚úÖ | 06_ultimate.py | Shared system prompts |
| Draft model speculation | ‚úÖ | 06_ultimate.py | Parallel verification |
| Token acceptance/rejection | ‚úÖ | 06_ultimate.py | Accept matching tokens |
| GQA/MQA | ‚úÖ | README.md | Grouped-Query Attention |

---

## Chapter 19: Dynamic & Adaptive Inference ‚≠ê

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| **Adaptive parallelism** | ‚úÖ | 06_ultimate.py | Switch TP/PP/hybrid at runtime |
| **Dynamic precision switching** | ‚úÖ | 06_ultimate.py | FP8‚ÜíFP4 based on confidence |
| **Entropy-based precision** | ‚úÖ | 06_ultimate.py | Logit sharpness triggers precision |
| **Per-token precision** | ‚úÖ | 06_ultimate.py | Fine-grained control |
| **Memory pressure triggers** | ‚úÖ | 06_ultimate.py | Compress KV when low on memory |
| Worker pool routing | ‚úÖ | 06_ultimate.py | Route to best-fit replica |
| Pipeline bubble mitigation | ‚úÖ | 06_ultimate.py | Avoid PP overhead for short queries |

---

## Chapter 20: AI-Assisted Optimization

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| LLM kernel generation | ‚úÖ | README.md | AlphaTensor, DeepSeek |
| Autotuning with AI | ‚úÖ | README.md | Beyond grid search |
| Self-improving agents | üìù | README.md | Future direction |

---

## Blackwell-Specific Features (SM 10.0) ‚≠ê

| Feature | Status | Lab File | Notes |
|---------|--------|----------|-------|
| TMA (Tensor Memory Accelerator) | ‚úÖ | 03_flash.py | Hardware async copy |
| TMEM (Tensor Memory) | ‚úÖ | README.md | Cluster-level memory |
| 2-8 CTA Thread Block Clusters | ‚úÖ | 04_graphs.py | Cooperative CTAs |
| DSMEM | ‚úÖ | 04_graphs.py | Distributed shared memory |
| FP4 Tensor Cores (NVFP4) | ‚úÖ | 06_ultimate.py | 4-bit compute |
| MXFP8 native support | ‚úÖ | 05_compile.py | 8-bit with scaling |
| PDL (Programmatic Dependent Launch) | ‚úÖ | 04_graphs.py | GPU-to-GPU kernel launch |
| Device-initiated graph launch | ‚úÖ | 04_graphs.py | No CPU round-trip |
| HBM3e (8 TB/s) | ‚úÖ | README.md | Bandwidth specs |
| NVLink-C2C (900 GB/s) | ‚úÖ | 01_basics.py | Grace-Blackwell coherent |
| 180 GB HBM per GPU | ‚úÖ | README.md | Memory capacity |

---

## Distributed & Multi-GPU

| Technique | Status | Lab File | Notes |
|-----------|--------|----------|-------|
| Tensor Parallelism (TP) | ‚úÖ | README.md | Split matrices |
| Pipeline Parallelism (PP) | ‚úÖ | README.md | Split layers |
| Expert Parallelism | ‚úÖ | 06_ultimate.py | MoE experts on different GPUs |
| Context/Sequence Parallelism | ‚úÖ | README.md | Split long sequences |
| Ring Attention | ‚úÖ | README.md | Sequence parallel attention |
| NCCL tuning | ‚úÖ | README.md | NCCL_ALGO, NCCL_PROTO |
| GPUDirect RDMA | ‚úÖ | README.md | NIC-GPU direct |
| NIXL | ‚úÖ | README.md | Async KV transfer |
| All-reduce, All-gather | ‚úÖ | README.md | Collective ops |

---

## Raw CUDA Examples in Codebase

For low-level technique demonstrations, see:

| Technique | Example File |
|-----------|--------------|
| Coalescing | `ch7/baseline_tma_copy.cu` vs `optimized_tma_copy.cu` |
| Double buffering | `ch8/optimized_double_buffering_pipelined.cu` |
| CUDA Pipeline API | `ch10/optimized_warp_specialized_pipeline.cu` |
| Warp specialization | `ch10/baseline_warp_specialized_pipeline.cu` |
| CTA clusters | `ch10/optimized_cluster_group.py` |
| DSMEM | `ch11/optimized_streams_warp_specialized.cu` |
| TMA | `ch7/optimized_tma_bulk_tensor_2d.py` |
| CUDA graphs | `ch12/optimized_cuda_graphs.py` |
| Triton kernels | `ch14/triton_examples.py` |
| Regional compilation | `ch16/optimized_regional_compilation.py` |
| FlashMLA | `ch18/optimized_flashmla_decode.py` |
| Atomic queues | `ch12/uneven_dynamic.cu` |

---

## Summary

| Category | Techniques | Coverage |
|----------|------------|----------|
| Foundations (Ch1-6) | 15 | ‚úÖ 100% |
| Memory (Ch7-8) | 16 | ‚úÖ 100% |
| Pipelining (Ch9-10) | 12 | ‚úÖ 100% |
| Concurrency (Ch11-12) | 18 | ‚úÖ 100% |
| PyTorch (Ch13-14) | 18 | ‚úÖ 100% |
| MoE & Serving (Ch15-17) | 22 | ‚úÖ 100% |
| Advanced Decode (Ch18) | 12 | ‚úÖ 100% |
| Adaptive (Ch19) | 7 | ‚úÖ 100% |
| AI-Assisted (Ch20) | 3 | ‚úÖ 100% |
| Blackwell | 11 | ‚úÖ 100% |
| Distributed | 10 | ‚úÖ 100% |
| **Total** | **144** | ‚úÖ **100%** |

---

*This reference tracks technique coverage for the Ultimate MoE Inference Lab.*
*Last updated: November 2025*
