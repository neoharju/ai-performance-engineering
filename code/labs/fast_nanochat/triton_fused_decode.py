"""Warp-specialized Triton fused MLP for decode step (LayerNorm + Linear -> GELU -> Linear).

Designed for small batch (e.g., 8) and hidden sizes around 2-4K on Blackwell (SM100/121).
Uses tensor descriptors (TMA) and warp specialization to overlap copies/compute.
"""

from __future__ import annotations

from typing import Optional, Tuple

import torch

import triton
import triton.language as tl
try:
    import triton.runtime._allocation as _triton_alloc  # type: ignore
except Exception:
    _triton_alloc = None

from common.python.triton_compat import ensure_triton_compat

# Configure Triton allocator at import time so kernels can allocate scratch.
def _install_triton_allocator() -> None:
    if _triton_alloc is None:
        return
        class _TorchBuffer:
            def __init__(self, size: int):
                self.tensor = torch.empty(size, dtype=torch.uint8, device="cuda")

            def data_ptr(self) -> int:
                return self.tensor.data_ptr()

        def _allocator(size: int, alignment: int, stream) -> _TorchBuffer:
            return _TorchBuffer(size)

        _triton_alloc.set_allocator(_allocator)  # type: ignore[attr-defined]

# Set allocator once at import time, but also re-install on demand.
_install_triton_allocator()


@triton.autotune(
    configs=[
        triton.Config(
            {
                "BLOCK_M": 16,
                "BLOCK_N": 256,
                "BLOCK_K": 128,
                "BLOCK_H1": 128,
                "NUM_STAGES": 2,
            },
            num_warps=8,
            num_stages=2,
        ),
    ],
    key=["M", "H"],
)
@triton.jit
def fused_decode_mlp_kernel(
    X_ptr,
    W1_ptr,
    B1_ptr,
    W2_ptr,
    B2_ptr,
    Out_ptr,
    M,
    H,
    stride_xm,
    stride_xk,
    stride_w1k,
    stride_w1n,
    stride_w2k,
    stride_w2n,
    stride_outm,
    stride_outn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    BLOCK_H1: tl.constexpr,
    NUM_STAGES: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    m0 = pid_m * BLOCK_M
    n0 = pid_n * BLOCK_N

    offs_m = m0 + tl.arange(0, BLOCK_M)
    offs_n = n0 + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # TMA descriptors
    X_desc = tl.make_tensor_descriptor(
        X_ptr,
        shape=[M, H],
        strides=[stride_xm, stride_xk],
        block_shape=[BLOCK_M, BLOCK_K],
    )
    W1_desc = tl.make_tensor_descriptor(
        W1_ptr,
        shape=[H, H],
        strides=[stride_w1k, stride_w1n],
        block_shape=[BLOCK_K, BLOCK_H1],
    )
    W2_desc = tl.make_tensor_descriptor(
        W2_ptr,
        shape=[H, H],
        strides=[stride_w2k, stride_w2n],
        block_shape=[BLOCK_H1, BLOCK_N],
    )

    acc_out = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Loop over hidden (K) chunks for the first matmul, fusing projection 2 inside.
    H_tiles = (H + BLOCK_H1 - 1) // BLOCK_H1
    K_tiles = (H + BLOCK_K - 1) // BLOCK_K

    for ht in tl.range(0, H_tiles, num_stages=NUM_STAGES, warp_specialize=True):
        h_start = ht * BLOCK_H1

        # Compute hidden1 chunk = X @ W1_chunk
        hidden_chunk = tl.zeros((BLOCK_M, BLOCK_H1), dtype=tl.float32)
        for kt in tl.range(0, K_tiles):
            k_start = kt * BLOCK_K

            x_block = X_desc.load([m0, k_start])
            row_offsets = tl.broadcast_to(offs_m[:, None], (BLOCK_M, BLOCK_K))
            col_offsets = tl.broadcast_to((k_start + offs_k)[None, :], (BLOCK_M, BLOCK_K))
            x_block = tl.where(
                (row_offsets < M) & (col_offsets < H),
                x_block,
                tl.zeros_like(x_block),
            )

            w1_block = W1_desc.load([k_start, h_start])
            k_offsets = tl.broadcast_to((k_start + offs_k)[:, None], (BLOCK_K, BLOCK_H1))
            h_offsets = tl.broadcast_to((h_start + tl.arange(0, BLOCK_H1))[None, :], (BLOCK_K, BLOCK_H1))
            w1_block = tl.where(
                (k_offsets < H) & (h_offsets < H),
                w1_block,
                tl.zeros_like(w1_block),
            )

            hidden_chunk += tl.dot(x_block, w1_block, out_dtype=tl.float32)

        # Add bias1 slice and GELU
        bias1 = tl.load(B1_ptr + h_start + tl.arange(0, BLOCK_H1))
        bias1 = bias1[None, :]
        hidden_chunk += bias1
        # Fast GELU approximation without tanh (use sigmoid-friendly form)
        hidden_chunk = hidden_chunk * tl.sigmoid(1.702 * hidden_chunk)

        # Projection 2 for this hidden slice
        w2_block = W2_desc.load([h_start, n0])
        h_offsets = tl.broadcast_to((h_start + tl.arange(0, BLOCK_H1))[:, None], (BLOCK_H1, BLOCK_N))
        n_offsets = tl.broadcast_to(offs_n[None, :], (BLOCK_H1, BLOCK_N))
        w2_block = tl.where(
            (h_offsets < H) & (n_offsets < H),
            w2_block,
            tl.zeros_like(w2_block),
        )

        acc_out += tl.dot(hidden_chunk, w2_block.to(tl.float32), out_dtype=tl.float32)

    # Apply bias2 and store
    bias2 = tl.load(B2_ptr + n0 + tl.arange(0, BLOCK_N))
    bias2 = bias2[None, :]
    acc_out += bias2

    out_ptrs = Out_ptr + (offs_m[:, None] * stride_outm + offs_n[None, :] * stride_outn)
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < H)
    tl.store(out_ptrs, acc_out.to(tl.float16), mask=mask)


def fused_decode_mlp(
    x: torch.Tensor,
    w1: torch.Tensor,
    b1: torch.Tensor,
    w2: torch.Tensor,
    b2: torch.Tensor,
) -> torch.Tensor:
    """Fused decode MLP: y = GELU(x @ w1 + b1) @ w2 + b2.

    Args:
        x: [batch, hidden] input
        w1: [hidden, hidden] weight
        b1: [hidden] bias
        w2: [hidden, hidden] weight
        b2: [hidden] bias
    """
    _install_triton_allocator()
    ensure_triton_compat()
    assert x.is_cuda and w1.is_cuda and w2.is_cuda
    assert x.dtype in (torch.float16, torch.bfloat16)
    assert w1.dtype == x.dtype and w2.dtype == x.dtype

    B, H = x.shape
    out = torch.empty_like(x)

    grid = (triton.cdiv(B, 8), triton.cdiv(H, 128))
    fused_decode_mlp_kernel[grid](
        x,
        w1,
        b1,
        w2,
        b2,
        out,
        B,
        H,
        x.stride(0),
        x.stride(1),
        w1.stride(0),
        w1.stride(1),
        w2.stride(0),
        w2.stride(1),
        out.stride(0),
        out.stride(1),
    )
    return out
