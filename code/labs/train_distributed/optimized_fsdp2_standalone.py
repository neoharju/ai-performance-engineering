#!/usr/bin/env python3
"""Optimized: FSDP2 (FP8) training on Blackwell.

Advanced FSDP2 training with:
- FP8 mixed precision via torchao
- Fused AdamW optimizer
"""

from __future__ import annotations

import argparse
import os
import sys
import time
from pathlib import Path
from typing import Optional

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import MixedPrecision

# Add common to path
repo_root = Path(__file__).parent.parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from core.benchmark.verification import PrecisionFlags, simple_signature
from core.harness.benchmark_harness import (
    BaseBenchmark,
    BenchmarkConfig,
    LaunchVia,
    TorchrunLaunchSpec,
)
from core.utils.logger import get_logger

logger = get_logger(__name__)

_DEFAULT_BATCH_SIZE = 4
_DEFAULT_MICRO_BATCH = 1
_DEFAULT_SEQ_LEN = 2048
_DEFAULT_HIDDEN = 4096
_DEFAULT_LAYERS = 8

try:
    from torchao.float8 import convert_to_float8_training, Float8LinearConfig
except ImportError as exc:  # pragma: no cover - torchao required on Blackwell
    convert_to_float8_training = None
    Float8LinearConfig = None
    _TORCHAO_IMPORT_ERROR = exc
else:
    _TORCHAO_IMPORT_ERROR = None


class SimpleTransformerLayer(nn.Module):
    def __init__(self, hidden_size: int):
        super().__init__()
        self.linear1 = nn.Linear(hidden_size, hidden_size * 4, bias=False)
        self.linear2 = nn.Linear(hidden_size * 4, hidden_size, bias=False)
        self.norm = nn.LayerNorm(hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.linear2(torch.relu(self.linear1(self.norm(x))))


def _init_distributed() -> tuple[int, int, int]:
    if "RANK" not in os.environ or "WORLD_SIZE" not in os.environ:
        raise RuntimeError("FSDP2 optimized requires torchrun (RANK/WORLD_SIZE missing).")
    if not dist.is_initialized():
        dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_rank = int(os.environ.get("LOCAL_RANK", rank))
    torch.cuda.set_device(local_rank)
    return rank, world_size, local_rank


def _require_torchao() -> None:
    if convert_to_float8_training is None or Float8LinearConfig is None:
        raise RuntimeError("torchao float8 is required for optimized_fsdp2_standalone")


def _build_model(hidden_size: int, num_layers: int, device: torch.device) -> nn.ModuleList:
    return nn.ModuleList([
        SimpleTransformerLayer(hidden_size)
        for _ in range(num_layers)
    ]).to(device)


def _run_worker(
    *,
    iters: int,
    warmup: int,
    batch_size: int,
    micro_batch_size: int,
    seq_length: int,
    hidden_size: int,
    num_layers: int,
) -> None:
    _require_torchao()
    rank, world_size, local_rank = _init_distributed()
    if world_size < 2:
        raise RuntimeError("FSDP2 optimized requires >=2 GPUs.")

    if batch_size % micro_batch_size != 0:
        raise ValueError("batch_size must be divisible by micro_batch_size")

    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)

    device = torch.device(f"cuda:{local_rank}")

    model = _build_model(hidden_size, num_layers, device)

    fp8_config = Float8LinearConfig(
        enable_fsdp_float8_all_gather=True,
        enable_pre_and_post_forward=True,
    )
    convert_to_float8_training(model, config=fp8_config)

    mixed_precision = MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        buffer_dtype=torch.bfloat16,
    )
    fsdp_model = FSDP(model, mixed_precision=mixed_precision, use_orig_params=True)

    input_tensor = torch.randn(
        micro_batch_size,
        seq_length,
        hidden_size,
        device=device,
        dtype=torch.bfloat16,
    )

    optimizer = torch.optim.AdamW(
        fsdp_model.parameters(),
        lr=1e-4,
        fused=True,
    )
    grad_accum_steps = batch_size // micro_batch_size

    def _step() -> float:
        optimizer.zero_grad(set_to_none=True)
        for _ in range(grad_accum_steps):
            output = input_tensor
            for layer in fsdp_model:
                output = layer(output)
            loss = output.mean() / grad_accum_steps
            loss.backward()
        optimizer.step()
        return float(loss.detach().item() * grad_accum_steps)

    for _ in range(max(warmup, 0)):
        _step()
    torch.cuda.synchronize(device)

    start = time.perf_counter()
    loss_value = 0.0
    for _ in range(max(iters, 1)):
        loss_value = _step()
    torch.cuda.synchronize(device)
    elapsed = time.perf_counter() - start

    tokens_per_iter = batch_size * seq_length
    tokens_per_s = tokens_per_iter * (max(iters, 1) / max(elapsed, 1e-9))

    if rank == 0:
        print(f"rank0 tokens/s: {tokens_per_s:.2f} tokens/s")
        print(f"rank0 time_per_iter_ms: {(elapsed / max(iters,1)) * 1000.0:.3f}")
        print(f"rank0 loss: {loss_value:.6f}")

    dist.barrier()
    dist.destroy_process_group()


def main() -> None:
    parser = argparse.ArgumentParser(description="Optimized FSDP2 FP8 training")
    parser.add_argument("--iters", type=int, default=3)
    parser.add_argument("--warmup", type=int, default=5)
    parser.add_argument("--batch-size", type=int, default=_DEFAULT_BATCH_SIZE)
    parser.add_argument("--micro-batch-size", type=int, default=_DEFAULT_MICRO_BATCH)
    parser.add_argument("--seq-length", type=int, default=_DEFAULT_SEQ_LEN)
    parser.add_argument("--hidden-size", type=int, default=_DEFAULT_HIDDEN)
    parser.add_argument("--num-layers", type=int, default=_DEFAULT_LAYERS)
    args = parser.parse_args()

    _run_worker(
        iters=args.iters,
        warmup=args.warmup,
        batch_size=args.batch_size,
        micro_batch_size=args.micro_batch_size,
        seq_length=args.seq_length,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
    )


def run_benchmark(**_kwargs) -> dict:
    """Legacy entrypoint; use torchrun via the harness for multi-GPU runs."""
    raise RuntimeError("optimized_fsdp2_standalone must be executed via torchrun.")


class OptimizedFSDP2Benchmark(BaseBenchmark):
    """Harness entry that launches this module via torchrun."""

    verification_not_applicable_reason = "torchrun benchmarks execute in external processes"
    skip_input_check = True
    skip_output_check = True

    def __init__(self) -> None:
        super().__init__()
        self.register_workload_metadata(requests_per_iteration=float(_DEFAULT_BATCH_SIZE))

    def benchmark_fn(self) -> None:
        raise RuntimeError("SKIPPED: optimized_fsdp2_standalone requires torchrun")

    def get_config(self) -> BenchmarkConfig:
        return BenchmarkConfig(
            launch_via=LaunchVia.TORCHRUN,
            nproc_per_node=2,
            iterations=3,
            warmup=5,
            multi_gpu_required=True,
            measurement_timeout_seconds=900,
        )

    def get_torchrun_spec(self, config: Optional[BenchmarkConfig] = None) -> TorchrunLaunchSpec:
        return TorchrunLaunchSpec(
            script_path=Path(__file__).resolve(),
            script_args=[],
            multi_gpu_required=True,
            name="optimized_fsdp2_standalone",
            config_arg_map={
                "iterations": "--iters",
                "warmup": "--warmup",
            },
        )

    def get_input_signature(self) -> dict:
        signature = simple_signature(
            batch_size=_DEFAULT_BATCH_SIZE,
            dtype="bfloat16",
            seq_length=_DEFAULT_SEQ_LEN,
            hidden_size=_DEFAULT_HIDDEN,
            num_layers=_DEFAULT_LAYERS,
            micro_batch_size=_DEFAULT_MICRO_BATCH,
            precision_flags=PrecisionFlags(fp8=True, bf16=True, tf32=False),
        )
        signature.world_size = 2
        return signature

    def get_output_tolerance(self) -> tuple:
        return (0.1, 1.0)


def get_benchmark() -> BaseBenchmark:
    return OptimizedFSDP2Benchmark()


if __name__ == "__main__":
    main()
