"""Baseline FSDP2 example using composable fully_shard (multi-GPU)."""

from __future__ import annotations

import argparse
import os
from pathlib import Path

import torch
import torch.distributed as dist
from torch.distributed._composable.fsdp import MixedPrecisionPolicy, fully_shard
from torch.distributed.device_mesh import init_device_mesh
from torch.utils.data import DataLoader, DistributedSampler

from core.benchmark.gpu_requirements import require_min_gpus
from labs.train_distributed.training_utils.torchrun_harness import TorchrunScriptBenchmark
from labs.train_distributed.utils import (
    ThroughputTracker,
    create_collate_fn,
    get_model_flops_per_token,
    gpu_memory_usage,
    load_tinystories_packed,
    load_tinystories,
    setup_tokenizer,
)

MODEL_ID = "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
LOCAL_DATA_PATH = Path(__file__).parent / "data" / "tinystories_sample.jsonl"


def parse_args():
    parser = argparse.ArgumentParser(description="Baseline FSDP2 BF16 training")
    parser.add_argument("--steps", type=int, default=100, help="Number of optimizer steps to run")
    parser.add_argument("--sequence-length", type=int, default=4096)
    parser.add_argument("--micro-batch-size", type=int, default=1, help="Per-rank microbatch size")
    parser.add_argument("--grad-accum", type=int, default=1, help="Gradient accumulation steps")
    parser.add_argument("--learning-rate", type=float, default=1e-4)
    return parser.parse_args()


def _init_distributed() -> tuple[int, int, int]:
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)
    if not dist.is_initialized():
        dist.init_process_group(backend="nccl", device_id=local_rank)
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    return rank, world_size, local_rank


def _build_dataloader(
    seq_len: int,
    micro_batch: int,
    rank: int,
    world_size: int,
    *,
    steps: int,
    grad_accum: int,
):
    fast_mode = os.getenv("AISP_FSDP_FAST") == "1"
    if fast_mode:
        vocab_size = int(os.getenv("AISP_TINYSTORIES_VOCAB", "32000"))
        num_samples = max(steps * grad_accum * micro_batch * world_size, 256)
        input_ids = torch.randint(0, vocab_size, (num_samples, seq_len), dtype=torch.long)
        labels = input_ids.clone()

        class SyntheticTokenDataset(torch.utils.data.Dataset):
            def __init__(self, input_ids: torch.Tensor, labels: torch.Tensor):
                self._input_ids = input_ids
                self._labels = labels

            def __len__(self) -> int:
                return self._input_ids.size(0)

            def __getitem__(self, idx: int):
                return {"input_ids": self._input_ids[idx], "labels": self._labels[idx]}

        dataset = SyntheticTokenDataset(input_ids, labels)
        sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True, drop_last=True)
        dataloader = DataLoader(
            dataset,
            batch_size=micro_batch,
            sampler=sampler,
            num_workers=0,
            pin_memory=True,
            drop_last=False,
        )
        return dataloader, sampler

    packed_path = os.getenv("AISP_TINYSTORIES_PACKED_PATH")
    if packed_path:
        dataset = load_tinystories_packed(packed_path, seq_len, is_main_process=rank == 0)
    else:
        tokenizer = setup_tokenizer(MODEL_ID)
        dataset = load_tinystories(tokenizer, seq_len, is_main_process=rank == 0)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True, drop_last=True)
    dataloader = DataLoader(
        dataset,
        batch_size=micro_batch,
        sampler=sampler,
        num_workers=0,
        pin_memory=True,
        drop_last=False,
        collate_fn=create_collate_fn(),
    )
    return dataloader, sampler


def _apply_fsdp2(model: torch.nn.Module, mesh) -> torch.nn.Module:
    try:
        from transformers.models.llama.modeling_llama import LlamaDecoderLayer
    except ImportError as exc:
        raise RuntimeError("_apply_fsdp2() requires the `transformers` package") from exc
    mp_policy = MixedPrecisionPolicy(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        output_dtype=torch.bfloat16,
    )
    for module in model.modules():
        if module is model:
            continue
        if isinstance(module, LlamaDecoderLayer):
            fully_shard(module, mesh=mesh, mp_policy=mp_policy)
    fully_shard(model, mesh=mesh, mp_policy=mp_policy)
    return model


def main():
    require_min_gpus(2, script_name="baseline_fsdp2_multigpu.py")
    try:
        from transformers import AutoConfig, AutoModelForCausalLM
    except ImportError as exc:
        raise RuntimeError("baseline_fsdp2_multigpu requires the `transformers` package") from exc

    args = parse_args()
    rank, world_size, local_rank = _init_distributed()

    dataloader, sampler = _build_dataloader(
        args.sequence_length,
        args.micro_batch_size,
        rank,
        world_size,
        steps=args.steps,
        grad_accum=args.grad_accum,
    )
    if rank == 0:
        print("[baseline_fsdp2_multigpu] dataloader ready", flush=True)

    fast_mode = os.getenv("AISP_FSDP_FAST") == "1"
    if fast_mode:
        from transformers import LlamaConfig

        config = LlamaConfig(
            vocab_size=int(os.getenv("AISP_TINYSTORIES_VOCAB", "32000")),
            hidden_size=1024,
            intermediate_size=4096,
            num_hidden_layers=4,
            num_attention_heads=16,
            num_key_value_heads=4,
            max_position_embeddings=max(args.sequence_length, 2048),
            rms_norm_eps=1e-5,
            use_cache=False,
        )
    else:
        config_path = os.getenv("AISP_TINYSTORIES_CONFIG_PATH")
        if config_path:
            config = AutoConfig.from_pretrained(config_path)
            config.use_cache = False
            config.attn_implementation = "eager"
        else:
            config = AutoConfig.from_pretrained(MODEL_ID, use_cache=False, attn_implementation="eager")
    override_layers = os.getenv("AISP_TINYSTORIES_LAYERS")
    if override_layers:
        layers = int(override_layers)
        if layers < 1 or layers > config.num_hidden_layers:
            raise ValueError(
                f"AISP_TINYSTORIES_LAYERS must be between 1 and {config.num_hidden_layers}, got {layers}."
            )
        config.num_hidden_layers = layers
    if getattr(config, "max_position_embeddings", 0) < args.sequence_length:
        config.max_position_embeddings = args.sequence_length

    model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16, attn_implementation="eager")
    if rank == 0:
        print("[baseline_fsdp2_multigpu] model instantiated", flush=True)

    model = model.to(torch.cuda.current_device(), dtype=torch.bfloat16)
    mesh = init_device_mesh("cuda", (world_size,))
    model = _apply_fsdp2(model, mesh)

    if rank == 0:
        print("[baseline_fsdp2_multigpu] fsdp2 applied", flush=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.95), weight_decay=0.1)
    if rank == 0:
        print("[baseline_fsdp2_multigpu] optimizer ready", flush=True)

    flop_per_token = get_model_flops_per_token(model.config, args.sequence_length)
    tracker = ThroughputTracker(warmup_steps=5)

    total_updates = args.steps
    is_main = rank == 0
    optimizer_step = 0
    micro_step = 0
    epoch = 0

    while optimizer_step < total_updates:
        sampler.set_epoch(epoch)
        for batch in dataloader:
            batch = {k: v.cuda(non_blocking=True) for k, v in batch.items()}
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                outputs = model(**batch)
                loss = outputs.loss / args.grad_accum

            loss.backward()
            micro_step += 1
            loss_value = loss.item() * args.grad_accum

            should_step = micro_step % args.grad_accum == 0
            if should_step:
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
                optimizer_step += 1

            metrics = tracker.step(batch["input_ids"].numel(), flop_per_token)
            if (
                metrics
                and should_step
                and is_main
                and (optimizer_step % 5 == 0 or optimizer_step == total_updates)
            ):
                metrics.update(gpu_memory_usage(local_rank))
                msg = (
                    f"[baseline_fsdp2_multigpu] step {optimizer_step}/{total_updates} "
                    f"loss={loss_value:.4f}" + ThroughputTracker.format(metrics, include_memory=True)
                )
                print(msg, flush=True)

            if optimizer_step >= total_updates:
                break

        epoch += 1

    dist.barrier()
    if is_main:
        print("[baseline_fsdp2_multigpu] training completed", flush=True)

    dist.destroy_process_group()


if __name__ == "__main__":
    main()


def get_benchmark():
    """Expose torchrun-wrapped benchmark for the harness."""
    # Scale up by switching to a larger config (ex: tinyllama_config.json)
    # and matching it with a packed dataset at the desired sequence length.
    packed_data_path = Path(__file__).parent / "data" / "tinystories_packed_seq128.jsonl"
    config_path = Path(__file__).parent / "data" / "tinyllama_config.json"
    return TorchrunScriptBenchmark(
        script_path=Path(__file__).parent / "train_fsdp2.py",
        base_args=[
            "--mode",
            "baseline",
            "--variant",
            "multigpu",
            "--sequence-length",
            "128",
            "--micro-batch-size",
            "4",
            "--grad-accum",
            "2",
        ],
        config_arg_map={"iterations": "--steps"},
        multi_gpu_required=True,
        target_label="labs/train_distributed:fsdp2_multigpu",
        default_nproc_per_node=None,
        default_iterations=200,
        measurement_timeout_seconds=900,
        env={
            "AISP_TINYSTORIES_LOCAL_PATH": str(LOCAL_DATA_PATH),
            "AISP_TINYSTORIES_PACKED_PATH": str(packed_data_path),
            "AISP_TINYSTORIES_CONFIG_PATH": str(config_path),
            "AISP_TINYSTORIES_LAYERS": "4",
            "TOKENIZERS_PARALLELISM": "false",
        },
        name="baseline_fsdp2_multigpu",
    )
