"""Paged/NVMe-style KV-cache offload microbenchmarks with fused FP8 gating.

This module provides a small, GPU-backed benchmark that simulates:
1) A hot KV window that remains resident on the GPU.
2) Cold KV pages that live in either pageable CPU memory or an NVMe-backed
   memmap file.
3) Optional pinned staging + async H2D copies for overlap.
4) FP8 KV usage that is only enabled when a fused FlashAttention-style path is
   likely to exist (B200/GB200 or newer).

The goal is to encode the practical rule from the post:
- Use FP8 KV only when a fused attention kernel is available; otherwise fall
  back to FP16 to avoid paying dequant cost with no speedup.
- Use paged/NVMe-style offload when context length forces it, and measure the
  TTFT impact of pulling pages back in.
"""

from __future__ import annotations

import os
import tempfile
from contextlib import nullcontext
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple

import numpy as np
import torch
import torch.nn.functional as F

from core.benchmark.verification_mixin import VerificationPayloadMixin
from core.harness.benchmark_harness import BaseBenchmark, BenchmarkConfig

try:  # Prefer the Blackwell-aware SDPA selector when available.
    from arch_config import prefer_sdpa_backends  # type: ignore
except Exception:  # pragma: no cover - defensive import
    prefer_sdpa_backends = None  # type: ignore




def _supports_fp8_kv() -> bool:
    """Return True if FP8 KV is even representable in this build of PyTorch."""
    return hasattr(torch, "float8_e4m3fn") and torch.cuda.is_available()


def _supports_fused_fp8_attention() -> bool:
    """Return True if this runtime can execute FP8 SDPA for (q, k, v).

    PyTorch builds vary in FP8 kernel availability; rely on a minimal runtime
    probe rather than GPU CC heuristics to avoid hard failures like:
    "No available kernel. Aborting execution."
    """
    if not torch.cuda.is_available():
        return False
    fp8_dtype = getattr(torch, "float8_e4m3fn", None)
    if fp8_dtype is None:
        return False
    try:
        q = torch.randn((1, 1, 16, 64), device="cuda", dtype=torch.float16).to(fp8_dtype)
        ctx = prefer_sdpa_backends() if prefer_sdpa_backends is not None else nullcontext()
        with ctx:
            _ = F.scaled_dot_product_attention(q, q, q)
        return True
    except Exception:
        return False


def _np_dtype_for(torch_dtype: torch.dtype) -> np.dtype:
    """Map a torch dtype to a numpy dtype used for the memmap backing store."""
    float8_e4m3 = getattr(torch, "float8_e4m3fn", None)
    float8_e5m2 = getattr(torch, "float8_e5m2fn", None)
    if torch_dtype in {float8_e4m3, float8_e5m2}:
        # memmap sticks to fp16; conversion to fp8 happens during staging/H2D.
        return np.float16
    return torch.empty([], dtype=torch_dtype).numpy().dtype


@dataclass
class PagedKVConfig:
    """Configuration for paged KV-cache simulation."""

    batch_size: int = 2
    num_heads: int = 16
    head_dim: int = 128
    max_seq_len: int = 8192
    page_tokens: int = 512
    decode_tokens: int = 64
    repeat_pages: int = 8
    use_pinned_stage: bool = False
    use_async_stream: bool = False
    use_memmap: bool = False  # When True, store cold pages on disk to mimic NVMe.
    prefer_fp8: bool = True
    require_fused_fp8: bool = False  # If True, FP8 is only used when fused path is present.
    fallback_dtype: torch.dtype = torch.float16
    prefetch_next_page: bool = False
    use_direct_h2d: bool = False


class PagedKVOffloadBenchmark(VerificationPayloadMixin, BaseBenchmark):
    """Synthetic decode microbenchmark with paged KV offload and FP8 gating."""

    def __init__(self, cfg: Optional[PagedKVConfig] = None, label: str = "paged_kv_offload"):
        super().__init__()
        self.cfg = cfg or PagedKVConfig()
        self.label = label

        self.runtime_dtype: torch.dtype = self.cfg.fallback_dtype
        self.enable_flash: bool = False
        self._fp8_reason: str = ""

        self.hot_k: Optional[torch.Tensor] = None
        self.hot_v: Optional[torch.Tensor] = None
        self.hot_k_bufs: list[torch.Tensor] = []
        self.hot_v_bufs: list[torch.Tensor] = []
        self.active_buf_idx: int = 0
        self.prefetch_buf_idx: Optional[int] = None
        self.prefetch_slice_len: Optional[int] = None
        self.prefetch_event: Optional[torch.cuda.Event] = None
        self.staging: Optional[torch.Tensor] = None
        self.prefetch_staging: Optional[torch.Tensor] = None
        self.prefetched_range: Optional[Tuple[int, int]] = None
        self.copy_stream: Optional[torch.cuda.Stream] = None
        self.q: Optional[torch.Tensor] = None

        self.host_cache: Optional[torch.Tensor] = None
        self.host_memmap: Optional[np.memmap] = None
        self._memmap_path: Optional[Path] = None

        self.page_cursor: int = 0
        self._bytes_per_iteration: float = 0.0
        self.output: Optional[torch.Tensor] = None
        self.register_workload_metadata(requests_per_iteration=1.0)

    # -------------------- Setup helpers --------------------

    def _select_runtime_dtype(self) -> torch.dtype:
        if self.cfg.prefer_fp8 and _supports_fp8_kv():
            if not _supports_fused_fp8_attention():
                self._fp8_reason = (
                    "FP8 requested but FP8 SDPA kernel unavailable; "
                    f"falling back to {self.cfg.fallback_dtype}."
                )
                return self.cfg.fallback_dtype
            self._fp8_reason = "Using FP8 KV: FP8 SDPA kernel available."
            return torch.float8_e4m3fn  # type: ignore[attr-defined]
        return self.cfg.fallback_dtype

    def _init_host_cache(self, shape: Tuple[int, ...]) -> None:
        generator = torch.Generator().manual_seed(42)
        if self.cfg.use_memmap:
            np_dtype = _np_dtype_for(self.runtime_dtype)
            tmp_dir = Path(tempfile.mkdtemp(prefix="paged_kv_cache_"))
            self._memmap_path = tmp_dir / "kv_cache.bin"
            self.host_memmap = np.memmap(self._memmap_path, mode="w+", dtype=np_dtype, shape=shape)
            host = torch.randn(shape, dtype=torch.float16, generator=generator).numpy().astype(np_dtype, copy=False)
            self.host_memmap[:] = host
        else:
            self.host_cache = torch.randn(
                shape,
                dtype=torch.float16,
                generator=generator,
                pin_memory=self.cfg.use_pinned_stage,
            )

    def _stage_page(self, start: int, into_prefetch: bool = False) -> Tuple[torch.Tensor, int]:
        end = min(start + self.cfg.page_tokens, self.cfg.max_seq_len)
        slice_len = end - start
        target = self.prefetch_staging if into_prefetch else self.staging
        assert target is not None

        if self.host_memmap is not None:
            np_slice = self.host_memmap[..., start:end, :]
            target[..., :slice_len, :].copy_(torch.from_numpy(np_slice))
        elif self.host_cache is not None:
            target[..., :slice_len, :].copy_(self.host_cache[..., start:end, :])
        else:
            raise RuntimeError("Host cache not initialized")
        return target, slice_len

    def _copy_to_device(
        self,
        staged: torch.Tensor,
        slice_len: int,
        buffer_idx: int = 0,
        wait_for_copy: bool = True,
        record_event: Optional[torch.cuda.Event] = None,
    ) -> None:
        if not self.hot_k_bufs or not self.hot_v_bufs:
            raise RuntimeError("Device KV buffers are not initialized")
        target_k = self.hot_k_bufs[buffer_idx]
        target_v = self.hot_v_bufs[buffer_idx]
        direct_copy = self.cfg.use_direct_h2d and staged.dtype == self.runtime_dtype

        def _copy_planes() -> None:
            src_k = staged[0, ..., :slice_len, :]
            src_v = staged[1, ..., :slice_len, :]
            if direct_copy:
                target_k[..., :slice_len, :].copy_(src_k, non_blocking=self.cfg.use_pinned_stage)
                target_v[..., :slice_len, :].copy_(src_v, non_blocking=self.cfg.use_pinned_stage)
            else:
                target_k[..., :slice_len, :].copy_(
                    src_k.to(self.device, dtype=self.runtime_dtype, non_blocking=self.cfg.use_pinned_stage)
                )
                target_v[..., :slice_len, :].copy_(
                    src_v.to(self.device, dtype=self.runtime_dtype, non_blocking=self.cfg.use_pinned_stage)
                )

        if self.copy_stream is not None:
            with torch.cuda.stream(self.copy_stream):
                _copy_planes()
                if record_event is not None:
                    record_event.record()
            if wait_for_copy:
                torch.cuda.current_stream().wait_stream(self.copy_stream)
        else:
            _copy_planes()
            if record_event is not None:
                record_event.record()

    def setup(self) -> None:
        torch.manual_seed(42)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(42)
        self.runtime_dtype = self._select_runtime_dtype()
        self.enable_flash = _supports_fused_fp8_attention() or self.runtime_dtype in (torch.float16, torch.bfloat16)

        head_shape = (
            self.cfg.batch_size,
            self.cfg.num_heads,
            self.cfg.page_tokens,
            self.cfg.head_dim,
        )
        buffer_count = 2 if (self.cfg.prefetch_next_page and self.cfg.use_async_stream) else 1
        self.hot_k_bufs = [torch.zeros(head_shape, device=self.device, dtype=self.runtime_dtype) for _ in range(buffer_count)]
        self.hot_v_bufs = [torch.zeros_like(self.hot_k_bufs[0]) for _ in range(buffer_count)]
        self.hot_k = self.hot_k_bufs[0]
        self.hot_v = self.hot_v_bufs[0]
        self.active_buf_idx = 0
        self.prefetch_buf_idx = None
        self.prefetch_slice_len = None
        self.prefetch_event = torch.cuda.Event() if buffer_count == 2 else None

        staging_dtype = torch.float16
        staging_shape = (
            2,  # k and v planes
            self.cfg.batch_size,
            self.cfg.num_heads,
            self.cfg.page_tokens,
            self.cfg.head_dim,
        )
        self.staging = torch.empty(
            staging_shape,
            device="cpu",
            dtype=staging_dtype,
            pin_memory=self.cfg.use_pinned_stage,
        )
        if self.cfg.prefetch_next_page:
            self.prefetch_staging = torch.empty(
                staging_shape,
                device="cpu",
                dtype=staging_dtype,
                pin_memory=self.cfg.use_pinned_stage,
            )
        self.copy_stream = torch.cuda.Stream() if self.cfg.use_async_stream else None

        host_shape = (
            2,  # k and v
            self.cfg.batch_size,
            self.cfg.num_heads,
            self.cfg.max_seq_len,
            self.cfg.head_dim,
        )
        self._init_host_cache(host_shape)

        bytes_per_page = (
            2
            * self.cfg.batch_size
            * self.cfg.num_heads
            * self.cfg.page_tokens
            * self.cfg.head_dim
            * torch.finfo(self.runtime_dtype).bits
            / 8.0
        )
        self._bytes_per_iteration = float(bytes_per_page * max(1, self.cfg.repeat_pages))
        self.register_workload_metadata(bytes_per_iteration=self._bytes_per_iteration)

        # Precompute a deterministic query tensor in the runtime dtype.
        # Some PyTorch builds lack RNG kernels for FP8; generate in FP16 and cast.
        q_dtype = self.runtime_dtype
        fp8_e4m3 = getattr(torch, "float8_e4m3fn", None)
        fp8_e5m2 = getattr(torch, "float8_e5m2fn", None)
        needs_cast = q_dtype in (fp8_e4m3, fp8_e5m2)
        gen_dtype = torch.float16 if needs_cast else q_dtype
        q = torch.randn(
            self.cfg.batch_size,
            self.cfg.num_heads,
            self.cfg.decode_tokens,
            self.cfg.head_dim,
            device=self.device,
            dtype=gen_dtype,
        )
        if needs_cast:
            q = q.to(dtype=q_dtype)
        self.q = q

    # -------------------- Benchmark --------------------

    def _maybe_use_prefetch(self, start: int) -> Optional[Tuple[torch.Tensor, int]]:
        if self.prefetched_range is None or self.prefetch_staging is None:
            return None
        pref_start, pref_end = self.prefetched_range
        if start != pref_start:
            return None
        return self.prefetch_staging, pref_end - pref_start

    def benchmark_fn(self) -> None:
        repeats = max(1, self.cfg.repeat_pages)
        attn_out = None
        for _ in range(repeats):
            start = self.page_cursor
            active_idx = self.active_buf_idx
            prefetched = self._maybe_use_prefetch(start)
            if prefetched is not None:
                staged, slice_len = prefetched
                if self.prefetch_event is not None and self.prefetch_buf_idx is not None:
                    torch.cuda.current_stream().wait_event(self.prefetch_event)
                    active_idx = self.prefetch_buf_idx
                else:
                    self._copy_to_device(staged, slice_len, buffer_idx=active_idx, wait_for_copy=True)
            else:
                staged, slice_len = self._stage_page(start)
                self._copy_to_device(staged, slice_len, buffer_idx=active_idx, wait_for_copy=True)

            next_start = (start + self.cfg.page_tokens) % self.cfg.max_seq_len
            if self.cfg.prefetch_next_page:
                # Launch next-page prefetch early so H2D can overlap attention compute.
                staged_prefetch, pref_len = self._stage_page(next_start, into_prefetch=True)
                self.prefetched_range = (next_start, next_start + pref_len)
                self.prefetch_slice_len = pref_len
                if self.copy_stream is not None and len(self.hot_k_bufs) > 1:
                    prefetch_idx = 1 - active_idx
                    self.prefetch_buf_idx = prefetch_idx
                    if self.prefetch_event is None:
                        self.prefetch_event = torch.cuda.Event()
                    self._copy_to_device(
                        staged_prefetch,
                        pref_len,
                        buffer_idx=prefetch_idx,
                        wait_for_copy=False,
                        record_event=self.prefetch_event,
                    )
                else:
                    self.prefetch_buf_idx = None
            else:
                self.prefetched_range = None
                self.prefetch_slice_len = None
                self.prefetch_buf_idx = None

            self.hot_k = self.hot_k_bufs[active_idx]
            self.hot_v = self.hot_v_bufs[active_idx]

            # Simple attention step that will pick flash/mathematics based on dtype/backend.
            q = self.q
            if q is None:
                raise RuntimeError("Query tensor not initialized")
            k = self.hot_k[..., :slice_len, :]
            v = self.hot_v[..., :slice_len, :]
            ctx = prefer_sdpa_backends() if prefer_sdpa_backends is not None else nullcontext()
            with ctx:
                attn_out = F.scaled_dot_product_attention(q, k, v)

            self.page_cursor = next_start

        if attn_out is None:
            raise RuntimeError("benchmark_fn() did not produce output")
        # Capture a slice of attention output for verification
        self.output = attn_out[:, :, :1, : min(8, attn_out.shape[-1])].detach().float().clone()
        fp8_dtype = getattr(torch, "float8_e4m3fn", None)
        fp8_enabled = fp8_dtype is not None and self.runtime_dtype == fp8_dtype
        self._payload_fp8_enabled = fp8_enabled
        self._payload_k = k
        self._payload_q = q
        self._payload_v = v

    def capture_verification_payload(self) -> None:
        fp8_enabled = self._payload_fp8_enabled
        k = self._payload_k
        q = self._payload_q
        v = self._payload_v
        self._set_verification_payload(
            inputs={"q": q.detach(), "k": k.detach(), "v": v.detach()},
            output=self.output,
            batch_size=self.cfg.batch_size,
            parameter_count=0,
            precision_flags={
                "fp16": self.runtime_dtype == torch.float16,
                "bf16": self.runtime_dtype == torch.bfloat16,
                "fp8": fp8_enabled,
                "tf32": torch.backends.cuda.matmul.allow_tf32,
            },
            output_tolerance=(0.1, 1.0),
        )

    # -------------------- Teardown --------------------

    def teardown(self) -> None:
        self.hot_k = None
        self.hot_v = None
        self.hot_k_bufs = []
        self.hot_v_bufs = []
        self.active_buf_idx = 0
        self.prefetch_buf_idx = None
        self.prefetch_slice_len = None
        self.prefetch_event = None
        self.q = None
        self.staging = None
        self.prefetch_staging = None
        self.copy_stream = None
        self.host_cache = None
        if self.host_memmap is not None:
            self.host_memmap._mmap.close()  # type: ignore[attr-defined]
        self.host_memmap = None
        if self._memmap_path is not None:
            try:
                os.remove(self._memmap_path)
                os.rmdir(self._memmap_path.parent)
            except OSError:
                pass
        self._memmap_path = None
        self.output = None
        super().teardown()

    # -------------------- Harness config --------------------

    def get_config(self) -> BenchmarkConfig:
        return BenchmarkConfig(
            iterations=12,
            warmup=5,
            timeout_seconds=300,
            measurement_timeout_seconds=300,
            deterministic=False,
        )

    def validate_result(self) -> Optional[str]:
        if self._fp8_reason and self.cfg.prefer_fp8:
            # Report the path we took for visibility; not a failure.
            print(f"[{self.label}] {self._fp8_reason}")
        return None

    def get_custom_metrics(self) -> Optional[dict]:
        """Return paged KV offload performance metrics."""
        return {
            f"{self.label}.bytes_per_iteration": self._bytes_per_iteration,
            f"{self.label}.batch_size": float(self.cfg.batch_size),
            f"{self.label}.num_heads": float(self.cfg.num_heads),
            f"{self.label}.head_dim": float(self.cfg.head_dim),
            f"{self.label}.page_tokens": float(self.cfg.page_tokens),
            f"{self.label}.decode_tokens": float(self.cfg.decode_tokens),
            f"{self.label}.max_seq_len": float(self.cfg.max_seq_len),
            f"{self.label}.use_fp8": float(self.runtime_dtype == getattr(torch, "float8_e4m3fn", torch.float16)),
            f"{self.label}.use_pinned": float(self.cfg.use_pinned_stage),
            f"{self.label}.use_async": float(self.cfg.use_async_stream),
        }

 
