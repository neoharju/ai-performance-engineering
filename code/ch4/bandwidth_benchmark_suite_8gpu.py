"""
Comprehensive Bandwidth Benchmark Suite for 8x B200 GPUs
=========================================================

Measures all communication patterns and generates detailed analysis:
1. P2P bandwidth matrix (all 28 GPU pairs)
2. Collective operations (AllReduce, AllGather, ReduceScatter)
3. Latency vs bandwidth curves
4. NVLink topology visualization
5. Scaling efficiency analysis

Hardware:
- 8x Blackwell B200 GPUs
- NVLink 5.0: 1800 GB/s bidirectional per GPU pair
- Total bandwidth: 62.4 TB/s aggregate

Expected Results:
- P2P: 800-900 GB/s per GPU pair
- AllReduce 1GB: 700-800 GB/s bus bandwidth
- Small message latency: <2 μs

Requirements:
- PyTorch 2.9+
- 8 GPUs (requires torchrun)

Usage:
    torchrun --nproc_per_node=8 bandwidth_benchmark_suite_8gpu.py --full

    # Quick test
    torchrun --nproc_per_node=8 bandwidth_benchmark_suite_8gpu.py --quick

    # Save results to file
    torchrun --nproc_per_node=8 bandwidth_benchmark_suite_8gpu.py --output results.json
"""

import os
import time
import json
import argparse
from typing import Dict, List, Tuple
import torch
import torch.distributed as dist


def setup_distributed():
    """Initialize distributed environment."""
    if not dist.is_initialized():
        rank = int(os.environ.get("RANK", 0))
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        local_rank = int(os.environ.get("LOCAL_RANK", rank))
        
        dist.init_process_group(backend="nccl", init_method="env://")
        torch.cuda.set_device(local_rank)
    
    return dist.get_rank(), dist.get_world_size()


# ============================================================================
# P2P Bandwidth Matrix
# ============================================================================

def benchmark_p2p_bandwidth(
    src_rank: int,
    dst_rank: int,
    size_mb: int = 256,
    iterations: int = 100,
) -> float:
    """
    Benchmark point-to-point bandwidth between two GPUs.
    
    Args:
        src_rank: Source GPU rank
        dst_rank: Destination GPU rank
        size_mb: Transfer size in MB
        iterations: Number of iterations
        
    Returns:
        Bandwidth in GB/s
    """
    rank = dist.get_rank()
    device = torch.cuda.current_device()
    
    size = size_mb * 1024 * 1024 // 4  # Convert to float32 elements
    tensor = torch.randn(size, device=device, dtype=torch.float32)
    
    # Warmup
    for _ in range(10):
        if rank == src_rank:
            dist.send(tensor, dst=dst_rank)
        elif rank == dst_rank:
            recv_tensor = torch.empty_like(tensor)
            dist.recv(recv_tensor, src=src_rank)
    
    torch.cuda.synchronize()
    dist.barrier()
    
    # Benchmark
    if rank == src_rank:
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        start.record()
        for _ in range(iterations):
            dist.send(tensor, dst=dst_rank)
        end.record()
        end.synchronize()
        
        elapsed_ms = start.elapsed_time(end) / iterations
        bandwidth_gbs = (size_mb / 1024) / (elapsed_ms / 1000)
        
        return bandwidth_gbs
    elif rank == dst_rank:
        for _ in range(iterations):
            recv_tensor = torch.empty_like(tensor)
            dist.recv(recv_tensor, src=src_rank)
        
        return 0.0  # Only src_rank returns valid bandwidth
    else:
        return 0.0


def measure_p2p_matrix(rank: int, world_size: int) -> Dict[Tuple[int, int], float]:
    """
    Measure P2P bandwidth for all GPU pairs.
    
    Returns:
        Dictionary mapping (src, dst) pairs to bandwidth in GB/s
    """
    if rank == 0:
        print("\n" + "=" * 80)
        print("P2P Bandwidth Matrix (8x8 = 28 unique pairs)")
        print("=" * 80)
        print("Testing bidirectional bandwidth for all GPU pairs...")
        print("Transfer size: 256 MB per direction\n")
    
    bandwidth_matrix = {}
    
    # Test all pairs
    for src in range(world_size):
        for dst in range(src + 1, world_size):
            # Test src -> dst
            bw = benchmark_p2p_bandwidth(src, dst, size_mb=256, iterations=50)
            if rank == src:
                bandwidth_matrix[(src, dst)] = bw
            
            # Broadcast result to all ranks
            bw_tensor = torch.tensor([bw], device='cuda')
            dist.broadcast(bw_tensor, src=src)
            bandwidth_matrix[(src, dst)] = bw_tensor.item()
            
            if rank == 0:
                print(f"  GPU {src} → GPU {dst}: {bw_tensor.item():6.2f} GB/s")
    
    if rank == 0:
        # Calculate statistics
        bandwidths = list(bandwidth_matrix.values())
        avg_bw = sum(bandwidths) / len(bandwidths)
        min_bw = min(bandwidths)
        max_bw = max(bandwidths)
        
        print(f"\nStatistics:")
        print(f"  Average: {avg_bw:.2f} GB/s")
        print(f"  Min: {min_bw:.2f} GB/s")
        print(f"  Max: {max_bw:.2f} GB/s")
        print(f"  Target: 800-900 GB/s per pair (NVLink 5.0)")
        print("=" * 80)
    
    return bandwidth_matrix


# ============================================================================
# Collective Operations Benchmark
# ============================================================================

def benchmark_collective(
    op_type: str,
    size: int,
    rank: int,
    world_size: int,
    iterations: int = 100,
) -> Tuple[float, float]:
    """
    Benchmark a collective operation.
    
    Args:
        op_type: "allreduce", "allgather", or "reducescatter"
        size: Tensor size in elements
        rank: Current rank
        world_size: Total ranks
        iterations: Number of iterations
        
    Returns:
        (latency_ms, bandwidth_gbs)
    """
    device = torch.cuda.current_device()
    tensor = torch.randn(size, device=device, dtype=torch.float32)
    
    # Warmup
    for _ in range(10):
        if op_type == "allreduce":
            dist.all_reduce(tensor.clone())
        elif op_type == "allgather":
            output = [torch.empty_like(tensor) for _ in range(world_size)]
            dist.all_gather(output, tensor)
        elif op_type == "reducescatter":
            output = torch.empty(size // world_size, device=device, dtype=torch.float32)
            input_list = list(tensor.chunk(world_size))
            dist.reduce_scatter(output, input_list)
    
    torch.cuda.synchronize()
    dist.barrier()
    
    # Benchmark
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    start.record()
    for _ in range(iterations):
        if op_type == "allreduce":
            dist.all_reduce(tensor.clone())
        elif op_type == "allgather":
            output = [torch.empty_like(tensor) for _ in range(world_size)]
            dist.all_gather(output, tensor)
        elif op_type == "reducescatter":
            output = torch.empty(size // world_size, device=device, dtype=torch.float32)
            input_list = list(tensor.chunk(world_size))
            dist.reduce_scatter(output, input_list)
    end.record()
    end.synchronize()
    
    latency_ms = start.elapsed_time(end) / iterations
    
    # Calculate bandwidth
    data_bytes = size * 4  # float32
    if op_type == "allreduce":
        # AllReduce: 2*(N-1)/N algorithm bandwidth
        bandwidth_gbs = (data_bytes * 2 * (world_size - 1) / world_size) / (latency_ms / 1000) / 1e9
    elif op_type == "allgather":
        # AllGather: N * data
        bandwidth_gbs = (data_bytes * world_size) / (latency_ms / 1000) / 1e9
    elif op_type == "reducescatter":
        # ReduceScatter: (N-1)/N algorithm bandwidth
        bandwidth_gbs = (data_bytes * (world_size - 1) / world_size) / (latency_ms / 1000) / 1e9
    
    return latency_ms, bandwidth_gbs


def measure_collectives(rank: int, world_size: int, quick: bool = False) -> Dict[str, Dict]:
    """
    Measure all collective operations across different sizes.
    
    Returns:
        Dictionary with results for each operation and size
    """
    if rank == 0:
        print("\n" + "=" * 80)
        print("Collective Operations Benchmark")
        print("=" * 80)
    
    operations = ["allreduce", "allgather", "reducescatter"]
    
    if quick:
        sizes = [1024, 1024*1024, 64*1024*1024]  # 4KB, 4MB, 256MB
    else:
        sizes = [
            1024,           # 4 KB (latency-bound)
            256*1024,       # 1 MB
            1024*1024,      # 4 MB
            16*1024*1024,   # 64 MB
            64*1024*1024,   # 256 MB
            256*1024*1024,  # 1 GB (bandwidth-bound)
        ]
    
    results = {}
    
    for op in operations:
        if rank == 0:
            print(f"\n{op.upper()}:")
            print("-" * 80)
        
        results[op] = {}
        
        for size in sizes:
            latency_ms, bandwidth_gbs = benchmark_collective(
                op, size, rank, world_size, iterations=100
            )
            
            size_mb = size * 4 / (1024 * 1024)
            results[op][size_mb] = {
                "latency_ms": latency_ms,
                "bandwidth_gbs": bandwidth_gbs,
            }
            
            if rank == 0:
                print(f"  {size_mb:8.2f} MB | {latency_ms:7.2f} ms | {bandwidth_gbs:7.2f} GB/s")
    
    if rank == 0:
        print("=" * 80)
    
    return results


# ============================================================================
# Latency vs Bandwidth Curves
# ============================================================================

def measure_latency_bandwidth_curve(rank: int, world_size: int) -> Dict[int, Tuple[float, float]]:
    """
    Measure latency and bandwidth across wide range of message sizes.
    
    Returns:
        Dictionary mapping size (bytes) to (latency_us, bandwidth_gbs)
    """
    if rank == 0:
        print("\n" + "=" * 80)
        print("Latency vs Bandwidth Curve (AllReduce)")
        print("=" * 80)
        print("Measuring across 4B to 1GB range...\n")
    
    # Logarithmic sweep from 4 bytes to 1GB
    sizes_bytes = [4 * (2**i) for i in range(30)]  # 4B, 8B, 16B, ..., 1GB
    
    results = {}
    device = torch.cuda.current_device()
    
    for size_bytes in sizes_bytes:
        size_elements = size_bytes // 4
        if size_elements == 0:
            size_elements = 1
        
        tensor = torch.randn(size_elements, device=device, dtype=torch.float32)
        
        # Warmup
        for _ in range(5):
            dist.all_reduce(tensor.clone())
        torch.cuda.synchronize()
        dist.barrier()
        
        # Benchmark
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        iterations = max(10, 10000 // size_elements)  # More iterations for small sizes
        
        start.record()
        for _ in range(iterations):
            dist.all_reduce(tensor.clone())
        end.record()
        end.synchronize()
        
        latency_ms = start.elapsed_time(end) / iterations
        latency_us = latency_ms * 1000
        
        # Calculate bandwidth
        data_bytes = size_elements * 4
        bandwidth_gbs = (data_bytes * 2 * (world_size - 1) / world_size) / (latency_ms / 1000) / 1e9
        
        results[size_bytes] = (latency_us, bandwidth_gbs)
        
        if rank == 0 and size_bytes in [4, 1024, 1024*1024, 256*1024*1024]:
            if size_bytes < 1024:
                size_str = f"{size_bytes}B"
            elif size_bytes < 1024*1024:
                size_str = f"{size_bytes//1024}KB"
            else:
                size_str = f"{size_bytes//(1024*1024)}MB"
            
            print(f"  {size_str:>10s} | Latency: {latency_us:8.2f} μs | BW: {bandwidth_gbs:7.2f} GB/s")
    
    if rank == 0:
        print("\nKey Observations:")
        print("  - Small messages (<1KB): Latency-bound, <2 μs")
        print("  - Medium messages (1KB-1MB): Transition region")
        print("  - Large messages (>1MB): Bandwidth-bound, ~700-800 GB/s")
        print("=" * 80)
    
    return results


# ============================================================================
# Topology Visualization
# ============================================================================

def visualize_topology(rank: int, world_size: int, bandwidth_matrix: Dict[Tuple[int, int], float]):
    """Print ASCII visualization of GPU topology and bandwidth."""
    if rank != 0:
        return
    
    print("\n" + "=" * 80)
    print("NVLink Topology Visualization (8x B200)")
    print("=" * 80)
    
    # Print matrix
    print("\nBandwidth Matrix (GB/s):")
    print("     ", end="")
    for i in range(world_size):
        print(f"GPU{i:1d}  ", end="")
    print()
    
    for i in range(world_size):
        print(f"GPU{i}: ", end="")
        for j in range(world_size):
            if i == j:
                print("  -   ", end="")
            elif (i, j) in bandwidth_matrix:
                bw = bandwidth_matrix[(i, j)]
                print(f"{bw:5.0f} ", end="")
            elif (j, i) in bandwidth_matrix:
                bw = bandwidth_matrix[(j, i)]
                print(f"{bw:5.0f} ", end="")
            else:
                print("  ?   ", end="")
        print()
    
    # Topology detection
    print("\nTopology Analysis:")
    bandwidths = list(bandwidth_matrix.values())
    avg_bw = sum(bandwidths) / len(bandwidths)
    
    if avg_bw > 700:
        print("  ✓ NVSwitch all-to-all topology detected")
        print("  → All GPUs have direct high-bandwidth connections")
        print("  → Optimal for: AllReduce, AllGather with NVLS")
    elif avg_bw > 400:
        print("  ⚠ Partial NVLink topology")
        print("  → Some GPUs connected via multi-hop")
        print("  → Performance may vary by GPU pair")
    else:
        print("  ⚠ PCIe-based topology")
        print("  → Limited by PCIe bandwidth")
    
    print("=" * 80)


# ============================================================================
# Main Benchmark Suite
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="8-GPU Bandwidth Benchmark Suite")
    parser.add_argument("--quick", action="store_true", help="Quick test (fewer sizes)")
    parser.add_argument("--full", action="store_true", help="Full benchmark suite")
    parser.add_argument("--output", type=str, default=None, help="Save results to JSON file")
    args = parser.parse_args()
    
    rank, world_size = setup_distributed()
    
    if world_size != 8 and rank == 0:
        print(f"⚠ Warning: This benchmark is optimized for 8 GPUs, running with {world_size}")
    
    if rank == 0:
        print("=" * 80)
        print("8x B200 Comprehensive Bandwidth Benchmark Suite")
        print("=" * 80)
        print(f"GPUs: {world_size}")
        print(f"Mode: {'Quick' if args.quick else 'Full' if args.full else 'Standard'}")
        print("=" * 80)
    
    all_results = {}
    
    # Benchmark 1: P2P Matrix
    if args.full or world_size <= 8:
        bandwidth_matrix = measure_p2p_matrix(rank, world_size)
        all_results["p2p_matrix"] = {f"{k[0]}-{k[1]}": v for k, v in bandwidth_matrix.items()}
        
        # Visualize topology
        visualize_topology(rank, world_size, bandwidth_matrix)
    
    # Benchmark 2: Collective Operations
    collective_results = measure_collectives(rank, world_size, quick=args.quick)
    all_results["collectives"] = collective_results
    
    # Benchmark 3: Latency vs Bandwidth
    if args.full:
        latency_bw_curve = measure_latency_bandwidth_curve(rank, world_size)
        all_results["latency_bandwidth_curve"] = {
            str(k): {"latency_us": v[0], "bandwidth_gbs": v[1]} 
            for k, v in latency_bw_curve.items()
        }
    
    # Summary
    if rank == 0:
        print("\n" + "=" * 80)
        print("Summary")
        print("=" * 80)
        
        # Get AllReduce 1GB result
        if 1024.0 in collective_results.get("allreduce", {}):
            allreduce_1gb = collective_results["allreduce"][1024.0]
            print(f"\nKey Metric - AllReduce 1GB:")
            print(f"  Latency: {allreduce_1gb['latency_ms']:.2f} ms")
            print(f"  Bus Bandwidth: {allreduce_1gb['bandwidth_gbs']:.2f} GB/s")
            print(f"  Target: 700-800 GB/s")
            
            if allreduce_1gb['bandwidth_gbs'] >= 700:
                print(f"  ✓ Target achieved!")
            else:
                efficiency = (allreduce_1gb['bandwidth_gbs'] / 750) * 100
                print(f"  ⚠ {efficiency:.1f}% of target")
        
        print("\nRecommendations:")
        print("  1. Ensure NCCL 2.28+ with 8-GPU optimizations")
        print("  2. Check NVLink topology with nvidia-smi topo -m")
        print("  3. Verify no CPU throttling or power limits")
        print("  4. Use NCCL_NVLS_ENABLE=1 for 8-GPU configurations")
        print("=" * 80)
    
    # Save results
    if args.output and rank == 0:
        with open(args.output, 'w') as f:
            json.dump(all_results, f, indent=2)
        print(f"\nResults saved to {args.output}")
    
    dist.destroy_process_group()


if __name__ == "__main__":
    main()

