// baseline_cublaslt_gemm.cu -- Host-staged GEMM baseline for cuBLASLt comparison.

#include <cuda_runtime.h>

#include <cmath>
#include <iostream>
#include <random>
#include <vector>

#include "../core/common/headers/cuda_verify.cuh"
#include "../core/common/nvtx_utils.cuh"

#define CUDA_CHECK(call)                                                         \
  do {                                                                           \
    cudaError_t status = (call);                                                 \
    if (status != cudaSuccess) {                                                 \
      std::cerr << "CUDA error " << __FILE__ << ":" << __LINE__ << " "           \
                << cudaGetErrorString(status) << std::endl;                      \
      std::exit(EXIT_FAILURE);                                                   \
    }                                                                            \
  } while (0)
#define CUDA_CHECK_LAST_ERROR()                                                  \
  do {                                                                           \
    cudaError_t status = cudaGetLastError();                                     \
    if (status != cudaSuccess) {                                                 \
      std::cerr << "CUDA error " << __FILE__ << ":" << __LINE__ << " "           \
                << cudaGetErrorString(status) << std::endl;                      \
      std::exit(EXIT_FAILURE);                                                   \
    }                                                                            \
  } while (0)

// Simple GEMM kernel with no tiling (intentionally bandwidth bound).
__global__ void simple_gemm_kernel(const float* __restrict__ A,
                                   const float* __restrict__ B,
                                   float* __restrict__ C,
                                   int M,
                                   int N,
                                   int K,
                                   float alpha,
                                   float beta) {
    const int row = blockIdx.y * blockDim.y + threadIdx.y;
    const int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row >= M || col >= N) {
        return;
    }

    const volatile float* A_volatile = reinterpret_cast<const volatile float*>(A);
    const volatile float* B_volatile = reinterpret_cast<const volatile float*>(B);
    float sum = 0.0f;
    for (int k = 0; k < K; ++k) {
        float a = A_volatile[row * K + k];
        float b = B_volatile[k * N + col];
        sum = fmaf(a, b, sum);
    }
    C[row * N + col] = alpha * sum + beta * C[row * N + col];
}

int main() {
    NVTX_RANGE("main");
    constexpr int M = 1024;
    constexpr int N = 1024;
    constexpr int K = 1024;
    constexpr int kIterations = 5;
    constexpr int kBatchCount = 32;

    const size_t elements_A = static_cast<size_t>(M) * K;
    const size_t elements_B = static_cast<size_t>(K) * N;
    const size_t elements_C = static_cast<size_t>(M) * N;
    const size_t size_A = elements_A * sizeof(float) * kBatchCount;
    const size_t size_B = elements_B * sizeof(float) * kBatchCount;
    const size_t size_C = elements_C * sizeof(float);

    float* h_A = nullptr;
    float* h_B = nullptr;
    float* h_C0 = nullptr;
    CUDA_CHECK(cudaMallocHost(&h_A, size_A));
    CUDA_CHECK(cudaMallocHost(&h_B, size_B));
    CUDA_CHECK(cudaMallocHost(&h_C0, size_C));

    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dis(-1.0f, 1.0f);
    for (size_t i = 0; i < elements_A * kBatchCount; ++i) {
        NVTX_RANGE("setup");
        h_A[i] = dis(gen);
    }
    for (size_t i = 0; i < elements_B * kBatchCount; ++i) {
        NVTX_RANGE("setup");
        h_B[i] = dis(gen);
    }

    float *d_A = nullptr, *d_B = nullptr, *d_C = nullptr;
    CUDA_CHECK(cudaMalloc(&d_A, size_A));
    CUDA_CHECK(cudaMalloc(&d_B, size_B));
    CUDA_CHECK(cudaMalloc(&d_C, size_C * kBatchCount));

    cudaStream_t stream;
    CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));
    CUDA_CHECK(cudaMemcpyAsync(d_A, h_A, size_A, cudaMemcpyHostToDevice, stream));
    CUDA_CHECK(cudaMemcpyAsync(d_B, h_B, size_B, cudaMemcpyHostToDevice, stream));
    CUDA_CHECK(cudaMemsetAsync(d_C, 0, size_C * kBatchCount, stream));

    dim3 block_size(16, 8);
    dim3 grid_size((N + block_size.x - 1) / block_size.x,
                   (M + block_size.y - 1) / block_size.y);

    // Warmup (single batch).
    simple_gemm_kernel<<<grid_size, block_size, 0, stream>>>(
        d_A, d_B, d_C, M, N, K, 1.0f, 0.0f);
    CUDA_CHECK_LAST_ERROR();
    CUDA_CHECK(cudaStreamSynchronize(stream));

    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));

    CUDA_CHECK(cudaEventRecord(start, stream));
    for (int iter = 0; iter < kIterations; ++iter) {
        NVTX_RANGE("batch");
        for (int batch = 0; batch < kBatchCount; ++batch) {
            NVTX_RANGE("compute_kernel:simple_gemm_kernel");
            const size_t a_off = static_cast<size_t>(batch) * elements_A;
            const size_t b_off = static_cast<size_t>(batch) * elements_B;
            const size_t c_off = static_cast<size_t>(batch) * elements_C;
            simple_gemm_kernel<<<grid_size, block_size, 0, stream>>>(
                d_A + a_off,
                d_B + b_off,
                d_C + c_off,
                M,
                N,
                K,
                1.0f,
                0.0f);
            CUDA_CHECK_LAST_ERROR();
        }
    }
    CUDA_CHECK(cudaEventRecord(stop, stream));
    CUDA_CHECK(cudaEventSynchronize(stop));

    float time_total = 0.0f;
    CUDA_CHECK(cudaEventElapsedTime(&time_total, start, stop));
    const float time_avg = time_total / static_cast<float>(kIterations * kBatchCount);
    std::cout << "Naive batched GEMM (baseline): " << time_avg << " ms" << std::endl;

    CUDA_CHECK(cudaMemcpyAsync(h_C0, d_C, size_C, cudaMemcpyDeviceToHost, stream));
    CUDA_CHECK(cudaStreamSynchronize(stream));
    std::cout << "Checksum sample: " << h_C0[0] << std::endl;

#ifdef VERIFY
    double checksum = 0.0;
    for (size_t i = 0; i < elements_C; ++i) {
        NVTX_RANGE("verify");
        checksum += std::abs(static_cast<double>(h_C0[i]));
    }
    VERIFY_PRINT_CHECKSUM(static_cast<float>(checksum));
#endif

    CUDA_CHECK(cudaEventDestroy(start));
    CUDA_CHECK(cudaEventDestroy(stop));
    CUDA_CHECK(cudaStreamDestroy(stream));
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));
    CUDA_CHECK(cudaFreeHost(h_A));
    CUDA_CHECK(cudaFreeHost(h_B));
    CUDA_CHECK(cudaFreeHost(h_C0));

    return 0;
}
