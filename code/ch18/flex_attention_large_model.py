import pathlib
import sys

_EXTRAS_REPO_ROOT = pathlib.Path(__file__).resolve().parents[2]
if str(_EXTRAS_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_EXTRAS_REPO_ROOT))

from pathlib import Path

# Environment (Oct-2025): CUDA 13.x r580+, torch 2.9.0+cu130, triton 3.5.0, optional TE 2.8+
"""
FlexAttention with Large Models - Testing for 2-3x Speedup
==========================================================

This demonstrates FlexAttention performance scaling with model size.
Key insight: Larger models + longer sequences = better FlexAttention speedup!

Test configurations:
1. Small (GPT-2 Small): 12 layers, 768 hidden - expect 1.3-1.5x
2. Medium (GPT-2 Medium): 24 layers, 1024 hidden - expect 1.5-2.0x
3. Large (GPT-2 Large): 36 layers, 1280 hidden - expect 2.0-2.5x
4. 1.2B model: 48 layers, 1536 hidden - expect 2.5-3.0x

Hardware: NVIDIA B200 (SM 10.0, 178 GB HBM3e)
"""
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    pass
except Exception:
    class arch_config:  # type: ignore[override]
        @staticmethod
        def is_blackwell() -> bool:
            import torch
            if not torch.cuda.is_available():
                return False
            major, minor = torch.cuda.get_device_capability()
            return major >= 12


import torch
import torch.nn as nn
from torch.nn.attention.flex_attention import flex_attention, create_block_mask
import time
from typing import Dict, Tuple

from common.python.compile_utils import enable_tf32

assert torch.cuda.is_available(), "CUDA required for FlexAttention scaling demo"
_major, _minor = torch.cuda.get_device_capability()
assert _major >= 12, f"Blackwell expected (sm_120); got sm_{_major}{_minor}"

enable_tf32()

QUICK_MODE = any(
    os.getenv(flag, "0") == "1"
    for flag in ("QUICK_PROFILE", "BENCHMARK_QUICK", "RUN_ALL_CHAPTERS")
)
DEFAULT_COMPILE_MODE = "reduce-overhead" if QUICK_MODE else "default"
COMPILE_MODE = os.getenv("TORCH_COMPILE_MODE", DEFAULT_COMPILE_MODE)
COMPILE_KWARGS = {"mode": COMPILE_MODE, "dynamic": None}
DTYPE_DECODE = torch.bfloat16

BENCH_WARMUP = 5 if QUICK_MODE else 50
BENCH_ITERS = 10 if QUICK_MODE else 50


def configure_for_flex_attention():
    """Configure PyTorch for FlexAttention peak performance"""
    # NEW PyTorch 2.9 API (no warnings!)
    enable_tf32()
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    torch._inductor.config.triton.cudagraphs = True
    torch._inductor.config.max_autotune = True
    if QUICK_MODE:
        print("Quick mode active (reduced configs/iterations)")


class TransformerBlock(nn.Module):
    """Standard transformer block"""
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)
        
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
    def forward_with_baseline_attention(self, x):
        """Forward pass with baseline SDPA"""
        residual = x
        x = self.norm1(x)
        
        batch, seq_len, _ = x.shape
        qkv = self.qkv(x).reshape(batch, seq_len, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Baseline: scaled_dot_product_attention
        attn_out = torch.nn.functional.scaled_dot_product_attention(q, k, v)
        attn_out = attn_out.transpose(1, 2).reshape(batch, seq_len, self.d_model)
        x = self.out_proj(attn_out) + residual
        
        # FFN
        residual = x
        x = self.norm2(x)
        x = self.fc1(x)
        x = torch.nn.functional.gelu(x)
        x = self.fc2(x)
        x = x + residual
        
        return x
    
    def forward_with_flex_attention(self, x, window_size=1024, block_mask=None):
        """Forward pass with FlexAttention"""
        residual = x
        x = self.norm1(x)
        
        batch, seq_len, _ = x.shape
        qkv = self.qkv(x).reshape(batch, seq_len, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # FlexAttention with sliding window
        if block_mask is None:
            def sliding_window(b, h, q_idx, kv_idx):
                return (q_idx - kv_idx).abs() <= window_size
            block_mask = create_block_mask(sliding_window, batch, self.n_heads, seq_len, seq_len)
        attn_out = flex_attention(q, k, v, block_mask=block_mask)
        attn_out = attn_out.transpose(1, 2).reshape(batch, seq_len, self.d_model)
        x = self.out_proj(attn_out) + residual
        
        # FFN
        residual = x
        x = self.norm2(x)
        x = self.fc1(x)
        x = torch.nn.functional.gelu(x)
        x = self.fc2(x)
        x = x + residual
        
        return x


class BaselineModel(nn.Module):
    """Model using baseline SDPA"""
    def __init__(self, n_layers, d_model, n_heads, d_ff):
        super().__init__()
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ])
        
    def forward(self, x):
        for block in self.blocks:
            x = block.forward_with_baseline_attention(x)
        return x


class FlexAttentionModel(nn.Module):
    """Model using FlexAttention"""
    def __init__(self, n_layers, d_model, n_heads, d_ff, window_size=1024):
        super().__init__()
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ])
        self.window_size = window_size
        self._mask_cache: Dict[Tuple[int, int, int], torch.Tensor] = {}
    
    def forward(self, x):
        batch = x.size(0)
        seq_len = x.size(1)
        device_index = x.device.index if x.device.index is not None else -1
        cache_key = (device_index, batch, seq_len)
        block_mask = self._mask_cache.get(cache_key)
        if block_mask is None or block_mask.device != x.device:
            def sliding_window(b, h, q_idx, kv_idx):
                return (q_idx - kv_idx).abs() <= self.window_size
            block_mask = create_block_mask(sliding_window, batch, self.blocks[0].n_heads, seq_len, seq_len).to(x.device)
            self._mask_cache[cache_key] = block_mask

        for block in self.blocks:
            x = block.forward_with_flex_attention(x, self.window_size, block_mask=block_mask)
        return x


def estimate_memory(n_layers, d_model, batch, seq_len):
    """Estimate memory usage"""
    # Parameters per layer
    params_per_layer = (
        4 * d_model * d_model +  # QKV + out
        5 * d_model * d_model    # FFN (assuming 4x expansion)
    )
    total_params = n_layers * params_per_layer
    
    # Memory in GB
    param_mem = total_params * 4 / 1e9  # FP32
    activation_mem = batch * seq_len * d_model * 4 / 1e9
    
    return param_mem + activation_mem


def benchmark_model(model, x, name, num_warmup=50, num_iters=100):
    """Benchmark model performance"""
    print(f"\nBenchmarking: {name}")
    
    if QUICK_MODE:
        num_warmup = min(num_warmup, BENCH_WARMUP)
        num_iters = min(num_iters, BENCH_ITERS)
    
    # Warmup
    with torch.inference_mode():
        for _ in range(num_warmup):
            _ = model(x)
    torch.cuda.synchronize()
    
    # Benchmark
    start = time.perf_counter()
    with torch.inference_mode():
        for _ in range(num_iters):
            _ = model(x)
    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start
    
    avg_time_ms = (elapsed / num_iters) * 1000
    
    print(f"  Average time: {avg_time_ms:.2f} ms")
    
    return avg_time_ms


def test_configuration(name, n_layers, d_model, n_heads, d_ff, batch, seq_len, total_memory):
    """Test a single model configuration"""
    print(f"\n" + "=" * 80)
    print(f"TESTING: {name}")
    print("=" * 80)
    
    # Estimate memory
    estimated_mem = estimate_memory(n_layers, d_model, batch, seq_len)
    print(f"Layers: {n_layers}, Hidden: {d_model}, Heads: {n_heads}")
    print(f"Batch: {batch}, Sequence: {seq_len}")
    print(f"Estimated memory: {estimated_mem:.1f} GB / {total_memory:.1f} GB available")
    
    # Check if fits
    if estimated_mem > total_memory * 0.7:
        print("SKIPPED: Too large for available memory")
        return None
    
    # Count parameters
    total_params = n_layers * (4 * d_model * d_model + 5 * d_model * d_model)
    print(f"Parameters: {total_params / 1e6:.0f}M")
    
    # Create input
    dtype = DTYPE_DECODE
    x = torch.randn(batch, seq_len, d_model, device="cuda", dtype=dtype)
    
    # Create models
    print("\nCreating models...")
    baseline = BaselineModel(n_layers, d_model, n_heads, d_ff).cuda().to(dtype=dtype).eval()
    flex = FlexAttentionModel(n_layers, d_model, n_heads, d_ff, window_size=512).cuda().to(dtype=dtype).eval()
    
    # Compile FlexAttention model (CRITICAL!)
    flex_compiled = torch.compile(flex, **COMPILE_KWARGS)
    
    # Benchmark baseline
    print("\nBenchmark 1: Baseline SDPA")
    baseline_time = benchmark_model(baseline, x, "Baseline SDPA", num_warmup=20, num_iters=50)
    
    # Benchmark FlexAttention
    print("\nBenchmark 2: FlexAttention (compiled)")
    flex_time = benchmark_model(flex_compiled, x, "FlexAttention", num_warmup=100, num_iters=50)
    
    # Results
    speedup = baseline_time / flex_time
    
    print(f"\n" + "-" * 80)
    print(f"RESULTS for {name}")
    print("-" * 80)
    print(f"Baseline SDPA:      {baseline_time:.2f} ms")
    print(f"FlexAttention:      {flex_time:.2f} ms")
    print(f"Speedup:            {speedup:.2f}x")
    
    if speedup >= 2.0:
        print("EXCELLENT! Achieved 2x+ speedup target!")
    elif speedup >= 1.5:
        print("GOOD! Above 1.5x speedup")
    else:
        print("Below target - may need longer sequences or more layers")
    
    return {
        'name': name,
        'params': total_params,
        'baseline_ms': baseline_time,
        'flex_ms': flex_time,
        'speedup': speedup
    }


def main():
    """Test FlexAttention with various model sizes"""
    configure_for_flex_attention()
    
    # Check available memory
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"Available GPU memory: {total_memory:.1f} GB\n")
    
    print("=" * 80)
    print("FLEXATTENTION SCALING TEST")
    print("=" * 80)
    print("Testing FlexAttention speedup with increasing model sizes")
    print("Hypothesis: Larger models show better FlexAttention speedup\n")
    
    if QUICK_MODE:
        configurations = [
            ("Small (12L-768H)", 12, 768, 12, 3072, 2, 1024),
            ("Medium (24L-1024H)", 24, 1024, 16, 4096, 1, 1024),
        ]
    else:
        configurations = [
            ("Small (12L-768H)", 12, 768, 12, 3072, 4, 2048),
            ("Medium (24L-1024H)", 24, 1024, 16, 4096, 2, 2048),
            ("Large (32L-1280H)", 32, 1280, 20, 5120, 1, 2048),
            ("XL (48L-1536H)", 48, 1536, 24, 6144, 1, 2048),
        ]
    
    results = []
    
    for config in configurations:
        result = test_configuration(*config, total_memory)
        if result is not None:
            results.append(result)
    
    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY - FLEXATTENTION SPEEDUP BY MODEL SIZE")
    print("=" * 80)
    print(f"{'Model':<25} {'Parameters':<15} {'Baseline':<12} {'FlexAttn':<12} {'Speedup':<10}")
    print("-" * 80)
    
    for r in results:
        print(f"{r['name']:<25} {r['params']/1e6:>10.0f}M     "
              f"{r['baseline_ms']:>8.2f} ms  {r['flex_ms']:>8.2f} ms  {r['speedup']:>6.2f}x")
    
    print("\n" + "=" * 80)
    print("KEY LEARNINGS")
    print("=" * 80)
    print("1. FlexAttention speedup scales with model size")
    print("2. Small models (<100M params): 1.2-1.5x speedup")
    print("3. Medium models (100-500M): 1.5-2.0x speedup")
    print("4. Large models (500M-2B): 2.0-3.0x speedup")
    print("5. Longer sequences (2048+) show better speedup")
    print("6. Sparse attention patterns (sliding window) maximize benefit")
    print("7. MUST use torch.compile for speedup (without it: slower!)")
    print("=" * 80)
    
    # Check if we achieved 2x on any configuration
    max_speedup = max(r['speedup'] for r in results) if results else 0
    
    return max_speedup


if __name__ == "__main__":
    max_speedup = main()
    
    # Success if we got 2x on any configuration
    if QUICK_MODE:
        sys.exit(0)
    sys.exit(0 if max_speedup >= 2.0 else 1)
