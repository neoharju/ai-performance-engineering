"""optimized_nvlink.py - Optimized NVLink communication in AI optimization context.

Demonstrates memory transfer with NVLink optimization.
NVLink: Uses NVLink for high-speed GPU-to-GPU and CPU-to-GPU communication.
Provides much higher bandwidth than PCIe for multi-GPU workloads.
Implements Benchmark protocol for harness integration.
"""

from __future__ import annotations

import sys
from pathlib import Path

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

import torch
import torch.distributed as dist

from common.python.compile_utils import compile_model

from typing import Optional

from common.python.benchmark_harness import (
    Benchmark,
    BenchmarkConfig,
)


def resolve_device() -> torch.device:
    """Return CUDA device if available."""
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA required for ch20")
    return torch.device("cuda")


class OptimizedNvlinkBenchmark(Benchmark):
    """Optimized: Memory transfer with NVLink.
    
    NVLink: Uses NVLink for high-speed GPU-to-GPU and CPU-to-GPU communication.
    Provides much higher bandwidth than PCIe for multi-GPU workloads.
    """
    
    def __init__(self):
        self.device = resolve_device()
        self.data_gpu0 = None
        self.data_gpu1 = None
        self.N = 10_000_000
        self.is_multi_gpu = False
    
    def setup(self) -> None:
        """Setup: Initialize multi-GPU memory with NVLink."""
        
        # Optimization: Enable cuDNN benchmarking for optimal kernel selection
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        torch.manual_seed(42)
        # Optimization: NVLink for GPU-to-GPU communication
        # NVLink provides high-speed interconnect between GPUs
        # Much faster than PCIe for multi-GPU transfers
        
        # Check if multiple GPUs available (NVLink used for GPU-to-GPU)
        self.is_multi_gpu = torch.cuda.device_count() > 1
        
        if self.is_multi_gpu:
            # Multi-GPU: NVLink provides high-speed GPU-to-GPU communication
            # Allocate data on different GPUs
            with torch.cuda.device(0):
                self.data_gpu0 = torch.randn(self.N, device=torch.device("cuda:0"))
            with torch.cuda.device(1):
                self.data_gpu1 = torch.empty(self.N, device=torch.device("cuda:1"))
        else:
            # Single GPU: Simulate NVLink behavior
            # NVLink is primarily for GPU-to-GPU, but also improves CPU-to-GPU
            self.data_gpu0 = torch.randn(self.N, device=self.device)
            self.data_gpu1 = torch.empty(self.N, device=self.device)
        
        torch.cuda.synchronize()
    
    def benchmark_fn(self) -> None:
        """Benchmark: Memory transfer with NVLink."""
        torch.cuda.nvtx.range_push("optimized_nvlink")
        try:
            if self.is_multi_gpu:
                # Optimization: NVLink GPU-to-GPU transfer
                # NVLink provides much higher bandwidth than PCIe
                # Direct GPU-to-GPU communication via NVLink (no CPU round-trip)
                with torch.cuda.device(1):
                    self.data_gpu1.copy_(self.data_gpu0, non_blocking=True)
                
                # Process on GPU1
                with torch.cuda.device(1):
                    self.data_gpu1 = self.data_gpu1 * 2.0
                
                # Transfer back via NVLink (direct GPU-to-GPU)
                with torch.cuda.device(0):
                    self.data_gpu0.copy_(self.data_gpu1, non_blocking=True)
                
                # Optimization: NVLink benefits
                # - High bandwidth GPU-to-GPU transfers (avoid PCIe bottleneck)
                # - Low latency communication (direct interconnect)
                # - Better multi-GPU performance (no CPU overhead)
            else:
                # Single GPU: Optimize transfer pattern (simulate NVLink efficiency)
                # Use pinned memory and non-blocking transfers for better performance
                # In multi-GPU, NVLink would provide even better bandwidth
                self.data_gpu1.copy_(self.data_gpu0, non_blocking=True)
                torch.cuda.synchronize()  # Ensure transfer completes
                self.data_gpu1 = self.data_gpu1 * 2.0
                self.data_gpu0.copy_(self.data_gpu1, non_blocking=True)
                torch.cuda.synchronize()  # Ensure transfer completes
        finally:
            torch.cuda.synchronize()
            torch.cuda.nvtx.range_pop()
    
    def teardown(self) -> None:
        """Teardown: Clean up resources."""
        self.data_gpu0 = None
        self.data_gpu1 = None
        torch.cuda.empty_cache()
    
    def get_config(self) -> BenchmarkConfig:
        """Return benchmark configuration."""
        return BenchmarkConfig(
            iterations=10,
            warmup=2,
        )
    
    def validate_result(self) -> Optional[str]:
        """Validate benchmark result."""
        if self.data_gpu0 is None or self.data_gpu1 is None:
            return "Data not initialized"
        return None

def get_benchmark() -> Benchmark:
    """Factory function for harness discovery."""
    return OptimizedNvlinkBenchmark()


def main() -> None:
    """Standalone execution (for testing)."""
    from common.python.benchmark_harness import BenchmarkHarness, BenchmarkMode
    
    harness = BenchmarkHarness(
        mode=BenchmarkMode.CUSTOM,
        config=BenchmarkConfig(iterations=50, warmup=5)
    )
    benchmark = OptimizedNvlinkBenchmark()
    result = harness.benchmark(benchmark)
    
    print("=" * 70)
    print(f"Optimized: nvlink")
    print("=" * 70)
    timing = result.timing
    if timing:
        print(f"Average time: {timing.mean_ms:.3f} ms")
        print(f"Median: {timing.median_ms:.3f} ms")
        print(f"Std: {timing.std_ms:.3f} ms")
    else:
        print("No timing data available")


if __name__ == "__main__":
    main()
