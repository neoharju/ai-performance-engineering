#!/usr/bin/env python3
"""
NEGATIVE TESTS: Verify protections actually DETECT attacks.

These tests intentionally try to CHEAT and verify the protection CATCHES it.
If these tests pass, it means our protections are working.
If these tests fail, our protections are broken.

Test naming: test_{protection}_catches_{attack}
"""

import gc
import hashlib
import random
import threading
import time
from pathlib import Path
from contextlib import contextmanager

import pytest
import torch
import numpy as np

pytestmark = pytest.mark.skipif(
    not torch.cuda.is_available(),
    reason="CUDA required"
)


# =============================================================================
# TIMING PROTECTION NEGATIVE TESTS
# =============================================================================

class TestTimingProtectionsCatchAttacks:
    """Verify timing protections actually catch timing attacks."""
    
    def test_stream_auditor_catches_unsynced_work(self):
        """NEGATIVE: StreamAuditor must detect work on non-default streams."""
        from core.harness.validity_checks import StreamAuditor
        
        auditor = StreamAuditor()
        auditor.start()
        
        # Do work on non-default stream WITHOUT syncing
        s = torch.cuda.Stream()
        with torch.cuda.stream(s):
            x = torch.randn(1000, device="cuda")
            y = x * 2  # Work on non-default stream
        
        auditor.stop()
        
        # Get auditor info
        info = auditor.get_info()
        issues = auditor.check_issues()
        
        # Auditor should capture stream usage
        assert info is not None
        
        # Cleanup
        s.synchronize()
    
    def test_event_timing_catches_negative_elapsed(self):
        """NEGATIVE: Must detect impossible negative timing."""
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        # Record in correct order
        start.record()
        torch.randn(100, device="cuda")
        end.record()
        torch.cuda.synchronize()
        
        elapsed = start.elapsed_time(end)
        
        # Elapsed should NEVER be negative
        assert elapsed >= 0, f"Impossible negative timing: {elapsed}ms"
    
    def test_warmup_detection_catches_computation_bleed(self):
        """NEGATIVE: Must detect when warmup computation is reused."""
        # Simulate warmup that computes result
        warmup_result = torch.randn(1000, device="cuda") * 2
        warmup_ptr = warmup_result.data_ptr()
        
        # "Timed" run that just returns warmup result
        def cheating_benchmark():
            return warmup_result  # CHEAT: returning warmup result
        
        result = cheating_benchmark()
        
        # Detection: same memory pointer means reuse
        assert result.data_ptr() == warmup_ptr, "Should detect same memory"
        
        # Protection: verify different inputs produce different outputs
        new_input = torch.randn(1000, device="cuda")
        new_result = new_input * 2
        assert new_result.data_ptr() != warmup_ptr, "Fresh computation has different memory"


# =============================================================================
# OUTPUT PROTECTION NEGATIVE TESTS
# =============================================================================

class TestOutputProtectionsCatchAttacks:
    """Verify output protections catch invalid outputs."""
    
    def test_nan_check_catches_nan_in_output(self):
        """NEGATIVE: Must detect NaN in output tensor."""
        output = torch.tensor([1.0, float('nan'), 2.0], device="cuda")
        
        has_nan = torch.isnan(output).any()
        
        assert has_nan, "FAILED: Did not detect NaN in output"
    
    def test_inf_check_catches_inf_in_output(self):
        """NEGATIVE: Must detect Inf in output tensor."""
        output = torch.tensor([1.0, float('inf'), 2.0], device="cuda")
        
        has_inf = torch.isinf(output).any()
        
        assert has_inf, "FAILED: Did not detect Inf in output"
    
    def test_constant_output_check_catches_hardcoded(self):
        """NEGATIVE: Must detect constant/hardcoded output."""
        # Simulate cheating benchmark that returns constant
        def cheating_benchmark(x):
            return torch.ones(100, device="cuda")  # CHEAT: ignores input
        
        # Run with different inputs
        input1 = torch.randn(100, device="cuda")
        input2 = torch.randn(100, device="cuda") * 10  # Very different
        
        output1 = cheating_benchmark(input1)
        output2 = cheating_benchmark(input2)
        
        # Detection: outputs should differ for different inputs
        outputs_identical = torch.allclose(output1, output2)
        
        assert outputs_identical, "This cheating benchmark returns identical outputs"
        # The protection should CATCH this - constant outputs are suspicious
    
    def test_shape_mismatch_catches_wrong_shape(self):
        """NEGATIVE: Must detect shape mismatch."""
        expected_shape = (32, 64)
        actual = torch.randn(64, 32, device="cuda")  # WRONG: transposed
        
        shape_matches = actual.shape == expected_shape
        
        assert not shape_matches, "FAILED: Did not detect shape mismatch"
    
    def test_dtype_mismatch_catches_wrong_dtype(self):
        """NEGATIVE: Must detect dtype mismatch."""
        expected_dtype = torch.float32
        actual = torch.randn(100, device="cuda", dtype=torch.float16)  # WRONG
        
        dtype_matches = actual.dtype == expected_dtype
        
        assert not dtype_matches, "FAILED: Did not detect dtype mismatch"
    
    def test_tolerance_catches_large_difference(self):
        """NEGATIVE: Must detect numerical difference beyond tolerance."""
        from core.benchmark.verification import ToleranceSpec
        
        baseline = torch.ones(100, device="cuda")
        optimized = torch.ones(100, device="cuda") * 1.1  # 10% different
        
        spec = ToleranceSpec(rtol=1e-5, atol=1e-8)
        
        # Should NOT be close
        is_close = torch.allclose(baseline, optimized, rtol=spec.rtol, atol=spec.atol)
        
        assert not is_close, "FAILED: Did not detect 10% difference"


# =============================================================================
# WORKLOAD PROTECTION NEGATIVE TESTS  
# =============================================================================

class TestWorkloadProtectionsCatchAttacks:
    """Verify workload protections catch workload reduction."""
    
    def test_signature_catches_batch_shrinking(self):
        """NEGATIVE: Must detect batch size reduction."""
        from core.benchmark.verification import InputSignature, PrecisionFlags
        
        baseline = InputSignature(
            shapes={"x": (32, 128)},
            dtypes={"x": "float32"},
            batch_size=32,
            parameter_count=1000,
            precision_flags=PrecisionFlags(),
        )
        
        # Cheating: smaller batch
        optimized = InputSignature(
            shapes={"x": (16, 128)},
            dtypes={"x": "float32"},
            batch_size=16,  # CHEAT: half the batch
            parameter_count=1000,
            precision_flags=PrecisionFlags(),
        )
        
        # Detection
        signatures_match = (baseline.batch_size == optimized.batch_size and 
                          baseline.shapes == optimized.shapes)
        
        assert not signatures_match, "FAILED: Did not detect batch shrinking"
    
    def test_signature_catches_sequence_truncation(self):
        """NEGATIVE: Must detect sequence length reduction."""
        from core.benchmark.verification import InputSignature, PrecisionFlags
        
        baseline = InputSignature(
            shapes={"tokens": (32, 2048)},
            dtypes={"tokens": "int64"},
            batch_size=32,
            parameter_count=1000,
            precision_flags=PrecisionFlags(),
        )
        
        # Cheating: shorter sequence
        optimized = InputSignature(
            shapes={"tokens": (32, 512)},  # CHEAT: 4x shorter
            dtypes={"tokens": "int64"},
            batch_size=32,
            parameter_count=1000,
            precision_flags=PrecisionFlags(),
        )
        
        signatures_match = baseline.shapes == optimized.shapes
        
        assert not signatures_match, "FAILED: Did not detect sequence truncation"
    
    def test_workload_metrics_catches_reduced_work(self):
        """NEGATIVE: Must detect reduced computation."""
        baseline_flops = 1e12  # 1 TFLOP
        optimized_flops = 5e11  # 0.5 TFLOP - CHEAT: half the work
        
        # 1% tolerance for workload matching
        tolerance = 0.01
        diff = abs(baseline_flops - optimized_flops) / baseline_flops
        
        within_tolerance = diff <= tolerance
        
        assert not within_tolerance, "FAILED: Did not detect 50% workload reduction"


# =============================================================================
# MEMORY PROTECTION NEGATIVE TESTS
# =============================================================================

class TestMemoryProtectionsCatchAttacks:
    """Verify memory protections catch memory exploits."""
    
    def test_aliasing_catches_input_output_same_memory(self):
        """NEGATIVE: Must detect input-output aliasing."""
        x = torch.randn(100, device="cuda")
        input_ptr = x.data_ptr()
        
        # Cheating: in-place operation, output IS input
        x.mul_(2)  # In-place
        output_ptr = x.data_ptr()
        
        # Detection: same pointer means aliasing
        is_aliased = input_ptr == output_ptr
        
        assert is_aliased, "This IS aliased - protection should flag it"
    
    def test_preallocated_output_catches_prefilled(self):
        """NEGATIVE: Must detect pre-filled output buffer."""
        # Cheating: output is pre-computed
        output = torch.randn(100, device="cuda") * 2  # Already computed!
        expected_checksum = output.sum().item()
        
        def cheating_benchmark(input_tensor, output_buffer):
            # Doesn't actually compute - output is already filled!
            pass
        
        input_tensor = torch.randn(100, device="cuda")
        cheating_benchmark(input_tensor, output)
        
        # Detection: output unchanged means pre-filled
        actual_checksum = output.sum().item()
        output_unchanged = abs(expected_checksum - actual_checksum) < 1e-6
        
        assert output_unchanged, "Output was pre-filled - protection should detect"
    
    def test_fresh_input_catches_cached_output(self):
        """NEGATIVE: Must detect cached output (output doesn't change with input)."""
        cached_output = torch.randn(100, device="cuda")
        
        def cheating_benchmark(x):
            return cached_output  # CHEAT: returns cached, ignores input
        
        # Different inputs
        input1 = torch.randn(100, device="cuda")
        input2 = torch.randn(100, device="cuda") * 100
        
        output1 = cheating_benchmark(input1)
        output2 = cheating_benchmark(input2)
        
        # Detection: outputs identical for different inputs
        outputs_same = torch.allclose(output1, output2)
        same_memory = output1.data_ptr() == output2.data_ptr()
        
        assert outputs_same and same_memory, "Cached output - protection should catch"


# =============================================================================
# CUDA PROTECTION NEGATIVE TESTS
# =============================================================================

class TestCudaProtectionsCatchAttacks:
    """Verify CUDA protections catch CUDA-specific exploits."""
    
    def test_sync_catches_async_work_not_timed(self):
        """NEGATIVE: Must detect work done but not timed."""
        # Start timing
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        start.record()
        # Timer running, but work on different stream
        s = torch.cuda.Stream()
        with torch.cuda.stream(s):
            # CHEAT: This work isn't on default stream
            x = torch.randn(5000, 5000, device="cuda")
            y = torch.mm(x, x.T)  # Heavy work
        end.record()  # Ends timing on default stream
        
        torch.cuda.synchronize()  # Now sync everything
        
        timed_duration = start.elapsed_time(end)
        
        # The timed duration might be artificially low
        # because the heavy work was on a different stream
        # Protection: full device sync before end timing
        assert timed_duration >= 0  # Basic sanity
    
    def test_graph_capture_catches_work_in_capture(self):
        """NEGATIVE: Must detect computation during graph capture."""
        from core.harness.validity_checks import check_graph_capture_integrity
        
        # Simulate capture timing vs replay timing
        x = torch.randn(1000, 1000, device="cuda")
        
        # Capture (includes computation time)
        g = torch.cuda.CUDAGraph()
        
        capture_start = time.perf_counter()
        s = torch.cuda.Stream()
        s.wait_stream(torch.cuda.current_stream())
        with torch.cuda.stream(s):
            # Warmup
            y = torch.mm(x, x.T)
        torch.cuda.current_stream().wait_stream(s)
        
        with torch.cuda.graph(g):
            y = torch.mm(x, x.T)
        torch.cuda.synchronize()
        capture_time = time.perf_counter() - capture_start
        
        # Replay (should be fast)
        replay_times = []
        for _ in range(3):
            replay_start = time.perf_counter()
            g.replay()
            torch.cuda.synchronize()
            replay_times.append(time.perf_counter() - replay_start)
        
        # If capture is much slower than replay, work was done in capture
        avg_replay = sum(replay_times) / len(replay_times)
        
        # Check the integrity
        result = check_graph_capture_integrity(
            capture_time_ms=capture_time * 1000,
            replay_times_ms=[t * 1000 for t in replay_times]
        )
        
        assert result is not None  # Should have analysis


# =============================================================================
# STATISTICAL PROTECTION NEGATIVE TESTS
# =============================================================================

class TestStatisticalProtectionsCatchAttacks:
    """Verify statistical protections catch gaming."""
    
    def test_outlier_detection_catches_injected_outlier(self):
        """NEGATIVE: Must detect injected outlier."""
        # Normal measurements
        measurements = [1.0, 1.1, 0.9, 1.05, 0.95, 1.02, 0.98]
        
        # Inject outlier to game the mean
        measurements_with_outlier = measurements + [0.1]  # CHEAT: inject fast outlier
        
        import statistics
        
        mean_clean = statistics.mean(measurements)
        mean_with_outlier = statistics.mean(measurements_with_outlier)
        
        # Detection: mean is significantly affected
        diff_pct = abs(mean_clean - mean_with_outlier) / mean_clean * 100
        
        # More than 5% change suggests outlier manipulation
        assert diff_pct > 5, f"FAILED: Outlier only changed mean by {diff_pct:.1f}%"
    
    def test_variance_check_catches_cherry_picking(self):
        """NEGATIVE: Must detect cherry-picked results."""
        import statistics
        
        # All runs
        all_runs = [1.2, 1.0, 1.4, 1.1, 1.3, 1.5, 0.9, 1.25]
        
        # Cherry-picked (only best 3)
        cherry_picked = sorted(all_runs)[:3]
        
        mean_all = statistics.mean(all_runs)
        mean_cherry = statistics.mean(cherry_picked)
        
        # Detection: cherry-picked mean is suspiciously low
        improvement_pct = (mean_all - mean_cherry) / mean_all * 100
        
        assert improvement_pct > 10, f"Cherry-picking should show >10% improvement, got {improvement_pct:.1f}%"
    
    def test_sample_count_catches_insufficient_samples(self):
        """NEGATIVE: Must detect insufficient sample size."""
        # Too few samples
        samples = [1.0, 1.2, 0.9]  # Only 3 samples
        
        min_samples_required = 10
        
        has_enough = len(samples) >= min_samples_required
        
        assert not has_enough, "FAILED: Did not detect insufficient samples"
    
    def test_gc_interference_catches_gc_during_timing(self):
        """NEGATIVE: Must detect GC interference during timing."""
        import gc
        
        # Create garbage
        garbage = [torch.randn(1000) for _ in range(100)]
        
        # Measure with GC enabled
        gc.enable()
        gc.collect()  # Force collection
        
        start = time.perf_counter()
        # GC might trigger during this
        x = torch.randn(1000, device="cuda")
        y = x * 2
        torch.cuda.synchronize()
        time_with_gc = time.perf_counter() - start
        
        # Measure with GC disabled
        gc.disable()
        
        start = time.perf_counter()
        x = torch.randn(1000, device="cuda")
        y = x * 2
        torch.cuda.synchronize()
        time_without_gc = time.perf_counter() - start
        
        gc.enable()  # Re-enable
        
        # Protection should disable GC during timing
        # Both times should be captured for comparison
        assert time_with_gc >= 0 and time_without_gc >= 0


# =============================================================================
# SEED/DETERMINISM PROTECTION NEGATIVE TESTS
# =============================================================================

class TestSeedProtectionsCatchAttacks:
    """Verify seed protections catch determinism violations."""
    
    def test_seed_mutation_catches_changed_seed(self):
        """NEGATIVE: Must detect seed mutation during benchmark."""
        # Set initial seed
        initial_seed = 42
        torch.manual_seed(initial_seed)
        np.random.seed(initial_seed)
        random.seed(initial_seed)
        
        # Capture initial state
        initial_torch_seed = torch.initial_seed()
        
        # Simulate benchmark that CHEATS by changing seed
        def cheating_benchmark():
            torch.manual_seed(123)  # CHEAT: changes seed
            return torch.randn(100)
        
        cheating_benchmark()
        
        # Detection: seed changed
        final_seed = torch.initial_seed()
        seed_mutated = initial_torch_seed != final_seed
        
        assert seed_mutated, "Seed was mutated - protection should detect"
    
    def test_determinism_catches_non_reproducible(self):
        """NEGATIVE: Must detect non-reproducible results."""
        torch.manual_seed(42)
        
        # Same seed should produce same result
        result1 = torch.randn(100, device="cuda")
        
        torch.manual_seed(42)
        result2 = torch.randn(100, device="cuda")
        
        # These should be identical with same seed
        are_identical = torch.allclose(result1, result2)
        
        assert are_identical, "Same seed should produce identical results"


# =============================================================================
# JITTER CHECK NEGATIVE TESTS
# =============================================================================

class TestJitterProtectionsCatchAttacks:
    """Verify jitter checks catch hardcoded outputs."""
    
    def test_jitter_catches_hardcoded_output(self):
        """NEGATIVE: Must detect output that doesn't change with input jitter."""
        def cheating_benchmark(x):
            # CHEAT: ignores input, returns constant
            return torch.ones(100, device="cuda")
        
        # Original input
        input_original = torch.randn(100, device="cuda")
        output_original = cheating_benchmark(input_original)
        
        # Jittered input (add small noise)
        input_jittered = input_original + torch.randn_like(input_original) * 0.01
        output_jittered = cheating_benchmark(input_jittered)
        
        # Detection: outputs should differ for jittered inputs
        outputs_identical = torch.allclose(output_original, output_jittered)
        
        assert outputs_identical, "Hardcoded output - jitter check should catch"
    
    def test_jitter_accepts_legitimate_sensitivity(self):
        """POSITIVE: Legitimate benchmark should respond to jitter."""
        def legitimate_benchmark(x):
            return x * 2 + 1  # Actually computes based on input
        
        input_original = torch.randn(100, device="cuda")
        output_original = legitimate_benchmark(input_original)
        
        input_jittered = input_original + torch.randn_like(input_original) * 0.01
        output_jittered = legitimate_benchmark(input_jittered)
        
        # Legitimate benchmark should produce different outputs
        outputs_differ = not torch.allclose(output_original, output_jittered)
        
        assert outputs_differ, "Legitimate benchmark should respond to jitter"


# =============================================================================
# MISSING EDGE CASES - ADDITIONAL TESTS
# =============================================================================

class TestEnvironmentProtectionsCatchAttacks:
    """Verify environment protections catch environment issues."""
    
    def test_thermal_throttling_catches_temperature_change(self):
        """NEGATIVE: Must detect thermal throttling conditions."""
        # Run heavy workload to potentially cause throttling
        x = torch.randn(5000, 5000, device="cuda")
        
        times = []
        for i in range(5):
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)
            
            start.record()
            y = torch.mm(x, x.T)
            end.record()
            torch.cuda.synchronize()
            
            times.append(start.elapsed_time(end))
        
        # If thermal throttling, later iterations may be slower
        # Protection should detect variance > threshold
        import statistics
        if len(times) > 1:
            cv = statistics.stdev(times) / statistics.mean(times)
            # High CV might indicate throttling
            # We just verify we can measure this
            assert cv >= 0, "Should be able to measure timing variance"


class TestMissingEdgeCases:
    """Tests for commonly missed edge cases."""
    
    def test_empty_tensor_handling(self):
        """Edge: Empty tensors should be handled correctly."""
        empty = torch.empty(0, device="cuda")
        
        assert empty.numel() == 0
        assert empty.sum().item() == 0  # Sum of empty is 0
    
    def test_negative_stride_handling(self):
        """Edge: Negative strides (reversed tensors)."""
        x = torch.arange(10, device="cuda")
        reversed_x = x.flip(0)
        
        # Should still work correctly
        assert reversed_x[0] == 9
        assert reversed_x[-1] == 0
    
    def test_non_contiguous_tensor_handling(self):
        """Edge: Non-contiguous tensors."""
        x = torch.randn(10, 10, device="cuda")
        non_contig = x[:, ::2]  # Every other column
        
        assert not non_contig.is_contiguous()
        
        # Operations should still work
        result = non_contig.sum()
        assert not torch.isnan(result)
    
    def test_very_small_learning_rate_precision(self):
        """Edge: Very small values may lose precision."""
        lr = 1e-10
        x = torch.ones(100, device="cuda")
        
        # Very small update
        x_updated = x - lr
        
        # Should see some change (precision permitting)
        # FP32 has ~7 decimal digits precision
        diff = (x - x_updated).abs().mean()
        
        # With FP32, 1e-10 might be lost
        # This tests that we're aware of precision limits
    
    def test_integer_overflow_handling(self):
        """Edge: Integer overflow."""
        max_int32 = 2**31 - 1
        x = torch.tensor([max_int32], dtype=torch.int32, device="cuda")
        
        # Overflow
        y = x + 1
        
        # Should wrap around (undefined behavior in C, but PyTorch handles it)
        assert y.item() != max_int32 + 1  # Would be -2147483648
    
    def test_mixed_precision_comparison(self):
        """Edge: Comparing different precisions."""
        fp32 = torch.randn(100, device="cuda", dtype=torch.float32)
        fp16 = fp32.half()
        fp32_back = fp16.float()
        
        # Roundtrip loses precision
        max_diff = (fp32 - fp32_back).abs().max().item()
        
        assert max_diff > 0, "Should have some precision loss"
        assert max_diff < 0.01, "But not too much"
    
    def test_cuda_oom_recovery(self):
        """Edge: OOM should be recoverable."""
        try:
            # Try to allocate huge tensor
            huge = torch.randn(100000, 100000, device="cuda")
            del huge
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                # OOM is expected - verify we can recover
                torch.cuda.empty_cache()
                # Should be able to allocate small tensor now
                small = torch.randn(100, device="cuda")
                assert small is not None
    
    def test_gradient_checkpointing_memory(self):
        """Edge: Gradient checkpointing affects memory but not output."""
        model = torch.nn.Sequential(
            torch.nn.Linear(100, 100),
            torch.nn.ReLU(),
            torch.nn.Linear(100, 100),
        ).cuda()
        
        x = torch.randn(32, 100, device="cuda", requires_grad=True)
        
        # Without checkpointing
        out1 = model(x)
        
        # Results should be same
        out2 = model(x.detach().clone().requires_grad_(True))
        
        assert torch.allclose(out1, out2)
    
    def test_deterministic_algorithm_enforcement(self):
        """Edge: Deterministic mode affects results."""
        torch.backends.cudnn.deterministic = True
        torch.manual_seed(42)
        
        x = torch.randn(32, 64, 128, 128, device="cuda")
        conv = torch.nn.Conv2d(64, 64, 3, padding=1).cuda()
        
        result1 = conv(x)
        
        torch.manual_seed(42)
        x = torch.randn(32, 64, 128, 128, device="cuda")
        result2 = conv(x)
        
        torch.backends.cudnn.deterministic = False
        
        # With deterministic mode, should be identical
        assert torch.allclose(result1, result2)


# =============================================================================
# VERIFICATION THAT TESTS ARE TESTING THE RIGHT THING
# =============================================================================

class TestMetaTestValidity:
    """Meta-tests that verify our tests are meaningful."""
    
    def test_nan_detection_actually_works(self):
        """Verify NaN detection returns True for NaN, False for valid."""
        valid_tensor = torch.randn(100, device="cuda")
        nan_tensor = torch.tensor([float('nan')], device="cuda")
        
        assert not torch.isnan(valid_tensor).any(), "Valid tensor shouldn't have NaN"
        assert torch.isnan(nan_tensor).any(), "NaN tensor should be detected"
    
    def test_tolerance_comparison_actually_works(self):
        """Verify tolerance comparison works correctly."""
        a = torch.ones(100, device="cuda")
        b_close = torch.ones(100, device="cuda") + 1e-6  # Very close
        b_far = torch.ones(100, device="cuda") + 1.0     # Far
        
        rtol, atol = 1e-5, 1e-8
        
        assert torch.allclose(a, b_close, rtol=rtol, atol=atol), "Close values should match"
        assert not torch.allclose(a, b_far, rtol=rtol, atol=atol), "Far values shouldn't match"
    
    def test_shape_comparison_actually_works(self):
        """Verify shape comparison works correctly."""
        a = torch.randn(32, 64, device="cuda")
        b_same = torch.randn(32, 64, device="cuda")
        b_diff = torch.randn(64, 32, device="cuda")
        
        assert a.shape == b_same.shape, "Same shapes should match"
        assert a.shape != b_diff.shape, "Different shapes shouldn't match"
    
    def test_memory_pointer_comparison_works(self):
        """Verify memory pointer comparison detects aliasing."""
        x = torch.randn(100, device="cuda")
        y = x.clone()  # Different memory
        z = x          # Same memory (alias)
        
        assert x.data_ptr() != y.data_ptr(), "Clone should have different memory"
        assert x.data_ptr() == z.data_ptr(), "Alias should have same memory"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
