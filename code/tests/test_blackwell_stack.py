#!/usr/bin/env python3
"""
Blackwell validation suite covering PyTorch 2.9, CUDA 13.0, and Triton 3.5 features.
"""

import torch
from arch_config import ArchitectureConfig
import torch.distributed as dist
import torch.profiler as profiler
from torch.profiler import profile, record_function, ProfilerActivity, schedule
import torch.cuda.nvtx as nvtx
import time
import numpy as np
import sys
from pathlib import Path
from types import SimpleNamespace

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import triton as triton
    import triton.language as tl
except Exception:
    triton = None
    tl = None

try:
    from ch16.inference_serving_8xb200 import (
        DemoCausalLM,
        ShardedKVCacheManager,
        InferenceServer8GPU,
        InferenceRequest,
    )
except Exception as e:
    print(f"Warning: Could not import ch16 models: {e}")
    DemoCausalLM = None
    ShardedKVCacheManager = None
    InferenceServer8GPU = None
    InferenceRequest = None


def ensure_cuda(feature: str) -> bool:
    """Check CUDA availability and print a friendly skip message when missing."""
    if torch.cuda.is_available():
        return True
    driver_info = None
    try:
        driver_version = torch.cuda.driver_version()
        if driver_version:
            driver_info = str(driver_version)
    except Exception:
        try:
            from torch._C import _cuda_getDriverVersion  # type: ignore
            driver_version = _cuda_getDriverVersion()
            if driver_version:
                driver_info = str(driver_version)
        except Exception:
            driver_info = None
    suffix = f" (driver version {driver_info})" if driver_info else ""
    print(f" Skipping {feature}: CUDA unavailable{suffix}. Update the NVIDIA driver for full coverage.")
    return False

def test_architecture_detection():
    """Test architecture detection."""
    print("=== Architecture Detection Test ===")
    if not torch.cuda.is_available():
        ensure_cuda("architecture detection")
        return
    device_props = torch.cuda.get_device_properties(0)
    compute_capability = f"{device_props.major}.{device_props.minor}"
    gpu_name = device_props.name
    print(f"GPU: {gpu_name}")
    print(f"Compute Capability: {compute_capability}")
    arch_cfg = ArchitectureConfig()
    if arch_cfg.arch in {"blackwell", "grace_blackwell"}:
        print(f" Detected {arch_cfg.get_architecture_name()}")
    else:
        print(f" Non-Blackwell GPU detected (compute capability {compute_capability})")

def test_pytorch_29_features():
    """Test PyTorch 2.9 features."""
    print("\n=== PyTorch 2.9 Features Test ===")

    if not ensure_cuda("torch.compile tests"):
        return
    
    # Test torch.compile
    try:
        model = torch.nn.Linear(1000, 1000).cuda()
        compiled_model = torch.compile(model, mode="max-autotune")

        inputs = torch.randn(32, 1000, device="cuda")

        iters_warmup, iters_meas = 5, 20
        for _ in range(iters_warmup):
            _ = compiled_model(inputs)

        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        torch.cuda.synchronize()
        start.record()
        for _ in range(iters_meas):
            _ = compiled_model(inputs)
        end.record()
        end.synchronize()
        avg_ms = start.elapsed_time(end) / iters_meas
        print(f" torch.compile (max-autotune) avg {avg_ms:.3f} ms/iter")
        print("  Hint: use mode='reduce-overhead' only for shape-stable graphs that benefit from CUDA Graphs; keep defaults for dynamic workloads.")
    except Exception as e:
        print(f" torch.compile failed: {e}")
    
    # Test dynamic shapes
    try:
        torch._dynamo.config.automatic_dynamic_shapes = True
        print(" Dynamic shapes enabled")
    except Exception as e:
        print(f" Dynamic shapes failed: {e}")
    
    # Test Triton config access (robust to version changes)
    try:
        inductor_cfg = getattr(torch, "_inductor", None)
        if inductor_cfg is not None and hasattr(inductor_cfg, "config"):
            triton_cfg = getattr(inductor_cfg.config, "triton", None)
            if triton_cfg is not None:
                if hasattr(triton_cfg, "unique_kernel_names"):
                    setattr(triton_cfg, "unique_kernel_names", True)
                # Best-effort enable autotune if an appropriate knob exists
                if hasattr(triton_cfg, "autotune_experimental"):
                    setattr(triton_cfg, "autotune_experimental", True)
        print(" Triton configuration accessible")
    except Exception as e:
        print(f" Triton config access failed: {e}")

def test_cuda_130_features():
    """Test CUDA 13.0 features."""
    print("\n=== CUDA 13.0 Features Test ===")
    
    # Test stream-ordered memory allocation
    # Note: Actual implementation would require CUDA kernel code
    # For now, we verify CUDA 13.0+ is available which supports these features
    try:
        cuda_version = torch.version.cuda
        if cuda_version and float(cuda_version.split('.')[0] + '.' + cuda_version.split('.')[1]) >= 13.0:
            print(" Stream-ordered memory allocation support available (CUDA 13.0+)")
        else:
            print(f" Stream-ordered memory requires CUDA 13.0+, found {cuda_version}")
    except Exception as e:
        print(f" Stream-ordered memory check failed: {e}")
    
    # Test TMA (Tensor Memory Accelerator)
    # Note: Actual implementation would require CUDA kernel code
    # For now, we verify CUDA 13.0+ is available which supports these features
    try:
        cuda_version = torch.version.cuda
        if cuda_version and float(cuda_version.split('.')[0] + '.' + cuda_version.split('.')[1]) >= 13.0:
            print(" TMA support available (CUDA 13.0+)")
        else:
            print(f" TMA requires CUDA 13.0+, found {cuda_version}")
    except Exception as e:
        print(f" TMA check failed: {e}")

def test_profiling_tools():
    """Test profiling tools."""
    print("\n=== Profiling Tools Test ===")

    if ensure_cuda("PyTorch profiler"):
        try:
            with profile(
                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                profile_memory=True,
                record_shapes=True,
                with_stack=True,
                with_flops=True,
                with_modules=True,
                schedule=schedule(wait=1, warmup=1, active=3, repeat=2),
            ):
                x = torch.randn(1000, 1000, device="cuda")
                y = torch.randn(1000, 1000, device="cuda")
                _ = torch.mm(x, y)
                torch.cuda.synchronize()
            print(" PyTorch profiler works")
        except Exception as exc:
            print(f" PyTorch profiler failed: {exc}")

    try:
        # Use conditional NVTX ranges - only enabled when profiling
        from common.python.nvtx_helper import nvtx_range, get_nvtx_enabled
        config = SimpleNamespace(enable_nvtx=True)
        enable_nvtx = get_nvtx_enabled(config)

        with nvtx_range("test_region", enable=enable_nvtx):
            time.sleep(0.1)
        print(" NVTX annotations work")
    except Exception as exc:
        print(f" NVTX failed: {exc}")


def test_triton_35():
    """Test Triton 3.x features."""
    print("\n=== Triton 3.x Features Test ===")

    if not ensure_cuda("Triton kernels"):
        return

    try:
        if triton is None or tl is None:
            raise RuntimeError("Triton not available")
        print(f" Triton version: {triton.__version__}")

        @triton.jit
        def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
            pid = tl.program_id(axis=0)
            block_start = pid * BLOCK_SIZE
            offsets = block_start + tl.arange(0, BLOCK_SIZE)
            mask = offsets < n_elements
            x = tl.load(x_ptr + offsets, mask=mask)
            y = tl.load(y_ptr + offsets, mask=mask)
            tl.store(output_ptr + offsets, x + y, mask=mask)

        n = 1024
        block = 128
        x = torch.ones(n, dtype=torch.float32, device="cuda")
        y = torch.ones(n, dtype=torch.float32, device="cuda")
        out = torch.empty(n, dtype=torch.float32, device="cuda")
        grid = (triton.cdiv(n, block),)
        add_kernel[grid](x, y, out, n, BLOCK_SIZE=block)
        torch.cuda.synchronize()
        print(" Triton kernel compile and launch works")
    except Exception as exc:
        print(f" Triton test failed: {exc}")


def test_performance():
    """Test basic performance."""
    print("\n=== Performance Test ===")

    if not ensure_cuda("performance microbenchmarks"):
        return

    size = 1024 * 1024 * 1024
    x = torch.randn(size // 4, dtype=torch.float32, device="cuda")
    y = torch.randn(size // 4, dtype=torch.float32, device="cuda")

    torch.cuda.synchronize()
    start = time.time()
    _ = x + y
    torch.cuda.synchronize()
    end = time.time()
    bandwidth = (size * 2) / (end - start) / 1e9
    print(f" Memory bandwidth: {bandwidth:.2f} GB/s")

    a = torch.randn(2048, 2048, dtype=torch.float32, device="cuda")
    b = torch.randn(2048, 2048, dtype=torch.float32, device="cuda")

    torch.cuda.synchronize()
    start = time.time()
    _ = torch.mm(a, b)
    torch.cuda.synchronize()
    end = time.time()
    flops = 2 * 2048 * 2048 * 2048 / (end - start) / 1e12
    print(f" Compute performance: {flops:.2f} TFLOPS")

def test_kv_cache_batched_attention_2gpu():
    """
    Test batched attention with heterogeneous cache lengths on 2 GPUs.
    Validates padding/masking logic and head sharding.
    """
    print("\n=== KV Cache Batched Attention 2-GPU Test ===")
    
    if DemoCausalLM is None or ShardedKVCacheManager is None:
        print(" Skipping: ch16 models not available")
        return
    
    if not torch.cuda.is_available():
        print(" Skipping: CUDA unavailable")
        return
    
    if not dist.is_initialized():
        try:
            dist.init_process_group(backend="nccl")
        except Exception as e:
            print(f" Skipping: Cannot initialize distributed: {e}")
            return
    
    world_size = dist.get_world_size()
    if world_size != 2:
        print(f" Skipping: Requires 2 GPUs, found {world_size}")
        return
    
    rank = dist.get_rank()
    device = torch.device(f"cuda:{rank}")
    
    num_layers = 2
    num_heads = 8  # 4 heads per GPU
    d_model = 64
    head_dim = d_model // num_heads
    vocab_size = 128
    batch_size = 4
    
    # Create model and cache with 2 GPUs
    model = DemoCausalLM(
        vocab_size=vocab_size,
        d_model=d_model,
        num_layers=num_layers,
        num_heads=num_heads,
        num_gpus=2,
        max_batch_size=batch_size,
        max_seq_len=64,
    ).to(device)
    model.eval()
    
    kv_cache = ShardedKVCacheManager(
        num_layers=num_layers,
        num_heads=num_heads,
        head_dim=head_dim,
        max_batch_size=batch_size,
        max_seq_len=64,
        num_gpus=2,
        dtype=torch.float32,
        page_size=4,
    )
    
    # Allocate slots for batch
    slots = [kv_cache.allocate_slot() for _ in range(batch_size)]
    assert all(s is not None for s in slots), "Failed to allocate slots"
    
    # Create heterogeneous initial sequences: [3, 5, 7, 9] tokens
    initial_lengths = [3, 5, 7, 9]
    initial_prompts = []
    for length in initial_lengths:
        prompt = torch.randint(1, vocab_size, (1, length), device=device, dtype=torch.long)
        initial_prompts.append(prompt)
    
    # Process initial prompts and cache KV
    for idx, prompt in enumerate(initial_prompts):
        logits, keys, values = model(prompt)
        
        # Store in cache
        key_stack = torch.stack(
            [keys[layer_idx, 0] for layer_idx in range(num_layers)], dim=0
        )
        value_stack = torch.stack(
            [values[layer_idx, 0] for layer_idx in range(num_layers)], dim=0
        )
        kv_cache.append_tokens(
            slot=slots[idx],
            key=key_stack,
            value=value_stack,
            num_tokens=prompt.shape[1],
        )
    
    if rank == 0:
        print(f" Initial cache lengths: {initial_lengths}")
        print(f" Heads per GPU: {kv_cache.heads_per_gpu}")
    
    # Generate next token using cached KV with batched attention
    new_tokens = torch.randint(1, vocab_size, (batch_size, 1), device=device, dtype=torch.long)
    
    # Build past_kv for batched forward
    past_kv = []
    for layer_idx in range(num_layers):
        layer_cache = []
        for slot in slots:
            cache_k, cache_v = kv_cache.get_cache(slot, layer_idx)
            layer_cache.append((cache_k, cache_v))
        past_kv.append(layer_cache)
    
    # Forward with cached KV (tests batched attention with padding/masking)
    logits_batched, keys_batched, values_batched = model(new_tokens, past_kv=past_kv)
    
    if rank == 0:
        print(f" Batched logits shape: {logits_batched.shape}")
        assert logits_batched.shape == (batch_size, vocab_size), "Incorrect batched logits shape"
    
    # Validate against full-context forward for each request
    for idx in range(batch_size):
        full_sequence = torch.cat([initial_prompts[idx], new_tokens[idx:idx+1]], dim=1)
        logits_full, _, _ = model(full_sequence)
        
        # Compare logits
        torch.testing.assert_close(
            logits_batched[idx:idx+1],
            logits_full,
            rtol=1e-2,
            atol=5e-2,
            msg=f"Logits mismatch for request {idx} (rank {rank})",
        )
    
    if rank == 0:
        print(" Batched attention matches full-context forward")
        print(" Padding/masking logic validated")
        print(" Head sharding across 2 GPUs validated")
    
    # Cleanup
    for slot in slots:
        kv_cache.free_slot(slot)
    
    if dist.is_initialized():
        dist.barrier()


def test_inference_server_8gpu_distributed():
    """
    Full end-to-end test of InferenceServer8GPU with distributed execution.
    Tests continuous batching, cache management, and throughput.
    """
    print("\n=== Inference Server 8-GPU Distributed Test ===")
    
    if InferenceServer8GPU is None or InferenceRequest is None or DemoCausalLM is None:
        print(" Skipping: ch16 models not available")
        return
    
    if not torch.cuda.is_available():
        print(" Skipping: CUDA unavailable")
        return
    
    if not dist.is_initialized():
        try:
            dist.init_process_group(backend="nccl")
        except Exception as e:
            print(f" Skipping: Cannot initialize distributed: {e}")
            return
    
    world_size = dist.get_world_size()
    if world_size != 8:
        print(f" Skipping: Requires 8 GPUs, found {world_size}")
        return
    
    rank = dist.get_rank()
    
    # Create demo model
    model = DemoCausalLM(
        vocab_size=256,
        d_model=128,
        num_layers=4,
        num_heads=8,
        num_gpus=8,
        max_batch_size=32,
        max_seq_len=256,
    )
    
    # Create server
    server = InferenceServer8GPU(
        model=model,
        num_layers=4,
        d_model=128,
        num_heads=8,
        max_batch_size=32,
        max_seq_len=256,
    )
    
    # Submit test requests (all ranks enqueue identical work to keep the scheduler in sync)
    if rank == 0:
        print(" Submitting 32 test requests...")
    
    for i in range(32):
        # Varying prompt lengths: 10-50 tokens
        prompt_length = 10 + (i * 2)
        request = InferenceRequest(
            request_id=f"test_req_{i}",
            prompt_tokens=list(range(1, prompt_length + 1)),
            max_new_tokens=10,
            temperature=1.0,
            priority=0,
        )
        # All ranks add the same requests to avoid deadlock in all_reduce
        server.scheduler.add_request(request)
    
    # Synchronize all ranks before starting serve loop
    dist.barrier()
    
    # Run serving loop for 2 seconds (all ranks execute together)
    server.serve_loop(duration_seconds=2.0)
    
    # Check statistics
    if rank == 0:
        stats = server.scheduler.get_stats()
        cache_stats = server.kv_cache.stats()
        
        print(f" Total requests: {stats['total_requests']}")
        print(f" Completed: {stats['completed_requests']}")
        print(f" Tokens generated: {stats['total_tokens_generated']}")
        print(f" Active cache slots: {cache_stats['active_slots']}")
        print(f" Resident pages: {cache_stats['resident_pages']}")
        
        assert stats['completed_requests'] > 0, "No requests completed"
        assert stats['total_tokens_generated'] > 0, "No tokens generated"
        print(" Distributed inference server validated")
    
    dist.barrier()


def main():
    """Run all tests."""
    print("AI Performance Engineering - Blackwell Validation Test")
    print("=" * 60)
    
    test_architecture_detection()
    test_pytorch_29_features()
    test_cuda_130_features()
    test_profiling_tools()
    test_triton_35()
    test_performance()
    
    print("\n" + "=" * 60)
    print("Test completed!")

if __name__ == "__main__":
    main()
