"""Integration tests for profiling workflows (nsys/ncu).

Tests that profiling workflows work correctly, handle tool unavailability
gracefully, and produce valid artifacts.
"""

import pytest
import sys
from pathlib import Path
from unittest.mock import patch, Mock

# Add repo root to path
repo_root = Path(__file__).parent.parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from common.python.env_defaults import apply_env_defaults
apply_env_defaults()

import torch
from common.python.benchmark_harness import BenchmarkHarness, BenchmarkMode, BenchmarkConfig
from common.python.chapter_compare_template import discover_benchmarks, load_benchmark
from common.python.discovery import discover_all_chapters
from common.python.profiling_runner import (
    check_nsys_available,
    check_ncu_available,
    run_nsys_profiling,
    run_ncu_profiling,
)


# Skip tests if CUDA is not available
pytestmark = pytest.mark.skipif(
    not torch.cuda.is_available(),
    reason="CUDA required - NVIDIA GPU and tools must be available"
)


class TestProfilingWorkflowIntegration:
    """Integration tests for profiling workflows."""
    
    def test_profiling_tool_availability_checks(self):
        """Test that profiling tool availability checks work."""
        # These should not crash even if tools are unavailable
        nsys_available = check_nsys_available()
        ncu_available = check_ncu_available()
        
        assert isinstance(nsys_available, bool)
        assert isinstance(ncu_available, bool)
    
    def test_benchmark_with_profiling_disabled(self):
        """Test that benchmarks run correctly with profiling disabled."""
        # Find a real benchmark
        chapters = discover_all_chapters(repo_root)
        if not chapters:
            pytest.skip("No chapters found")
        
        pairs = None
        for chapter_dir in chapters:
            chapter_pairs = discover_benchmarks(chapter_dir)
            if chapter_pairs:
                pairs = chapter_pairs
                break
        
        if not pairs:
            pytest.skip("No benchmark pairs found")
        
        baseline_path, _, _ = pairs[0]
        benchmark = load_benchmark(baseline_path)
        if benchmark is None:
            pytest.skip("Failed to load benchmark")
        
        config = BenchmarkConfig(
            iterations=5,
            warmup=1,
            enable_profiling=False,  # Profiling disabled
            enable_nsys=False,
            enable_ncu=False,
        )
        harness = BenchmarkHarness(mode=BenchmarkMode.CUSTOM, config=config)
        
        result = harness.benchmark(benchmark)
        
        # Should complete successfully without profiling
        assert result.timing.iterations == 5
        # Profiling artifacts should be None
        assert result.artifacts is None or (
            result.artifacts.nsys_rep is None and
            result.artifacts.ncu_rep is None
        )
    
    def test_profiling_graceful_degradation(self):
        """Test that profiling gracefully degrades when tools are unavailable."""
        # Find a real benchmark
        chapters = discover_all_chapters(repo_root)
        if not chapters:
            pytest.skip("No chapters found")
        
        pairs = None
        for chapter_dir in chapters:
            chapter_pairs = discover_benchmarks(chapter_dir)
            if chapter_pairs:
                pairs = chapter_pairs
                break
        
        if not pairs:
            pytest.skip("No benchmark pairs found")
        
        baseline_path, _, _ = pairs[0]
        benchmark = load_benchmark(baseline_path)
        if benchmark is None:
            pytest.skip("Failed to load benchmark")
        
        # Enable profiling even if tools might not be available
        config = BenchmarkConfig(
            iterations=5,
            warmup=1,
            enable_profiling=True,
            enable_nsys=True,
            enable_ncu=True,
        )
        harness = BenchmarkHarness(mode=BenchmarkMode.CUSTOM, config=config)
        
        # Should not crash even if profiling tools are unavailable
        result = harness.benchmark(benchmark)
        
        # Should still produce valid timing results
        assert result.timing.iterations == 5
    
    @pytest.mark.skipif(not check_nsys_available(), reason="nsys not available")
    def test_nsys_profiling_workflow(self, tmp_path):
        """Test nsys profiling workflow if tool is available."""
        # Find a real benchmark
        chapters = discover_all_chapters(repo_root)
        if not chapters:
            pytest.skip("No chapters found")
        
        pairs = None
        for chapter_dir in chapters:
            chapter_pairs = discover_benchmarks(chapter_dir)
            if chapter_pairs:
                pairs = chapter_pairs
                break
        
        if not pairs:
            pytest.skip("No benchmark pairs found")
        
        baseline_path, _, _ = pairs[0]
        benchmark = load_benchmark(baseline_path)
        if benchmark is None:
            pytest.skip("Failed to load benchmark")
        
        output_dir = tmp_path / "profiles"
        output_dir.mkdir()
        
        # This is a simplified test - actual profiling would take longer
        # We're just testing that the workflow doesn't crash
        config = BenchmarkConfig(
            iterations=3,  # Minimal iterations for profiling test
            warmup=0,
            enable_profiling=True,
            enable_nsys=True,
            profiling_output_dir=str(output_dir),
        )
        
        # Note: Full profiling test would require running actual nsys,
        # which is slow and may not be available in test environment
        # This test verifies the integration path exists and doesn't crash

