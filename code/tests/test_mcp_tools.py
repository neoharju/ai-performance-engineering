from __future__ import annotations

import asyncio
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List

import pytest

import json

import mcp.mcp_server as mcp_server


REPO_ROOT = Path(__file__).resolve().parent.parent
ARTIFACT_RUNS_DIR = REPO_ROOT / "artifacts" / "runs"
ARTIFACT_DIR = ARTIFACT_RUNS_DIR
MICROBENCH_DIR = REPO_ROOT / "artifacts" / "mcp-microbench"
REPORT_OUTPUT = REPO_ROOT / "artifacts" / "mcp-report.pdf"
EXPORT_OUTPUT = REPO_ROOT / "artifacts" / "mcp-export.csv"
BENCH_FILE = REPO_ROOT / "benchmark_test_results.json"
PROFILE_FIXTURE_DIR = ARTIFACT_RUNS_DIR / "mcp-fixtures" / "profiles" / "bench" / "ch04"
NSYS_SAMPLE = PROFILE_FIXTURE_DIR / "baseline_nccl_baseline.nsys-summary.csv"
NCU_SAMPLE = PROFILE_FIXTURE_DIR / "baseline_nvlink_baseline.ncu-rep"
NSYS_REP_FIXTURE = PROFILE_FIXTURE_DIR / "baseline_fixture.nsys-rep"
NCU_REP_FIXTURE = PROFILE_FIXTURE_DIR / "baseline_fixture.ncu-rep"


@dataclass(frozen=True)
class ToolCase:
    name: str
    params: Dict[str, Any]
    category: str
    slow: bool = False
    timeout: int = 15


CATEGORY_TOOLS: Dict[str, List[str]] = {
    "gpu": [
        "gpu_info",
        "gpu_bandwidth",
        "gpu_topology",
        "gpu_topology_matrix",
        "gpu_power",
    ],
    "system": [
        "system_software",
        "system_dependencies",
        "system_context",
        "system_capabilities",
        "system_parameters",
        "system_container",
        "system_cpu_memory",
        "system_env",
        "system_network",
        "system_full",
    ],
    "info": [
        "info_features",
    ],
    "benchmarking": [
        "benchmark_targets",
        "list_chapters",
        "run_benchmarks",
        "benchmark_variants",
        "benchmark_deep_dive_compare",
        "benchmark_llm_patch_loop",
        "benchmark_report",
        "benchmark_export",
        "benchmark_compare_runs",
        "benchmark_triage",
        "benchmark_data",
        "benchmark_overview",
        "benchmark_history",
        "benchmark_trends",
        "benchmark_compare",
    ],
    "analysis": [
        "analyze_bottlenecks",
        "analyze_pareto",
        "analyze_scaling",
        "analyze_stacking",
        "analyze_whatif",
        "analyze_comm_overlap",
        "analyze_memory_patterns",
        "analyze_dataloader",
        "analyze_energy",
        "predict_scaling",
    ],
    "optimization": [
        "optimize",
        "recommend",
        "optimize_roi",
        "optimize_techniques",
    ],
    "distributed": [
        "distributed_plan",
        "distributed_nccl",
        "launch_plan",
    ],
    "inference": [
        "inference_vllm",
        "inference_quantization",
        "inference_deploy",
        "inference_estimate",
    ],
    "ai_llm": [
        "ask",
        "explain",
        "ai_status",
        "ai_troubleshoot",
    ],
    "profiling": [
        "profile_flame",
        "profile_memory",
        "profile_kernels",
        "profile_roofline",
        "profile_nsys",
        "profile_ncu",
        "profile_torch",
        "profile_hta",
        "profile_compare",
        "compare_nsys",
        "compare_ncu",
        "nsys_summary",
    ],
    "exports": [
        "export_csv",
        "export_pdf",
        "export_html",
    ],
    "hw": [
        "hw_speed",
        "hw_roofline",
        "hw_disk",
        "hw_pcie",
        "hw_cache",
        "hw_tc",
        "hw_network",
        "hw_ib",
        "hw_nccl",
        "hw_p2p",
    ],
    "huggingface": [
        "hf",
    ],
    "cluster_cost": [
        "cluster_slurm",
        "cost_estimate",
    ],
    "tools": [
        "tools_kv_cache",
        "tools_cost_per_token",
        "tools_compare_precision",
        "tools_detect_cutlass",
        "tools_dump_hw",
        "tools_probe_hw",
    ],
    "utility": [
        "status",
        "context_summary",
        "context_full",
        "triage",
        "job_status",
        "suggest_tools",
    ],
}

SLOW_TOOLS = {
    "gpu_bandwidth",
    "run_benchmarks",
    "optimize",
    "benchmark_variants",
    "benchmark_deep_dive_compare",
    "benchmark_llm_patch_loop",
    "profile_nsys",
    "profile_ncu",
    "profile_torch",
    "profile_hta",
    "profile_flame",
    "profile_memory",
    "profile_kernels",
    "profile_roofline",
    "compare_nsys",
    "compare_ncu",
    "hw_speed",
    "hw_roofline",
    "hw_disk",
    "hw_pcie",
    "hw_cache",
    "hw_tc",
    "hw_nccl",
    "hw_ib",
    "hw_p2p",
}

BENCHMARK_SLOW_TOOLS = {
    "run_benchmarks",
    "optimize",
    "benchmark_variants",
    "benchmark_deep_dive_compare",
    "benchmark_llm_patch_loop",
}

TOOL_PARAMS: Dict[str, Dict[str, Any]] = {
    "optimize": {
        "target": "ch10:atomic_reduction",
        "profile": "minimal",
        "iterations": 1,
        "warmup": 5,
        "llm_analysis": False,
        "force_llm": False,
        "apply_patches": False,
        "rebenchmark_llm_patches": False,
    },
    "run_benchmarks": {
        "targets": ["ch10:atomic_reduction"],
        "profile": "minimal",
        "iterations": 1,
        "warmup": 5,
        "llm_analysis": False,
    },
    "benchmark_variants": {
        "targets": ["ch10:atomic_reduction"],
        "profile": "minimal",
        "iterations": 1,
        "warmup": 5,
        "llm_analysis": False,
        "force_llm": False,
        "apply_patches": False,
        "rebenchmark_llm_patches": False,
    },
    "benchmark_report": {
        "data_file": str(BENCH_FILE),
        "output": str(REPORT_OUTPUT),
        "format": "pdf",
        "title": "MCP Report",
        "author": "MCP Tests",
    },
    "benchmark_export": {
        "data_file": str(BENCH_FILE),
        "format": "csv",
        "output": str(EXPORT_OUTPUT),
    },
    "benchmark_compare_runs": {
        "baseline": str(BENCH_FILE),
        "candidate": str(BENCH_FILE),
        "top": 3,
    },
    "analyze_whatif": {"max_vram_gb": 24, "max_latency_ms": 50, "include_context": False},
    "recommend": {"model_size": 7, "gpus": 1, "goal": "throughput", "include_context": False},
    "distributed_plan": {"model_size": 7, "gpus": 4, "nodes": 1, "include_context": False},
    "distributed_nccl": {"nodes": 1, "gpus": 4, "include_context": False},
    "launch_plan": {"nodes": 1, "gpus_per_node": 2, "script": "train.py"},
    "inference_vllm": {"model": "7b", "model_size": 7, "target": "throughput", "include_context": False},
    "inference_deploy": {"model": "7b", "model_size": 7, "goal": "throughput", "include_context": False},
    "inference_estimate": {"model": "7b", "model_size": 7, "goal": "throughput", "include_context": False},
    "inference_quantization": {"model_size": 7, "include_context": False},
    "ask": {"question": "What is tensor parallelism?", "include_context": False},
    "explain": {"concept": "warp divergence", "include_context": False},
    "profile_nsys": {
        "command": ["python", "-c", "print('nsys')"],
        "output_name": "mcp_nsys_test",
        "output_dir": str(ARTIFACT_RUNS_DIR),
        "run_id": "mcp_nsys_test",
        "preset": "light",
        "full_timeline": False,
        "trace_forks": False,
        "include_context": False,
    },
    "profile_ncu": {
        "command": ["python", "-c", "print('ncu')"],
        "output_name": "mcp_ncu_test",
        "output_dir": str(ARTIFACT_RUNS_DIR),
        "run_id": "mcp_ncu_test",
        "workload_type": "memory_bound",
        "include_context": False,
    },
    "compare_nsys": {"profiles_dir": str(PROFILE_FIXTURE_DIR), "include_context": False},
    "compare_ncu": {"profiles_dir": str(PROFILE_FIXTURE_DIR), "include_context": False},
    "nsys_summary": {"report_path": str(NSYS_SAMPLE), "include_context": False},
    "export_csv": {"detailed": False, "include_context": False},
    "export_pdf": {"include_context": False},
    "export_html": {"include_context": False},
    "hw_speed": {"gemm_size": 256, "mem_size_mb": 8, "mem_stride": 64, "include_context": False},
    "hw_roofline": {"size_mb": 8, "strides": [64, 128]},
    "hw_disk": {"file_size_mb": 8, "block_size_kb": 128, "tmp_dir": str(MICROBENCH_DIR)},
    "hw_pcie": {"size_mb": 8, "iters": 1},
    "hw_cache": {"size_mb": 8, "stride": 64},
    "hw_tc": {"size": 512, "precision": "fp16"},
    "hw_ib": {"size_mb": 64},
    "hw_nccl": {"collective": "all_reduce", "gpus": 2},
    "hw_p2p": {"size_mb": 64},
    "info_features": {},
    "profile_compare": {"chapter": "ch11"},
    "benchmark_deep_dive_compare": {
        "targets": ["ch10:atomic_reduction"],
        "output_dir": str(REPO_ROOT / "artifacts" / "mcp-deep-dive-tests"),
        "iterations": 1,
        "warmup": 5,
        "timeout_seconds": 900,
    },
    "benchmark_llm_patch_loop": {
        "targets": ["ch10:atomic_reduction"],
        "output_dir": str(REPO_ROOT / "artifacts" / "mcp-llm-loop-tests"),
        "compare_output_dir": str(REPO_ROOT / "artifacts" / "mcp-llm-loop-compare-tests"),
        "iterations": 1,
        "warmup": 5,
        "compare_iterations": 1,
        "compare_warmup": 5,
        "force_llm": True,
        "llm_explain": True,
    },
    "profile_torch": {
        "script": str(REPO_ROOT / "tests" / "fixtures" / "mcp_torch_profile_target.py"),
        "output_dir": str(ARTIFACT_RUNS_DIR),
        "run_id": "mcp_torch_test",
    },
    "profile_hta": {
        "command": ["python", "-c", "print('hta')"],
        "output_dir": str(ARTIFACT_RUNS_DIR),
        "run_id": "mcp_hta_test",
    },
    "hf": {"action": "search", "query": "llama", "limit": 3},
    "cluster_slurm": {"model": "7b", "nodes": 1, "gpus": 2},
    "cost_estimate": {"gpu_type": "h100", "num_gpus": 1, "hours_per_day": 1},
    "suggest_tools": {"query": "profile this model"},
    "job_status": {"job_id": "test_job_missing"},
    "benchmark_data": {"page": 1, "page_size": 10},
    "benchmark_overview": {},
    "benchmark_history": {},
    "benchmark_trends": {},
    "benchmark_compare": {"baseline": str(BENCH_FILE), "candidate": str(BENCH_FILE), "top": 3},
    "ai_troubleshoot": {"issue": "NCCL timeout", "symptoms": ["timeout"], "config": {"gpus": 4}},
}


def _build_cases() -> List[ToolCase]:
    cases: List[ToolCase] = []
    for category, tools in CATEGORY_TOOLS.items():
        for name in tools:
            params = TOOL_PARAMS.get(name, {})
            cases.append(
                ToolCase(
                    name=name,
                    params=params,
                    category=category,
                    slow=name in SLOW_TOOLS,
                    timeout=900 if name in BENCHMARK_SLOW_TOOLS else 600 if name in SLOW_TOOLS else 60,
                )
            )
    return cases


ALL_TOOL_CASES = _build_cases()
SLOW_TOOL_CASES = [case for case in ALL_TOOL_CASES if case.slow]


@pytest.fixture(scope="module", autouse=True)
def prepare_artifacts() -> None:
    ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)
    MICROBENCH_DIR.mkdir(parents=True, exist_ok=True)
    PROFILE_FIXTURE_DIR.mkdir(parents=True, exist_ok=True)
    nsys_csv = PROFILE_FIXTURE_DIR / "baseline_nccl_baseline.nsys-summary.csv"
    if not nsys_csv.exists():
        nsys_csv.write_text(
            "Section,Metric Name,Metric Value,Time (%)\n"
            "NVTX,range0,123,45.6\n"
        )


@pytest.fixture()
def server():
    return mcp_server.MCPServer()


def _payload_from_result(result: mcp_server.ToolResult) -> Dict[str, Any]:
    assert result.content, "Tool response must include content"
    entry = result.content[0]
    ctype = entry.get("type")
    if ctype == "text":
        payload = json.loads(entry.get("text"))
    elif ctype == "application/json":
        payload = entry.get("json")
    else:
        raise AssertionError(f"Unexpected content type: {ctype}")
    assert isinstance(payload, dict), "Payload must be a JSON object"
    return payload


def _call_with_timeout(server: mcp_server.MCPServer, case: ToolCase) -> mcp_server.ToolResult:
    with ThreadPoolExecutor(max_workers=1) as pool:
        fut = pool.submit(server.call_tool, case.name, case.params)
        return fut.result(timeout=case.timeout)


def _case_ids(cases: Iterable[ToolCase]) -> List[str]:
    return [case.name for case in cases]


def test_expected_tool_registration_matches_catalog():
    expected = {case.name for case in ALL_TOOL_CASES}
    registered = set(mcp_server.TOOLS.keys())
    assert expected == registered, "Tool catalog must mirror MCP server registry"
    assert len(expected) == 96


def test_optimize_path_resolution():
    from mcp.mcp_server import _resolve_benchmark_target_from_path

    path = REPO_ROOT / "ch10" / "baseline_atomic_reduction.py"
    target, err = _resolve_benchmark_target_from_path(str(path))
    assert err is None
    assert target == "ch10:atomic_reduction"


def test_tool_list_protocol_matches_registration(server: mcp_server.MCPServer):
    tool_list = server.get_tool_list()
    names = {tool["name"] for tool in tool_list}
    expected = {case.name for case in ALL_TOOL_CASES}
    assert names == expected


@pytest.mark.parametrize(
    ("query", "expected_tool"),
    [
        ("optimize ch10/baseline_atomic_reduction.py", "optimize"),
        ("tune cuda kernel occupancy", "profile_ncu"),
        ("torch.compile graph breaks", "profile_torch"),
        ("autotune tile sizes", "benchmark_variants"),
        ("compare nsys reports", "compare_nsys"),
        ("compare benchmark runs", "benchmark_compare_runs"),
        ("memory coalescing analysis", "analyze_memory_patterns"),
            ("slurm script for training", "cluster_slurm"),
            ("check cuda version", "system_software"),
            ("kv cache size", "tools_kv_cache"),
            ("benchmark history", "benchmark_history"),
            ("performance trends over time", "benchmark_trends"),
            ("gpu temperature", "gpu_power"),
            ("huggingface search llama", "hf"),
            ("export csv here", "export_csv"),
            ("network status ib", "system_network"),
            ("topology matrix", "gpu_topology_matrix"),
            ("cloud cost estimate", "cost_estimate"),
            ("llm status", "ai_status"),
    ],
)
def test_suggest_tools_common_intents(server: mcp_server.MCPServer, query: str, expected_tool: str):
    result = server.call_tool("suggest_tools", {"query": query})
    payload = _payload_from_result(result)
    tool_result = payload.get("result") or {}
    suggestions = tool_result.get("suggestions") or []
    tools = {entry.get("tool") for entry in suggestions if isinstance(entry, dict)}
    assert expected_tool in tools


def test_tool_response_is_text_only(server: mcp_server.MCPServer):
    """MCP responses must emit only text content to satisfy clients that reject other types."""
    result = server.call_tool("status", {})
    assert isinstance(result.content, list)
    assert len(result.content) == 1, "MCP content should contain exactly one text entry"
    entry = result.content[0]
    assert entry["type"] == "text"
    payload = json.loads(entry["text"])
    assert isinstance(payload, dict)


def test_nsys_summary_uses_fixture_csv(server: mcp_server.MCPServer):
    result = server.call_tool(
        "nsys_summary",
        {"report_path": str(NSYS_SAMPLE), "include_context": False},
    )
    payload = _payload_from_result(result)
    assert payload["tool"] == "nsys_summary"
    assert payload["status"] == "ok"
    tool_result = payload["result"]
    assert tool_result.get("success") is True
    assert tool_result.get("metrics")


FAST_TOOL_CASES = [case for case in ALL_TOOL_CASES if not case.slow]


@pytest.mark.parametrize("case", FAST_TOOL_CASES, ids=_case_ids(FAST_TOOL_CASES))
def test_tool_call_returns_json_envelope(server: mcp_server.MCPServer, case: ToolCase):
    result = server.call_tool(case.name, case.params)
    payload = _payload_from_result(result)
    assert payload["tool"] == case.name
    assert payload["status"] in {"ok", "error"}
    assert "result" in payload
    assert "context_summary" in payload


def test_mcp_protocol_round_trip(server: mcp_server.MCPServer):
    async def _exercise():
        init = await server.handle_message({"jsonrpc": "2.0", "id": 1, "method": "initialize", "params": {}})
        assert init and init["result"]["protocolVersion"] == "2024-11-05"

        tool_list = await server.handle_message({"jsonrpc": "2.0", "id": 2, "method": "tools/list", "params": {}})
        assert tool_list and "tools" in tool_list["result"]

        sample_tool = "status"
        call = await server.handle_message(
            {"jsonrpc": "2.0", "id": 3, "method": "tools/call", "params": {"name": sample_tool, "arguments": {}}}
        )
        assert call
        entry = call["result"]["content"][0]
        if entry["type"] == "text":
            payload = json.loads(entry["text"])
        elif entry["type"] == "application/json":
            payload = entry["json"]
        else:
            raise AssertionError(f"Unexpected content type: {entry['type']}")
        assert payload["tool"] == sample_tool

    asyncio.run(_exercise())


@pytest.mark.parametrize("case", SLOW_TOOL_CASES, ids=_case_ids(SLOW_TOOL_CASES))
def test_slow_tools_opt_in_execution(server: mcp_server.MCPServer, case: ToolCase):
    result = _call_with_timeout(server, case)
    payload = _payload_from_result(result)
    assert payload["tool"] == case.name
    assert payload["status"] in {"ok", "error"}
    if case.name == "benchmark_deep_dive_compare":
        assert payload["status"] == "ok"
        tool_result = payload["result"]
        assert tool_result.get("success") is True
        assert Path(tool_result["analysis_json"]).exists()
    if case.name == "run_benchmarks":
        tool_result = payload["result"]
        if tool_result.get("returncode", 1) == 0 and tool_result.get("results_json"):
            assert "triage" in tool_result
    if case.name == "profile_nsys":
        tool_result = payload["result"]
        if tool_result.get("success"):
            assert "nsys_metrics" in tool_result
            assert isinstance(tool_result["nsys_metrics"], dict)
    if case.name == "profile_ncu":
        tool_result = payload["result"]
        if tool_result.get("success"):
            assert "ncu_metrics" in tool_result
            assert isinstance(tool_result["ncu_metrics"], dict)
    if case.name == "profile_hta":
        tool_result = payload["result"]
        if tool_result.get("success"):
            assert "nsys_metrics" in tool_result
            assert isinstance(tool_result["nsys_metrics"], dict)
    if case.name == "profile_torch":
        tool_result = payload["result"]
        if tool_result.get("success"):
            assert "torch_metrics" in tool_result
            assert "report" in tool_result
    if case.name == "compare_nsys":
        tool_result = payload["result"]
        if NSYS_REP_FIXTURE.exists():
            assert tool_result.get("metrics")
            assert len(tool_result["metrics"]) > 0
        ncu_comparison = tool_result.get("ncu_comparison")
        if NCU_REP_FIXTURE.exists():
            assert ncu_comparison
            assert not ncu_comparison.get("error")
            assert ncu_comparison.get("kernel_comparison") or ncu_comparison.get("metrics")
            if ncu_comparison.get("kernel_comparison") is not None:
                assert len(ncu_comparison["kernel_comparison"]) > 0
    if case.name == "compare_ncu":
        tool_result = payload["result"]
        if NCU_REP_FIXTURE.exists():
            assert not tool_result.get("error")
            assert tool_result.get("kernel_comparison") or tool_result.get("metrics")
            if tool_result.get("kernel_comparison") is not None:
                assert len(tool_result["kernel_comparison"]) > 0
        nsys_comparison = tool_result.get("nsys_comparison")
        if NSYS_REP_FIXTURE.exists():
            assert nsys_comparison
            assert not nsys_comparison.get("error")
            assert nsys_comparison.get("metrics")
            assert len(nsys_comparison["metrics"]) > 0


def test_benchmark_export_runs_inprocess(server: mcp_server.MCPServer, tmp_path: Path):
    # Ensure a minimal benchmark file exists for the export tool.
    BENCH_FILE.write_text(json.dumps({"benchmarks": []}))
    output_path = tmp_path / "export.json"
    params = {"data_file": str(BENCH_FILE), "format": "json", "output": str(output_path)}
    result = server.call_tool("benchmark_export", params)
    payload = _payload_from_result(result)
    assert payload["tool"] == "benchmark_export"
    assert payload["result"].get("output") == str(output_path)
    assert output_path.exists()
